\section{Physics Simulation}
\label{sec:physicsSim}

Physics simulations are a very important class of scientific applications.
We study one representative: the \emph{Finite-Difference-Time-Domain (\FDTD) Method} used for Random Lasing Simulations.
This simulation from the field of optical physics simulates the propagation of light through a medium.

In the simulation two fields, the electric field $\vec{E}$ and the magnetic field $\vec{H}$, are iteratively updated using stencil computations.
We use the Maxwell's equations which are the basic equations describing electrodynamic processes in nature to describe the light propagating through a non-magnetic (\emph{dielectric}) medium.

\autoref{eq:div_e}---\autoref{eq:rot_d} show the Maxwell's equations consisting of four coupled partial differential equations (PDEs).
\begin{align}
  \vec{\nabla}\vec{E}\left(\vec{r}, t\right) &= 0, \label{eq:div_e}\\
  \vec{\nabla}\vec{H}\left(\vec{r}, t\right) &= 0, \label{eq:div_h}\\
  \frac{\partial\vec{H}\left(\vec{r}, t\right)}{\partial t} &= -\frac{1}{\mu_0}\vec{\nabla} \times \vec{E}\left(\vec{r}, t\right), \label{eq:rot_h}\\
  \frac{\partial\vec{D}\left(\vec{r}, t\right)}{\partial t} &= \frac{1}{\epsilon_0}\vec{\nabla} \times \vec{H}\left(\vec{r}, t\right), \label{eq:rot_d}
\end{align}

\noindent
To couple the polarisation of a medium $\vec{P}$ to the electric field, \autoref{eq:flussdichte} is introduced:
\begin{equation}
\vec{E}\left(\vec{r}, t\right) = \frac{\vec{D}\left(\vec{r}, t\right) - \vec{P}\left(\vec{r}, t, \vec{N}\right)}{\epsilon_0\epsilon_r\left(\vec{r}\right)}
\label{eq:flussdichte}
\end{equation}

\noindent
Here $\vec{N}$ is the induced energy distribution in the medium using the model proposed in~\cite{Jiang2000}.
The parameters $\mu_0$, $\epsilon_0$ and $\epsilon_r$ describe the permeability and permittivity of free space and the relative permittivity of the dielectric medium.

To solve this set of coupled PDEs, a method called Finite-Difference-Time-Domain (\FDTD)~\cite{Yee1966} can be used.
Here we use a form where the electric and magnet field are discretized within a n-dimensional regular grid.
$\vec{E}$ and $\vec{H}$ are shifted against each other by a half grid-cell.
This allows the calculation of the new values by computing finite differences between two values of the grid.
Using the \FDTD method, we implemented a simulation of the effect of random lasing on a nano-meter scale~\cite{Cao1999} for our evaluation.

\autoref{fig:fields} shows a visualization of the electric field (and the field intensity) after about  $1\,ps$ of simulation time equal to $60\,000$ iterations.
The shown field distribution can be found also in~\cite{Sebbah2002, Yamilov2005}, however the simulation parameters are different.

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{PPL/rl3.png}
    \caption[A 3D representation of the inensity of the 2D electric field as computed by the \SkelCL\ \FDTD implementation.]%
            {The image shows a 3D representation of the intensity for the 2D electric field as computed by the \SkelCL\ \FDTD implementation after $60\,000$ iterations.}
    \label{fig:fields}
\end{figure}

\begin{figure}[tbp]
\begin{lstlisting}[%
  caption={Source code of the \FDTD application in \SkelCL.},%
  label={lst:fdtd}]
auto updateEnergyDist = map(...);$\label{lst:fdtd:map}$
auto updateEField     = stencil(...);$\label{lst:fdtd:stencil1}$
auto updateHField     = stencil($\label{lst:fdtd@stencil2}$
 [](Neighborhood<float4>& E, Matrix<float4>& H) { ... });

Matrix<float4> N; // energy distribution in the medium
Matrix<float4> E; // E (electric) field
Matrix<float4> H; // H (magnetic) field

for (...) { // for each iteration
  updateEnergyDist(out(N), N, out(E));$\label{lst:fdtd:energy}$
  updateEField(out(E), H, E);$\label{lst:fdtd:efield}$
  updateHField(out(H), E, H); }$\label{lst:fdtd:hfield}$
\end{lstlisting}
\end{figure}

\subsection*{\SkelCL Implementation}

We implemented a two-dimensional version using \SkelCL as well as a manually tuned \OpenCL implementation.
To solve the PDEs in \autoref{eq:rot_h} and \autoref{eq:rot_d}, two separated three-point stencil computations are performed and one map computation for the gain-model is necessary.
\autoref{eq:div_e} and \autoref{eq:div_h} are implicitly solved by the \FDTD method~\cite{Yee1966}.
\autoref{lst:fdtd} shows the \SkelCL code of the application:
in every iteration first the energy distribution is updated (\autoref{lst:fdtd:energy}) using a map skeleton (defined in \autoref{lst:fdtd:map});
then the first stencil (defined in \autoref{lst:fdtd:stencil1}) updates the electric field $\vec{E}$ by combining a single element of $\vec{E}$ with three elements of the magnetic field $\vec{H}$ (\autoref{lst:fdtd:efield});
and finally the second stencil (defined in \autoref{lst:fdtd@stencil2}) updates $\vec{H}$ by combining a single element of $\vec{H}$ with three elements of $\vec{E}$ (\autoref{lst:fdtd:hfield}).

Please note that the two stencil computations require both fields ($\vec{E}$ and $\vec{H}$) as input.
To implement this, we use the \emph{additional argument} feature of \SkelCL which allows the additional field to be passed to skeletons on execution (see \autoref{lst:fdtd:efield} and \autoref{lst:fdtd:hfield}).
The additional arguments are passed unchanged to the customizing function of the skeleton, therefore, the function customizing the stencil in line 4 now accepts $\vec{H}$ as a second parameter.
This feature greatly increases the flexibility of applications written in \SkelCL.

% \subsubsection*{Programming effort}

\subsubsection*{Performance experiments}

In the evaluation we used a $2048 \times 2048$ sized matrix with a spatial resolution of $100$ cells per $\mu m$.
This matrix corresponds to a square-shaped medium with the edge length of $20.1\,\mu m$.
The medium size is actually smaller than the matrix size because of the border handling.
To provide a physically correct simulation, the borders of the magnet field must be treated specially.
The Stencil skeleton in \SkelCL provides sufficient functionality to allow for such border handling in the computation code.

\begin{figure}[t]
    \centering
    \includegraphics[width=.75\textwidth]{Plots/FDTD/fdtd_runtime.pdf}
    \caption{Runtime for one iteration of the \FDTD application.}
    \label{fig:fdtd_eval}
\end{figure}

We compared our \SkelCL based implementation to a handwritten, fine-tuned \OpenCL implementation which is based on \cite{Knitter2013} and was initially developed and described in~\cite{Haidl2011}.
The \OpenCL version is specifically designed for modern Nvidia \GPUs.
In particular, it exploits the L1 and L2 caches of the Nvidia Fermi and Kepler architecture and does not explicitly make use of the local memory.
We performed the experiments on a system with a modern Nvidia K20c Kepler \GPU with 5GB memory and 2496 compute cores.
\autoref{fig:fdtd_eval} shows the median runtimes of a simulation time of $1\,ps$ equal to $60\, 000$ iterations.
The \SkelCL version slightly outperforms the \OpenCL version by $2\%$.
The two stencil skeletons achieve ${\sim}10\%$ faster runtimes than the corresponding \OpenCL kernels but the map skeleton is ${\sim}20\%$ slower, because it reads and writes all elements exactly once, while the customized \OpenCL kernel does not write back all elements.
For this application it seems beneficial to make explicit usage of the local memory as our implementation of the Stencil skeleton does, instead of relying on the caches of the hardware, as the \OpenCL implementation does.
