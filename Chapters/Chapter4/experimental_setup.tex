\section{Experimental Setup}
\label{sec:skelcl:experimental_setup}
This section briefly discusses the evaluation metrics used in this chapter which were chosen to measure the quality of the abstractions introduced in the \SkelCL programming model and their implementation in the \SkelCL library.
We will also discuss the hardware used in the experiments.


\subsection{Evaluation Metrics}
We want to evaluate programmability, \ie, the ease of programming, and the performance of the \SkelCL programming model and library.

Measuring programmability is difficult.
Various studies~\cite{HochsteinCSAB2005,HochsteinBVG2008} have been conducted and metrics~\cite{VanderwielNL1997} have been proposed to measure how convenient a certain style of programming or a certain programming model is.
None of these metrics is widely established in the scientific or industrial communities.
We chose to use one of the simples metrics possible to quantify programming effort: counting the \emph{Lines Of source Code} (LOC).
The author wants to emphasize that this metric is not always a good representation of programmability.
A shorter program does not necessarily mean that the development of the program has been more convenient.
We will, therefore, alongside presenting the lines of code argue \emph{why} \SkelCL simplifies the programming of parallel devices, like \GPUs, as compared to the state-of-the-art approach \OpenCL.
% These discussions will argue that code written using the \SkelCL programming approach has, besides ease of programming, other properties of high quality software including:
% high level of re-use, portability, 

As the metric for performance we use absolute and relative runtime.
%-- where possible --
We only make comparisons using software executed on the same hardware.
We perform comparisons using published application and benchmark software from researchers or officially provided by Nvidia or AMD.
In addition, we compare self developed and optimized \OpenCL code versus code written using the \SkelCL library.

For all measurements we performed 100 runs and report the median runtime.

\subsection{Hardware Setup}
For performing our runtime experiments we used a PC equipped with a quad-core \CPU (Intel Xeon E5520, 2.26\,GHz) and 12\,GB of main memory.
The system is connected to a Nvidia Tesla S1070 computing system consisting of four Nvidia Tesla \GPUs.
The S1070 has 16\,GB of dedicated memory (4\,GB per \GPU) which is accessed with up to 408\,GB/s (102\,GB/s per \GPU).
Each \GPU comprises 240 streaming processor cores running at up to 1.44\,GHz.
The Linux based Ubuntu operating system was used.
The runtime experiments where conducted at different times from 2010 until 2015.
For each experiment the latest Nvidia \GPU driver available was used.

This hardware setup represents a common heterogeneous system comprising four \CPU cores and 960 \GPU streaming processor cores with a total of 28\,GB of memory.
Overall this system has a theoretical single-precision floating point performance of 4219.52 GFLOPS (4 $\times$ 1036.8 GFLOPS for the \GPUs and 72.32 GFLOPS for the \CPU).

