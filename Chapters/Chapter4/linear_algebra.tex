\section{Linear Algebra Applications}
\label{section:skelcl:evaluation:linearAlgebra}

In this section we are going to evaluate \SkelCL using two basic linear algebra applications:
\begin{itemize}
  \item the sum of absolute values of a vector and
  \item the dot product of two vectors.
\end{itemize}

Both applications are included in BLAS~\cite{Dongarra2002,Dongarra2002a}, the well known library of basic linear algebra functions.
The BLAS functions are used as basic building blocks by many high-performance computing applications.
% Especially dense matrix multiplication is an important operation found in numerous scientific applications.

Here we want to investigate how easily these applications can be expressed with skeletons in \SkelCL.
Furthermore, we are interested in the runtime performance of the \SkelCL implementations.

\subsection*{Sum of Absolute Values}
\label{sec:asum}
\autoref{eq:asum} shows the mathematical definition of the sum of absolute values (short \emph{asum}) for a vector $\vec{x}$ of length $n$ with elements $x_i$:
\begin{equation}
  asum\ \vec{x} = \sum_{i=0}^{n} | x_i |
  \label{eq:asum}
\end{equation}
For all elements of the vector the absolute values are added up to produce the final scalar result.

\subsubsection*{\SkelCL Implementation}
In the \SkelCL programming model we can express \emph{asum} using the \map and \reduce skeletons as follows:
\begin{align}
  asum\ \vec{x} &= reduce\ (+)\ 0\ \big(\ map\ (|\, .\, |)\ \vec{x}\ \big)\label{eq:asum:skelcl}\\
  \text{where:} \qquad | a | &=
    \left\{
      \begin{array}{r l}
      a & \text{if } a \geq 0\\
      -a & \text{if } a < 0
      \end{array}
    \right.\nonumber
\end{align}
The \map skeleton applies the $(|\, .\, |)$ function to each element of the input vector before the \reduce skeleton is used to sum up the elements.

The implementation of $asum$ using the \SkelCL library is shown in \autoref{lst:skelcl:asum}.
In \autoref{lst:skelcl:asum:skeletons:start}---\autoref{lst:skelcl:asum:skeletons:end} the customized skeletons are defined.
The \map skeleton in \autoref{eq:asum:skelcl} corresponds directly to \autoref{lst:skelcl:asum:skeletons:start} and \autoref{lst:skelcl:asum:abs} in \autoref{lst:skelcl:asum} where the $|\, .\, |$ function is represented using a \Cpp lambda expression.
The \autoref{lst:skelcl:asum:skeletons:sumUp} and \autoref{lst:skelcl:asum:skeletons:end} correspond directly to the \reduce skeleton in \autoref{eq:asum:skelcl}.
By applying the skeletons to the input vector (\autoref{lst:skelcl:asum:call}) the result is computed and accessed in \autoref{lst:skelcl:asum:return}.
In the \SkelCL library implementation the \reduce skeleton returns a vector containing a single element, called \code{result} in this particular case.
The containers in the \SkelCL library are implemented as \emph{futures}~\cite{HewittBa1977,FriedmanWi1978}.
This allows the computation of all skeletons to be performed asynchronously, \ie, when executing a skeleton the computation is launched and the called function returns immediately.
When accessing values of the returned container, \eg, via the array subscript operator as shown in \autoref{lst:skelcl:asum:return}, the call will block until the accessed value has been computed.
Here we could have also used the equivalent \code{front} member function for accessing the first element of the result vector.

\begin{lstlisting}[%                                                             
caption={Implementation of the \emph{asum} application in \SkelCL},%
numbers=left,%
float=tb,
label={lst:skelcl:asum}]
float asum(const Vector<float>& x) {
  skelcl::init();
  auto absAll = map($\label{lst:skelcl:asum:skeletons:start}$
      [](float a){ if (a >= 0) return a; else return -a; });$\label{lst:skelcl:asum:abs}$
  auto sumUp = reduce($\label{lst:skelcl:asum:skeletons:sumUp}$
      [](float a, float b){return a+b;}, 0);$\label{lst:skelcl:asum:skeletons:end}$
  auto result = sumUp( absAll( x ) );$\label{lst:skelcl:asum:call}$ return result[0]; }$\label{lst:skelcl:asum:return}$
\end{lstlisting}


\subsection*{Dot Product}
\label{sec:dot}
The computation of the dot product, \aka scalar product, is a common mathematical operation performed on two input vectors $\vec{x}$ and $\vec{y}$ of identical length $n$ as defined in \autoref{eq:dot_product}:
\begin{equation}
  dotProduct\ \vec{x}\ \vec{y} = \sum_{i=0}^{n} x_i \times y_i
  \label{eq:dot_product}
\end{equation}

\subsubsection*{\SkelCL Implementation}
In the \SkelCL programming model we can express $dotProduct$ using the \zip and \reduce skeletons as follows:
\begin{equation}
  dotProduct\ \vec{x}\ \vec{y} = reduce\ (+)\ 0\ \big(\ zip\ (\times)\ \vec{x}\ \vec{y}\ \big)
  \label{eq:skelcl:dot_product}
\end{equation}
The \zip skeleton performs pairwise multiplication of the input vectors before the \reduce skeleton is used to sum up the intermediate results.

\autoref{lst:skelcl:dot} shows the implementation using the \SkelCL library.
The structure of the implementation is very similar to the \emph{asum} application.
Here we use the \code{front} member function to access the first (and only) element of the computed result vector.

\begin{lstlisting}[%                                                             
postbreak=\space\space, breakautoindent=true, breakindent=84pt, breaklines,
caption={Implementation of the dot product application in \SkelCL},%
numbers=left,%
float=tb,
label={lst:skelcl:dot}]
float dotProduct(const Vector<float>& x,                  const Vector<float>& y) {
  skelcl::init();
  auto mult  = zip($\label{lst:skelcl:dot:skeletons:start}$
      [](float x, float y){return x*y;});
  auto sumUp = reduce(
      [](float x, float y){return x+y;}, 0);$\label{lst:skelcl:dot:skeletons:end}$
  return sumUp( mult( x, y ) ).front(); }$\label{lst:skelcl:dot:call}$
\end{lstlisting}





\bigskip

We now compare the \SkelCL implementations shown in \autoref{lst:skelcl:asum} and \autoref{lst:skelcl:dot} against a na{\"i}ve OpenCL implementation and an implementation using the CUBLAS library respectively.


\subsection*{Programming effort}
\autoref{fig:skelcl:sum:dot:loc} shows the lines of code for the \emph{asum} application on the left and for the dot product application on the right.
The CUBLAS implementations requires 58 respectively 65 lines in total, whereby no \GPU code is written directly.
The code contains a lengthy setup, memory allocations, memory transfers and launching the computation on the \GPU including all necessary error checks.
The na{\"i}ve \OpenCL implementations require over 90 lines each (\emph{asum}: 91, dot product: 96) for managing the \GPU execution and 20 lines of \GPU code.
By far the shortest implementations are the \SkelCL implementations with 7 lines each, as shown in \autoref{lst:skelcl:asum} and \autoref{lst:skelcl:dot}.
Here we count the lambda expressions customizing the algorithmic skeletons as \GPU code and all other lines as \CPU code.
The code is not just shorter but each single line is clear and straightforward to understand, where, \eg,  the CUBLAS API call for launching the dot product computation alone expects 7 arguments requiring to look up the documentation for using it correctly.

\begin{figure}
  \centering
  \includegraphics[width=.9\textwidth]{Plots/Asum/la_loc.pdf}
  \caption{Lines of code for both basic linear algebra applications.}
  \label{fig:skelcl:sum:dot:loc}
\end{figure}

\subsection*{Performance experiments}
\autoref{fig:skelcl:asum:dot} shows the runtime for the \emph{asum} application and the runtime for the computation of the dot product.
The figure uses a logarithmic scale on the vertical axis and shows the runtime for different sizes of the input vectors: from 4 megabyte to 512 megabyte.
These results include data transfers to and from the \GPU as well as the computation performed on the \GPU.

The results clearly show that the \SkelCL version is the slowest, followed by the na{\"i}ve \OpenCL implementation, and, finally, the implementation using CUBLAS which is the fastest.
For both applications the performance difference between the versions is roughly similar independent of the input size.
For \emph{asum} \SkelCL is between $2$ and $3.2$ times slower than the na{\"i}ve \OpenCL version and up to $7$ times slower than the CUBLAS version.
For dot product \SkelCL is on average about $2.25$ times slower than the na{\"i}ve \OpenCL version and between $3.5$ and $4$ times slower than CUBLAS.
We will discuss the reasons for the bad performance of the \SkelCL implementation in the next subsection.

The CUBLAS version is about $2$ times faster than the na{\"i}ve \OpenCL version for \emph{asum} and $1.7$ times for the dot product application.
One has to keep in mind, that these benchmark are mostly dominated by the memory transfers moving data between \CPU and \GPU which are the same in both version.
This explains the rather small difference in runtime between the na{\"i}ve \OpenCL version and the optimized CUBLAS version.

\begin{figure}[tb]
  \centering
  \includegraphics[width=\textwidth]{Plots/Asum/asum.pdf}
  \caption{Runtime for both basic linear algebra applications.}
  \label{fig:skelcl:asum:dot}
\end{figure}

\subsection*{Discussion}

\begin{figure}[tb]
  \centering
  \includegraphics[width=.9\textwidth]{Plots/Reduce/reduce.pdf}
  \caption{Relative runtime compared to CUBLAS of the na{\"i}ve \OpenCL and \SkelCL versions of \emph{asum} and \emph{sum}.}
  \label{fig:skelcl:reduce}
\end{figure}
As discussed in \autoref{chapter:skelcl} the \SkelCL library implementation generates one \OpenCL kernel for each skeleton (and two for the \reduce skeleton).
This procedure makes it difficult to \emph{fuse} multiple skeleton implementations into a single \OpenCL kernel, which would be required to achieve a competitive performance for the \emph{asum} and dot product benchmark.

To validate this explanation and quantify the effect of launching additional kernels where a single kernel would be sufficient we investigate the performance of \SkelCL's \reduce skeleton on its own.
This is similar to the \emph{asum} benchmark, but without applying the \map skeleton for obtaining the absolute value of every element, \ie, we measure the \emph{sum} benchmark:
\begin{equation}
  sum\ \vec{x} = reduce\ (+)\ 0\ \vec{x}
  \label{eq:sum}
\end{equation}
For comparison we also modified the na{\"i}ve \OpenCL implementation of \emph{asum} to not apply the absolute value function to every element of the input vector.
\autoref{fig:skelcl:reduce} shows the performance results of the na{\"i}ve \OpenCL version and the \SkelCL version for \emph{asum} on the left and \emph{sum} on the right.
The plots show the performance relative to the performance of the CUBLAS implementation of \emph{asum}.
We can see, that the performance difference for \emph{asum} and \emph{sum} it is very small (only up to $4$\%) for the na{\"i}ve \OpenCL version.
This makes sense, as the most part of the computation is spend for reducing the elements and not for applying a simple function to each element.
For the \SkelCL implementations the difference between the two benchmarks is very large.
This is due to the fact that for \emph{asum} a separate \OpenCL kernel is launched which reads each value from the global memory, applies the absolute value function to it, and stores the result back to global memory.
The reduction kernel then starts by reading each element back from global memory.
This leads to a huge performance penalty, where the \emph{asum} benchmark is up to $6.2$ times slower than the \emph{sum} benchmark.
Furthermore, we can see that for the \emph{sum} benchmark the \SkelCL implementation outperforms the na{\"i}ve \OpenCL implementation and is only between $16$\% and $37$\% slower than the CUBLAS \emph{asum} implementation.

In \autoref{ch:fifth}, we will discuss a novel compilation technique which addresses this drawback of \SkelCL.
This technique supports the generation of a single efficient \OpenCL kernel for applications like the \emph{asum} and dot product examples.

