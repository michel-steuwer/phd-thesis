\section{The SkelCL Library}
\label{section:skelcl-library}
In this section we discuss the \SkelCL Library, our implementation of the \SkelCL programming model.
It provides a \Cpp~\API that implements the features of the \SkelCL programming model, and thus liberates the application developer from writing low-level code.
In addition, the library provides some commonly used utility functions, \eg, for program initialization.
The \SkelCL Library is open source software and available at: \url{http://skelcl.uni-muenster.de}.

We start our discussion with an example showing how to use the \SkelCL library.
We describe the syntax used to represent the \SkelCL programming model introduced in the previous section.
This will include a discussion of \Cpp techniques used to implement the library.
We then shift the focus to the implementations of the memory management, algorithmic skeletons, and distributions.

\subsection{Programming with the SkelCL Library}

\autoref{lst:skelcl:dotproduct} shows the implementation of the dot product computation, discussed in the previous section, in \SkelCL.
After including the appropriate \SkelCL headers (lines~\autoref{lst:skelcl:dotproduct:inc:start}--\autoref{lst:skelcl:dotproduct:inc:end}) the \SkelCL library can be initialized as seen in line~\autoref{lst:skelcl:dotproduct:init}.
This will perform the initializations required by \OpenCL.
It is possible to provide arguments to the \code{init} function to specify which \OpenCL devices should be used by \SkelCL.
The dot product is specified using the \zip and \reduce skeletons.
The skeletons are created using \code{zipVector} (line~\autoref{lst:skelcl:dotproduct:zip}) and \code{reduce} (line~\autoref{lst:skelcl:dotproduct:reduce}) functions which expect the customizing functions of the skeletons as argument.
By using the \code{zipVector} function we specify that the \zip skeleton operates on two input vectors and not two input matrices.
In this example we use \Cpp lambda expressions (lines~\autoref{lst:skelcl:dotproduct:zip:lambda} and \autoref{lst:skelcl:dotproduct:reduce:lambda}) to specify the customizing functions of the skeletons.
We create the two input \code{Vector}s from C style pointers in this example (lines~\autoref{lst:skelcl:dotproduct:vecA} and \autoref{lst:skelcl:dotproduct:vecB}).
In line~\autoref{lst:skelcl:dotproduct:apply} we perform the computation by applying the customized skeletons to the data, before we finally return the computed result in line~\autoref{lst:skelcl:dotproduct:return}.

\begin{lstlisting}[%                                                             
caption={Implementation of the dot product computation in SkelCL},%
float=t,%
numbers=left,%
label={lst:skelcl:dotproduct}]
#include <SkelCL/SkelCL.h>$\label{lst:skelcl:dotproduct:inc:start}$
#include <SkelCL/Zip.h>
#include <SkelCL/Reduce.h>
#include <SkelCL/Vector.h>$\label{lst:skelcl:dotproduct:inc:end}$

float dotProduct(const float* a, const float* b, int n) {
  skelcl::init();$\label{lst:skelcl:dotproduct:init}$

  auto mult = skelcl::zipVector( $\label{lst:skelcl:dotproduct:zip}$
      [](float x, float y) { return x*y; } );$\label{lst:skelcl:dotproduct:zip:lambda}$
  auto sum  = skelcl::reduce($\label{lst:skelcl:dotproduct:reduce}$
      [](float x, float y) { return x+y; }, 0 );$\label{lst:skelcl:dotproduct:reduce:lambda}$

  skelcl::Vector<float> A(a, a+n);$\label{lst:skelcl:dotproduct:vecA}$
  skelcl::Vector<float> B(b, b+n);$\label{lst:skelcl:dotproduct:vecB}$

  skelcl::Vector<float> C = sum( mult(A, B) );$\label{lst:skelcl:dotproduct:apply}$

  return C.front();$\label{lst:skelcl:dotproduct:return}$
}
\end{lstlisting}

\autoref{lst:skelcl:dotproduct} shows that the \SkelCL library integrates nicely with \Cpp.
The interface of the \code{Vector} class looks familiar to \Cpp programmers and the usage of modern \Cpp features like lambda expressions and type deduction (\code{auto}) simplify the programming:
functions can be defined directly inline and type information can be omitted.

We will discuss how our implementation enables this level of integration in the next section.


\subsection{Syntax and Integration with \Cpp}
\label{section:skelcl-library:syntax}

The \SkelCL library is built on top of \OpenCL.
This has benefits as well as introduces technical challenges.
One of the technical challenges is, that \OpenCL requires the kernel to be specified as a string in the host program.
While this enables portability across different hardware architectures, it also introduces a burden on the application developer as strongly typing cannot be guarantied statically when the host program is compiled.
For the implementation of \SkelCL we choose to address this issue using a two-step implementation.
In the first step a custom compiler transforms the source code as seen in \autoref{lst:skelcl:dotproduct} to a representation where the kernel computations are represented as strings as required by \OpenCL.
In the second step the transformed program is compiled with a traditional \Cpp compiler to produce the final executable.

This allows us to free the user from writing strings in their application code and maintain the usual type safety guarantees from \Cpp at compile time.
Furthermore, we implemented type inference capabilities in our source-to-source compiler to free the application developer from specifying type information explicitly.
Our two-step design also allows application developers to compile their application code into a form which then can be deployed on systems where it might be difficult to install a custom compiler.

In the next paragraph we will start the discussion of our source-to-source compiler tool.
We will discuss how our custom compiler, together with our template-based implementation helps to maintain strong type safety at compile time.
Finally, we will look at issues regarding the integration with \Cpp like sharing code between host and device code.

\paragraph{The custom SkelCL compiler: \code{skelclc}}
To allow a deep integration of \SkelCL code with \Cpp we implemented a custom compiler: \code{skelclc}.
We leverage the LLVM infrastructure~\cite{Lattner2004} for building the compiler.
LLVM and the related Clang project offer well defined application programming interfaces for writing C and \Cpp related compiler tools.
In particular the \emph{LibTooling} API allows for writing tools which search for particular patterns in the abstract syntax tree (AST) which is representing the source code and perform actions on the found code fragments.
The \code{skelclc} tool does exactly this.
For example the tool searches for expressions like this:
\begin{lstlisting}[numbers=none,label={lst:skelclc:before},caption={Code snippet before transformation.}]
auto mult = skelcl::zipVector(
              [](float x, float y){ return x*y; } );
\end{lstlisting}
and replaces it with:
\begin{lstlisting}[numbers=none,label={lst:skelclc:after},caption={Code snippet after transformation.}]
auto mult = skelcl::Zip<Vector<float>(Vector<float>,
                                      Vector<float>)>(
              skelcl::Source("float func(float x, float y)"\
                             " { return x*y; }"), "func");
\end{lstlisting}

In this example the \code{zipVector} function has been transformed into an call of the constructor of the \code{Zip} class and the lambda expression has been transformed into a string.
Furthermore, type information have been added to the templated \code{Zip} class, which have been inferred from the lambda expression.

The user is free to write the expression in \autoref{lst:skelclc:after} explicitly, but it is arguably more convenient to write the expression in \autoref{lst:skelclc:before} and let our custom compiler perform the transformation automatically.

For every skeleton available in \SkelCL there exist a function like \code{zip} which is transformed by \code{skelclc} to a call of the constructor of the corresponding skeleton class.

\paragraph{Maintaining type safety}
Each skeleton in \SkelCL is represented by a templated class, as seen in \autoref{lst:skelclc:after} for the \zip skeleton.
The template arguments of the skeleton define the types which can be used when executing the skeleton.
When executing a skeleton \code{skeleton<T(U)>} an object of type \code{U} has to be provided to receive an object of type \code{T}.
In this respect skeletons behave exactly like functions in \Cpp which can be represented using the \code{std::function<T(U)>} template class.
Skeletons taking more then one argument (like the \code{zip} skeleton) are represented as \code{skeleton<T(U, V)>} using the \emph{variadic template} feature of \Cpp.

When performing the transformations \code{skelclc} infers the types used as template arguments of the skeleton class.
To do so, it determines the types of the lambda expression arguments and its result type.
For \autoref{lst:skelclc:before} this is: \code{float} and \code{float} as argument types and \code{float} as result type.
The result type is inferred following standard \Cpp rules.
Based on this types the template arguments of the skeleton are constructed.

The skeletons \map, \zip, and \stencil can operate on either \code{Vector} or \code{Matrix}.
For each of these skeletons there exists three kinds of factory functions:
1)~for creating a skeleton operating on vectors, \eg \code{mapVector};
2)~for creating a skeleton operating on matrices, \eg \code{mapMatrix};
3)~for creating a skeleton capable of operating on both vectors and matrices, \eg \code{map}.
To allow the last case the templated class \code{Container} was introduced.
Using the template specialization mechanism, the mentioned skeletons have a special implementation when \code{Container} is used as a template argument.
In this case the classes provide two distinct overloaded member functions for execution using either \code{Vector} or \code{Matrix}.

Type safety is guarantied, as the templated skeleton classes can only be executed with a container of compatible type and the type inference ensures that the user function has exactly the same type as the template argument of the skeleton class.
Therefore, applying the user function to the input data is always a type safe operation.

Because skeletons are strongly types the composition of skeletons is type safe as well, \ie, in \autoref{lst:skelcl:dotproduct} type safety between the two skeletons is guaranteed.
If the result type of \code{zip} would not match the input type of \code{reduce} a compile time error would occur.

\paragraph{Integration with \Cpp}
To allow a deep integration with \Cpp the customizing function is allowed to make use of user defined types and make calls to other functions.
The \code{skelclc} compiler detects these behaviors and ensures that the definition of the user defined type and the source code of all functions called is included in the source code passed to the skeleton.
This frees the user from providing type definitions and source code twice, which is required when using OpenCL directly or SkelCL without the \code{skelclc} compiler.
Code duplication should almost always be avoided as it can easily lead to hard maintainable code and subtle bugs.

In real-world applications (\eg, the LM OSEM discussed in detail in Section~\ref{section:medical-imaging}), user-defined functions often work not only on a skeleton's input vector, but may also take additional inputs.
With only a fixed number of input arguments, traditional skeletons would not be applicable for the implementation of such applications.
In \SkelCL skeletons can accept an arbitrary number of additional arguments which are passed to the skeleton's customizing function.

\autoref{lst:saxpy:addArgs} shows an example implementation of the \emph{single-precision real-alpha x plus y} ($saxpy$) computation -- a commonly used BLAS routine -- in \SkelCL.
\begin{lstlisting}[%
  caption={The BLAS $saxpy$ computation using a \zip skeleton with additional arguments},%
  float=tbp,%
  label={lst:saxpy:addArgs}]
float a = initScalar();

/* create skeleton Y <- a * X + Y */
auto saxpy = skelcl::zipVector(
      [](float x, float y, float a) { return a*x+y; });

/* create input vectors */
Vector<float> X(SIZE); initVector(X);
Vector<float> Y(SIZE); initVector(Y);

Y = saxpy( X, Y, a );      /* execute skeleton */ $\label{lst:saxpy:addArgs:execution}$
\end{lstlisting}
$saxpy$ is a combination of scalar multiplication of $a$ with vector $X$ followed by vector addition with $Y$.
In \autoref{lst:saxpy:addArgs}, the computation is implemented using a \zip skeleton:
vectors $X$ and $Y$ are passed as input, while factor $a$ is passed to the customizing function as an additional argument.
The additional argument is simply appended to the argument list when the skeleton is executed (line~\ref{lst:saxpy:addArgs:execution}).
Besides scalar values, like shown in the example, vectors and matrices can also be passed as additional arguments to a skeleton.
When vectors or matrices are used as additional arguments, the user is responsible to ensure that no race conditions occur when writing to or reading from the container.
If the container is only used for reading it is guaranteed that no race conditions will occur.

The additional argument feature is implemented in SkelCL using the variadic template feature from \Cpp.

In \Cpp lambda expressions can make use of variables which where accessible at the time the lambda expression is created.
The user has to explicitly indicate how these variable are \emph{captured},\ie how they will be accessed once the lambda is executed.
A variable can either be captured \emph{by-value} or \emph{by-reference}.
If a variable is captured by-value a copy of the variable is made at the time the lambda expression is created and accessed later when the lambda expression is executed.
If a variable is captured by-reference the original variable is accessed when the lambda expression is executed through a reference.
Capturing a variable by reference allows the user to change the content of the variable from inside the lambda expression, furthermore, in an concurrent setting the user has to ensure that the lifetime of the variable exceeds the time when the lambda expression is executed.

In \SkelCL the customizing function can be expressed as a lambda expression capturing variables.
The capturing of variables by-value is fully supported by the \code{skelclc} compiler and the capturing by-reference is supported with the following restrictions:
By-reference capturing is only allowed if the reference is used to read from the variable only, writing to the captured variable is forbidden.
The reason for this behavior is that when executing the lambda expression on the GPU a write to a by-reference captured variable requires a costly data transfer to ensure the variable, stored in CPU memory, is modified.
Furthermore, as the lambda expression will be executed multiple times in parallel as part of the skeletons execution it is likely that race conditions occur when writing to the captured reference.
We can rewrite the $saxpy$ example using a lambda expression capturing the variable $a$ as shown in \autoref{lst:saxpy:capture}.
\begin{lstlisting}[%
  caption={The BLAS $saxpy$ computation using a \zip skeleton customized with a lambda expression capturing the variable $a$.},%
  float=tbp,%
  label={lst:saxpy:capture}]
float a = initScalar();

/* create skeleton Y <- a * X + Y */
auto saxpy = skelcl::zipVector(
      [a](float x, float y) { return a*x+y; }); $\label{lst:saxpy:capture:capture}$

/* create input vectors */
Vector<float> X(SIZE); initVector(X);
Vector<float> Y(SIZE); initVector(Y);

Y = saxpy( X, Y );      /* execute skeleton */ $\label{lst:saxpy:capture:execute}$
\end{lstlisting}

The variable $a$ is now captured by the lambda expression in line~\ref{lst:saxpy:capture:capture}.
Note that $a$ is no longer passed as an argument when executing the skeleton in line~\ref{lst:saxpy:capture:execute}.

\subsection{Skeleton Execution on OpenCL Devices}
\label{section:skelcl-library:execution}
The process of executing a skeleton on an OpenCL device, \eg a GPU, follows always the same steps independent of the skeletons involved.
This process is described in this subsection before we will look at the individual skeleton implementations in the next subsection.

The \code{skelclc} compiler transforms the \SkelCL application code so that the source code of the customizing function of a skeleton is available to the \SkelCL library implementation as a string.
When a skeleton instance is created the \SkelCL library implementation performs the following steps to create an OpenCL kernel:
1)~the source code of the customizing function is merged with an OpenCL implementation of the skeleton;
2)~the merged source code is adapted into the final OpenCL kernel to support additional arguments and so that names and types of the customizing function and skeleton implementation match;
3)~the created OpenCL kernel is cached to a file to avoid performing steps 1) and 2) multiple times for the same kernel.

On execution of the skeleton the following steps are performed to execute the computation on an OpenCL device:
1)~the data of the input containers is uploaded to the OpenCL device, if it is not already there;
2)~the OpenCL kernel arguments are set, including the additional arguments;
3)~the OpenCL kernel is executed.

In the next two paragraphs we will look closer at these two processes and their individual steps.

\paragraph{Create an OpenCL kernel}
We will use the $saxpy$ example from the previous section as a running example to explain how an OpenCL kernel is created.
\autoref{lst:saxpy:transformed} shows the code for the $saxpy$ application emitted by the \code{skelclc} compiler after receiving the code shown in \autoref{lst:saxpy:addArgs} as input.
\begin{lstlisting}[%
  caption={Source code for the $saxpy$ application emitted by the \code{skelclc} compiler.},%
  float=tbp,%
  label={lst:saxpy:transformed}]
float a = initScalar();

/* create skeleton Y <- a * X + Y */
auto saxpy = skelcl::Zip<Vector<float>(Vector<float>,
                                       Vector<float>,
                                       float)>(
     skelcl::Source("float func(float x, float y, float a)"$\label{lst:saxpy:transformed:funcStart}$\
                    " { return a*x+y; }"), "func"); $\label{lst:saxpy:transformed:funcEnd}$

/* create input vectors */
Vector<float> X(SIZE); initVector(X);
Vector<float> Y(SIZE); initVector(Y);

Y = saxpy( X, Y, a );      /* execute skeleton */
\end{lstlisting}

To create the corresponding OpenCL kernel the source code of the customizing function in lines~\ref{lst:saxpy:transformed:funcStart} and \ref{lst:saxpy:transformed:funcEnd} is combined with the prototype implementation of the \zip skeleton which is shown in \autoref{lst:zip:impl}.
%
\begin{lstlisting}[%
  caption={Prototype implementation of the \zip skeleton in OpenCL.},%
  float=tbp,%
  label={lst:zip:impl}]
typedef int T0; typedef int T1; typedef int T2;$\label{lst:zip:impl:typedef}$

kernel void ZIP(const global T0* left,
                const global T1* right,
                      global T2* out,
                const unsigned int size) {
  unsigned int id = get_global_id(0);
  if (id < size) { $\label{lst:zip:impl:boundaryCheck}$
    out[id] = USER_FUNC(left[id], right[id]); } }  $\label{lst:zip:impl:call}$
\end{lstlisting}
%
The prototype implementation defined an OpenCL kernel \code{ZIP} receiving three pointers to global memory and one integer as arguments.
A boundary check is performed in line~\ref{lst:zip:impl:boundaryCheck} before a function with the name \code{USER\_FUNC} is called (line~\ref{lst:zip:impl:call}) with a pair of elements read from the two input arrays \code{left} and \code{right}.
The result of the function call is stored in the output array \code{out}.

The combined source codes clearly do not form a valid OpenCL kernel as there exists no function called \code{USER\_FUNC}.
This \emph{prototype} implementation has to be adapted to be a valid OpenCL kernel.
To do this adaption the \SkelCL library implementation makes use of the same LLVM and Clang infrastructure used to build the \code{skelclc} compiler.
Three steps are performed to create a valid OpenCL kernel:
1)~the customizing function is renamed into \code{USER\_FUNC};
2)~the types of the customizing functions are used to change the \code{typedef}s in line~\ref{lst:zip:impl:typedef} so that the types of the kernel function \code{ZIP} match;
3)~the additional arguments of the customizing function are appended to the parameter list of the kernel function \code{ZIP} and the call to the customizing function is adapted to forward the arguments.

After these steps the source code has been transformed into a valid OpenCL kernel as shown in \autoref{lst:zip:mergedImpl}.
\begin{lstlisting}[%
  caption={OpenCL implementation of the \zip skeleton customized for performing the $saxpy$ computation.},%
  float=tbp,%
  label={lst:zip:mergedImpl}]
float USER_FUNC(float x, float y, float a) { return a*x+y; }
typedef float T0; typedef float T1; typedef float T2;

kernel void ZIP(const global T0* left,
                const global T1* right,
                      global T2* out,
                const unsigned int size, float a) {
  unsigned int id = get_global_id(0);
  if (id < size) {
    out[id] = USER_FUNC(left[id], right[id], a); } }
\end{lstlisting}

Performing the adaption of the source code involves parsing the code and building an abstract syntax tree for it by the clang compiler library.
Performing this every time a skeleton is created is wasteful as this task can take up to several hundreds of milliseconds, which can be a considerable overhead for small kernels.
Therefore, the \SkelCL library implements a caching of already transformed kernels to disk.
If the same kernel is used again the transformed code is loaded from the cache.
We observed that loading kernels from disk is at least five times faster than creating them from source.


\paragraph{Execute a Skeleton on an OpenCL Device}
On execution of the skeleton the created OpenCL kernel is executed on possibly multiple OpenCL devices.
The data distribution of the input containers determine what OpenCL devices are used for the execution.
If the copy, block, or overlap distribution is used all available devices are involved in the computation as defined by the distributions.
If the single distribution is used just a single device is used.
If no distribution is set for the input containers each skeleton selects a default distribution.

The first step for execution is to upload the data of the input containers to the OpenCL devices.
Before performing a data transfer the \SkelCL implementation checks if the transfer is necessary or if the data is already up to date on the receiving side.
The details how this is implemented in the memory management are described in \autoref{section:skelcl-library:memory-management}.

Before enqueuing the OpenCL kernel in the OpenCL command queue its kernel arguments have to be set.
First the regular arguments are set corresponding to the order of arguments as defined in the skeletons prototype implementation of the OpenCL kernel.
Afterwards, the additional arguments are set.

Finally, the OpenCL kernel is enqueued with a global size corresponding to the size of the input containers.

\subsection{Algorithmic Skeletons Implementation}
\label{section:skelcl-library:skeletons}
The \SkelCL library implements each skeleton of the \SkelCL programming model using one or more OpenCL kernels.
In this subsection we briefly discuss the implementation of each skeleton targeting single and multi device systems.

\subsubsection{The \map Skeleton}
The OpenCL kernel implementing the \map skeleton is straightforward and similar to the implementation of the \zip skeleton shown in \autoref{lst:zip:impl}.
After a boundary check is performed the customizing function is called with an item of the input container.
The return value is stored in the output container.
On execution the size of the input container determines the number of work items launched executing the kernel code.
The same kernel code is used on single and multi device systems.
If the input container has no distribution the block distribution is used as default ensuring that all devices participate in the execution of the \map skeleton.

% There exists special implementations of the \map Skeleton ... IndexVector

\subsubsection{The \zip Skeleton}
The OpenCL kernel of the \zip skeleton was presented in the previous section in \autoref{lst:zip:impl}.
For multi device systems the block distribution is used as default.
The \SkelCL library implementation enforces that the distributions of the two input vectors are the same.
If this is not the case the distribution of one input is changed to meet this requirement.

\subsubsection{The \reduce Skeleton}
The \reduce skeleton is implemented as a parallel tree based reduction.
Two OpenCL kernels are used for the implementation.
The first kernel implements a reduction where each work group reduces a large number of elements to a single element.
This kernel is executed by multiple work groups in parallel to fully exploit the GPU.
The second kernel is only executed by a single work group and continues the reduction of the intermediate results from the first kernel.
This two kernel strategy is required for an efficient implementation in OpenCL as no synchronization between work items from different work groups is possible.

On a multi device system when using the block distribution each device performs a partial reduction of the data available in its memory.
Then the intermediate results are transfered to the first device which performs the final reduction.
The output of the \reduce skeleton is a vector with a single element which is single distributed on the first device.

\subsubsection{The \scan Skeleton}
The implementation of the \scan skeleton follows the implementation presented in~\cite{HarrisSeOw2007}.
This is a two phase algorithm where each phase is implemented as an OpenCL kernel.

The execution of the \scan skeleton on multi device systems is visualized in \autoref{fig:scan:impl}.
Here an example is shown where the prefix sum is computed on a four device system using the \scan skeleton.
The input vector with the values $[1,\ldots,16]$ is distributed using the \emph{block} distribution by default.
This is shown in the top line.
After performing the scan algorithm on all devices (second line of the figure), map skeletons are built implicitly using the marked values and executed on all devices except the first one.
This produces the final result, as shown in the bottom line.

\begin{figure*}[tbp]
    \centering
    \includegraphics[width=.9\textwidth]{ASHES/scan}
    \caption{Implementation of the \scan skeleton visualized for four GPUs:
            (1) All GPUs scan their parts independently.
            (2) map skeletons are created automatically and
             executed to produce the result.}
    \label{fig:scan:impl}
\end{figure*}

The \SkelCL implementation of the \scan skeleton assumes the \GPUs to have a fixed order, such that each \GPU (except the first one) has a predecessor:
\begin{enumerate}
 \item Every \GPU executes a local scan algorithm for its local part of data;
 \item The results of all \GPUs are downloaded to the \CPU;
 \item For each \GPU (except the first one), a map skeleton is implicitly created that combines the result of the \GPU's predecessors with all elements of its part using the user-defined operation of the \scan skeleton;
 \item The newly created map skeletons compute the final results on all \GPUs.
\end{enumerate}
The output vector is block-distributed among all \GPUs.


\subsubsection{The \stencil Skeleton}
\todo{Stencil Skeleton implementation ...}

\subsubsection{The \allpairs Skeleton}
\todo{Allpairs Skeleton implementation ...}
In \autoref{section:skelcl-programming-model} we defined two variants of the \allpairs skeleton.
The generic \allpairs skeleton and the specialized \allpairs skeleton.
In this subsubsection we will first discuss the generic and then the specialized implementation.
We will also estimate the performance benefit gained by the specialized implementation.
Finally, we will discuss the implementation of the \allpairs skeleton on multi device systems.

\paragraph{The Generic \allpairs Skeleton}
Let us assume targeting \GPU architectures and estimate the number of global (and therefore expensive) memory accesses required for computing an element of the matrix multiplication in the generic case.
One global memory read access for every element of both input vectors is performed, and a single global memory write access is required to write the result into the output matrix.
Therefore, 
\begin{equation}
  n\cdot m\cdot (d + d + 1)
  \label{eq:mm:accesses}
\end{equation}
global memory accesses are performed in total, where $n$ and $m$ are the height and width of matrix $C$ and $d$ is the width of $A$ and the height of $B$.
By using the fast but small local memory this number of global memory accesses can be reduces and, thus, performance can be improved.
Using the local memory for matrix matrix multiplication is a well known optimization.

\paragraph{The Specialized \allpairs Skeleton}
By expressing the customizing function of the \allpairs skeleton as a \zip-\reduce composition, we provide additional semantic information about the memory access pattern of the customizing function to the skeleton implementation, thus allowing for improving the performance.
Our idea of optimization is based on the OpenCL programming model that organizes \emph{work-items} (i.\,e., threads executing a kernel) in \emph{work-groups} which share the same \GPU local memory.
By loading data needed by multiple work-items of the same work-group into the local memory, we can avoid repetitive accesses to the global memory.

\begin{figure}[b]
  \centering
  \includegraphics[width=.4\textwidth]{HLPP/memory_access}
  \caption{Implementation schema of the specialized allpairs skeleton.}
  \label{fig:memory_access}
\end{figure}
For the \allpairs skeleton with the \zip-\reduce customizing function, we can adopt the implementation schema for \GPUs~\cite{SarjeAl2013}, as shown in Figure~\ref{fig:memory_access}.
We allocate two arrays in the local memory, one of size $r\times k$ ($r=2$, $k=3$ in Figure~\ref{fig:memory_access}) for elements of $A$ and one of size $k\times c$ ($c=3$ in Figure~\ref{fig:memory_access}) for elements of $B$.
A work-group consisting of $c\times r$ work-items computes $s$ blocks ($s=2$ in Figure~\ref{fig:memory_access}) of the result matrix $C$.
In Figure~\ref{fig:memory_access}, the two blocks marked as \circled{7} and \circled{8} are computed by the same work-group as follows.
In the first iteration, the elements of blocks \circled{1} and \circled{2} are loaded into the local memory and combined following the zip-reduce pattern.
The obtained intermediate result is stored in block \circled{7}.
Then, the elements of block \circled{3} are loaded and combined with the elements from \circled{2} which still reside in the local memory.
The intermediate result is stored in block \circled{8}.
In the second iteration, the algorithm continues in the same manner with blocks \circled{4}, \circled{5}, and \circled{6}, but this time, the elements of the blocks are also combined with the intermediate results of the first iteration, which are stored in blocks \circled{7} and \circled{8}.
The advantage of computing multiple blocks by the same work-group is that we keep the elements of $B$ in the local memory when computing the intermediate results, \ie, we do not reload block \circled{2} twice for the computation of blocks \circled{7} and \circled{8}.

Every element loaded from the global memory is used by multiple work-items:
\eg, the upper left element of block \circled{1} is loaded only once from the global memory, but used three times:
in the computation of the upper left, upper middle, and upper right elements of \circled{7}.
In general, every element loaded from $A$ is reused $c$ times, and every element from $B$ is reused $r\cdot s$ times.
As the intermediate results are stored in the global memory of matrix $C$, we perform two additional memory accesses (read/write) for every iteration, \ie, $2\cdot \frac{d}{k}$ in total.
Therefore, instead of $n\cdot m\cdot (d + d + 1)$ (see \autoref{eq:mm:accesses}) global memory accesses necessary when using the non-specialized skeleton and, thus, only the global memory, only
\begin{equation}
  n\cdot m\cdot (\frac{d}{r\cdot s} + \frac{d}{c} + 2\cdot \frac{d}{k})
\end{equation}
global memory accesses are performed.
By increasing the parameters $s$ and $k$, or the number of work-items in a work-group ($c$ and $r$), more global memory accesses can be saved.
However, the work-group size is limited by the \GPU hardware.
While the parameters can be chosen independently of the matrix sizes, we have to consider the amount of available local memory.
\cite{Friese2013}~and~\cite{SarjeAl2013}~discusses how suitable parameters can be found by performing runtime experiments.
In~\cite{Friese2013} the parameters $c = 32$, $r=8$, $s=32$, and $k=64$ are used on modern \GPU hardware showing good performance.

We will report measurements of the performance difference for the two skeleton implementations on real hardware in \autoref{chapter:skelcl:apps}.

\paragraph{The \allpairs skeleton using multiple devices}
\label{sec:allpairs:multi_gpu}
\begin{figure}[b]
  \centering
  \includegraphics[width=.3\textwidth]{HLPP/multi_gpu}
  \caption{Data distributions used for a system with two GPUs: matrices $A$ and $C$ are \emph{block} distributed, matrix $B$ is \emph{copy} distributed.}
  \label{fig:multi_gpu}
\end{figure}

The \allpairs skeleton can be efficiently implemented not only on systems with a single \GPU, but on multi-\GPU systems as well.
The necessary data distribution can be easily expressed using two of \SkelCL's \emph{distributions}, as shown in \autoref{fig:multi_gpu}:
Matrix $B$ is \emph{copy} distributed, \ie, it is copied entirely to all \GPUs in the system.
Matrix $A$ and $C$ are \emph{block} distributed, \ie, they are row-divided into as many equally-sized blocks as \GPUs are available;
each block is copied to its corresponding \GPU.
Following these distributions, each \GPU computes one block of the result matrix $C$.
In the example with two GPUs shown in \autoref{fig:multi_gpu}, the first two rows of $C$ are computed by \GPU 1 and the last two rows by \GPU 2.
The \allpairs skeleton automatically selects these distributions, therefore, the same source code can be used when using a single \GPU or multiple \GPUs.


\subsection{Distribution Implementation}
\label{section:skelcl-library:distribution}

% TODO: ...


%\begin{itemize}
%  \item 
%    In a SkelCL program, a map skeleton is created as an object for a unary function $f$, e.\,g. negation, like this:
%
%    \vspace{.5em}
%    \centerline{\lstinline!Map<float(float)> neg("float func(float x) \{ return -x;\}");!}
%    \vspace{.5em}
%
%    This map object can then be called as a function with a vector as argument:
%
%    \vspace{.5em}
%    \centerline{\lstinline!resultVector = neg( inputVector );!}
%    \vspace{.5em}
%  \item The zip skeleton operates on two vectors $cl$ and $cr$, applying a binary customizing operator $\oplus$ pairwise:
%    \vspace{-.5em}
%    \[ zip\ (\oplus)\ [cl_1, cl_2, \dots, cl_n]\ [cr_1, cr_2, \dots, cr_n] = [cl_1\oplus cr_1, cl_2\oplus cr_2, \dots, cl_n\oplus cr_n] \]
%
%    \vspace{-.5em}
%    In SkelCL, a zip skeleton object for adding two vectors is created like as:
%
%    \vspace{.5em}
%    \centerline{\lstinline!Zip<float(float, float)> add("float func(float x,float y) \{return x+y;\}");!}
%    \vspace{.5em}
%
%    and can then be called as a function with a pair of vectors as arguments:
%
%    \vspace{.5em}
%    \centerline{\lstinline!resultVector = add( leftVector, rightVector );!}
%    \vspace{.5em}
%    
%  \item The reduce skeleton computes a scalar value from a vector using a binary associative operator $\oplus$, i.\,e.
%    \vspace{-.5em}
%    \[ red\ (\oplus)\ [v_1, v_2, \dots, v_n] = v_1 \oplus v_2 \oplus \dots \oplus v_n \]
%
%    \vspace{-.5em}
%    For example, to sum up all elements of a vector, the reduce skeleton is created with addition as customizing function, and called as follows:
%
%    \vspace{.5em}
%    \centerline{\lstinline!Reduce<float(float)> sumUp("float func(float x,float y) \{ return x+y;\}");!}
%    \vspace{.5em}
%    \centerline{\lstinline! result = sumUp( inputVector );!}
%    \vspace{.5em}
%
% 
%  \item The scan skeleton (a.\,k.\,a. prefix-sum) yields an output vector with each element obtained by applying a binary associative operator $\oplus$ to the elements of the input vector up to the current element's index, i.\,e.
%    \vspace{-.5em}
%    \[ scan\ (\oplus)\ [v_1, v_2, \dots, v_n] = [v_1, v_1\oplus v_2, \dots, v_1\oplus v_2\oplus \dots \oplus v_n] \]
%
%    \vspace{-.5em}
%    The prefix sums customized with addition is specified and called in SkelCL as follows:
%
%    \vspace{.5em}
%    \centerline{\lstinline!Scan<float(float)> prefixSum("float func(float x,float y) \{return x+y;\}");!}
%    \vspace{.5em}
%    \centerline{\lstinline! result = prefixSum( inputVector );!}
%    \vspace{.5em}
%
%\end{itemize}


% HIPS
The parallelization approach for \texttt{Reduce} depends on whether the binary operator is commutative and/or associative.
SkelCL requires the operator to be associative, such that it can be applied to arbitrarily sized subranges of the input vector in parallel.
The final result is obtained by recursively combining the intermediate results for the subranges.
To improve the performance, SkelCL saves the intermediate results in the device's fast local memory.


% additional arguments
For flexibility, \SkelCL skeletons can accept additional arguments if the customizing function works not only on a skeleton's input containers, but needs access to additional data~\cite{StKG-11};
containers passed as additional arguments to a skeleton are automatically transfered to the GPUs.

% opencl interop
SkelCL can also be used in combination with existing OpenCL codes, as SkelCL is designed as an extension of OpenCL, rather than a replacement for it.

% front end compiler

% skeleton implementation
In OpenCL, kernels are compiled at runtime of the host program in order to be executable on different GPUs.
Therefore, in the SkelCL library implementation, the customizing functions are provided as strings to their skeletons.
SkelCL implementation merges the customizing function with the pre-implemented skeleton-specific program code to build a valid OpenCL kernel automatically.
The generated kernel fetches one or more data items from its input containers (vectors or matrices), passes them to the customizing function, and yields the function's result, e.\,g., by writing it to the output container.
Rather than working with pointers to GPU memory (like kernels do), customizing functions in SkelCL take a single data item as input and return a single result.
The SkelCL implementation of the vector container resembles the interface of the vector from the C++ Standard Template Library (STL), i.\,e., it can be used as a replacement for the standard vector.
Internally, the containers manage pointers to the corresponding areas of the main memory (accessible by the CPU) and GPU memory.
For possible optimizations of the kernel's source code, we rely on the optimization capabilities of the OpenCL compiler.

% data transfers
In some situations, our SkelCL implementation can optimize data transfers:
\eg, after executing a skeleton, the output data remains in the GPU memory;
this has the advantage that if the output container is used as the input to another skeleton, no data transfer has to be performed.
Such \emph{lazy copying} implemented in SkelCL minimizes costly data transfers between the CPU and GPUs.



%% MapOverlap / Stencil implementation

% In the actual source code, the application developer provides the function $f$ which receives a pointer to the element in the middle, $m_{in}[i,j]$.
% Listing~\ref{lst:mapoverlap01} shows a simple example of computing the sum of all direct neighboring values using the MapOverlap skeleton.
% To access the elements of the input matrix $m_{in}$, function \texttt{get} is used, as provided by SkelCL.
% All indices are specified relative to the middle element $m_{in}[i,j]$, therefore, for accessing this element the function call \texttt{get(m\_in, 0, 0)} is used.

% The application developer must ensure that only elements in the range specified by the second argument $d$ of the MapOverlap skeleton, are accessed.
% In Listing~\ref{lst:mapoverlap01}, range is specified as $d=1$, therefore, only direct neighboring elements are accessed.
% To enforce this property, boundary checks are performed at runtime by the \texttt{get} function.
% In future work, we plan to avoid boundary checks at runtime by statically proving that all memory accesses are in bounds, as it is the case in the shown example.


% \begin{lstlisting}[%
% caption={MapOverlap skeleton computing the sum of all direct neightbors for every element in a matrix},%
% float=tbp,%
% label={lst:mapoverlap01}]
% MapOverlap<float(float)> m("float func(float* m_in){
%      float sum = 0.0f;
%      for (int i = -1; i < 1; ++i)
%        for (int j = -1; j < 1; ++i)
%          sum += get(m_in, i, j);
%      return sum;
%    }", 1, SCL_NEUTRAL, 0.0f);
% \end{lstlisting}

% Listing~\ref{lst:raw_opencl01} shows how the same simple calculation can be performed in standard OpenCL.
% While the amount of lines of code increases by a factor of 2, the complexity of each single line also increases, as follows.
% Besides a pointer to the output memory, the width of the matrix has to be provided as parameter.
% The correct index has to be calculated for every memory access using an offset and the width of the matrix.
% Therefore, knowledge about how the two-dimensional matrix is stored in one-dimensional memory is required.
% In addition, manual boundary checks have to be performed to avoid faulty memory accesses.\bigskip

% \begin{lstlisting}[%
% caption={An OpenCL kernel performing the same calculation as the MapOverlap skeleton shown in Listing~\ref{lst:mapoverlap01}.\bigskip},%
% label={lst:raw_opencl01}]
% __kernel void sum_up(__global float* m_in,
%                      __global float* m_out,
%                      int width, int height) {
%   int i_off = get_global_id(0); int j_off = get_global_id(1);
%   float sum = 0.0f;
%   for (int i = i_off - 1; i < i_off + 1; ++i)
%     for (int j = j_off - 1; j < j_off + 1; ++j) {
%       // perform boundary checks
%       if ( i < 0 || i > width || j < 0 || j > height )
%         continue;
%       sum += m_in[ j * width + i ];     }
%   m_out[ j_off * width + i_off ] = sum; }
% \end{lstlisting}

% SkelCL avoids all these low-level details.
% Neither additional parameter, nor index calculations or manual boundary checks are necessary.
% In SkelCL, the application developer only provides the source code implementing the steps required by the algorithm.


% In the actual source code, the application developer provides the function $f$ which receives a pointer to the element in the middle, $m_{in}[i,j]$.

% \begin{lstlisting}[%
% caption={MapOverlap skeleton computing the sum of all direct neightbors for every element in a matrix},%
% float=bp,%
% label={lst:mapoverlap01}]
% MapOverlap<float(float)> m("float func(float* m_in){
% float sum = 0.0f;
% for (int i = -1; i < 1; ++i)
%   for (int j = -1; j < 1; ++i)
%     sum += get(m_in, i, j); return sum;
% }", 1, SCL_NEUTRAL, 0.0f);
% \end{lstlisting}


% Listing~\ref{lst:mapoverlap01} shows a simple example of computing the sum of all direct neighboring values using the MapOverlap skeleton.
% To access the elements of the input matrix $m_{in}$, function \texttt{get} is provided by SkelCL.
% All indices are specified relative to the middle element $m_{in}[i,j]$; therefore, for accessing this element the function call \texttt{get(m\_in, 0, 0)} is used.
% The application developer must ensure that only elements in the range specified by the second argument $d$ of the MapOverlap skeleton, are accessed.
% In Listing~\ref{lst:mapoverlap01}, range is specified as $d=1$, therefore, only direct neighboring elements are accessed.
% To enforce this property, boundary checks are performed at runtime by the \texttt{get} function.
% In future work, we plan to avoid boundary checks at runtime by statically proving that all memory accesses are in bounds, as it is the case in the shown example.

% \begin{lstlisting}[%
% caption={An OpenCL kernel performing the same calculation as the MapOverlap skeleton shown in Listing~\ref{lst:mapoverlap01}.},%
% float=tbp,%
% label={lst:raw_opencl01}]
% __kernel void sum_up(__global float* m_in,
%                      __global float* m_out,
%                      int width, int height) {
%   int i_off = get_global_id(0); 
%   int j_off = get_global_id(1);
%   float sum = 0.0f;
%   for (int i = i_off - 1; i < i_off + 1; ++i)
%     for (int j = j_off - 1; j < j_off + 1; ++j) {
%       // perform boundary checks
%       if ( i < 0 || i > width || j < 0 || j > height )
%         continue;
%       sum += m_in[ j * width + i ];     }
%   m_out[ j_off * width + i_off ] = sum; }
% \end{lstlisting}


% Listing~\ref{lst:raw_opencl01} shows how the same simple calculation can be performed in standard OpenCL.
% While the amount of lines of code increases by a factor of 2, the complexity of each single line also increases:
% 1) Besides a pointer to the output memory, the width of the matrix has to be provided as parameter; 2) the correct index has to be calculated for every memory access using an offset and the width of the matrix, i.\,e.
% knowledge about how the two-dimensional matrix is stored in one-dimensional memory is required.
% 3) In addition, manual boundary checks have to be performed to avoid faulty memory accesses. 

% SkelCL avoids all these low-level details.
% Neither additional parameter, nor index calculations or manual boundary checks are necessary.



% \from{HiStencils}
% Listing~\ref{lst:mapoverlap01} shows the implementation of the Sobel edge detection using the \emph{MapOverlap} skeleton.
% The MapOverlap skeleton applies a given function $func$ (defined in lines 2--6) to each element of an input matrix $in_{img}$ while taking the neighboring elements within the range $[-d, +d]$ in each dimension into account.
% Here, $d$ is the second parameter (line 7) and two additional parameters define how the skeleton handles out-of-bound memory accesses (line 8).
% A helper function (\code{get}) is used to easily access the neighboring elements.
% The indexes are specified relative to the current element, e.\,g. to access the element on the left the function call \code{get(in, -1, 0)} is used.

% Special handling is necessary when accessing elements out of the boundaries of the matrix, e.g., when the item in the top-left corner of the matrix accesses elements above and left of it.
% The MapOverlap skeleton can be configured to handle such out-of-bound memory accesses in two possible ways:
% 1) a specified neutral value is returned;
% 2) the nearest valid value inside the matrix is returned.
% In Listing~\ref{lst:mapoverlap01}, the first option is chosen and $0$ is provided as neutral value.

% \begin{lstlisting}[%
% caption={Implementation of Sobel edge detection using the MapOverlap skeleton},%
% float=tbp,%
% label={lst:mapoverlap01}]
% MapOverlap<char(char)> sobel(
%  "char func(const char* in_img) {
%     char ul = get(in_img, -1, -1);
%     ...
%     char lr = get(in_img, +1, +1);
%     return computeSobel(ul,..., lr);}",
%  1, Padding::NEUTRAL, 0);

% output = sobel(input);
% \end{lstlisting}

% Simple stencil computations with a regular stencil shape can easily be expressed using the MapOverlap skeleton.
% For more complex stencil computations, e.\,g. iterative stencils, we introduce the more advanced \emph{Stencil} skeleton.

% \paragraph{The MapOverlap Skeleton}

% Listing~\ref{lst:stencil01} shows the implementation of an iterative stencil application simulating heat transfer.
% This application simulates heat spreading from one location and flowing throughout a two-dimensional simulation space.
% \begin{lstlisting}[%
% caption={Implementation of heat simulation using the Stencil skeleton},%
% float=tbp,%
% label={lst:stencil01}]
% Stencil<char(char)> heatSim(
%  "char func(const char* in) {
%     char lt = get(in, -1, -1);
%     char lm = get(in, -1,  0);
%     char lb = get(in, -1, +1);
%     return computeHeat(lt, lm, lb); }",
%  StencilShape(1, 0, 1, 1),
%  Padding::NEUTRAL, 255);

% output = heatSim(100, input);
% \end{lstlisting}


% The application developer specifies the function (line 2--6) describing the computation and, therefore, the stencil shape, as well as the stencil shape's extents (line 7) and the out-of-bound handling (line 8).
% The stencil shape's extents are specified using four values for each of the directions:
% up, right, down, left.
% In the example in Listing~\ref{lst:stencil01}, the heat flows from left to right, therefore, no accesses to elements to the right are necessary and the stencil space's extents are specified accordingly (note the $0$ in line 7 representing the extent to the right).
% Figure~\ref{fig:stencilShape} illustrates this situation: the dark gray element is updated by using the values from the left.
% The specified stencil shape's extent is highlighted in light gray.
% In our current implementation, the user has to explicitly specify the stencil shape's extents, which is necessary for performing the out-of-bound handling on the GPU.
% In future work, we plan to automatically infer the stencil shape's extents
% from the customizing function using source code analysis in order to free the user from specifying this information explicitly.
% \begin{figure}
%   \begin{centering}
%     \includegraphics[width=.18\textwidth]{HiStencils/heat_transfer}
%     \caption{Stencil shape for heat transfer simulation}
%     \label{fig:stencilShape}
%     \vspace{-.5em}
%   \end{centering}
% \end{figure}

% Many stencil applications apply a stencil multiple times for a fixed number of iterations, or until a certain condition is met.
% For example, to iterate the heat transfer simulation for one hundred steps, we specify the number of iterations to perform when executing the skeleton (line 10).
% In the future, we plan to allow the user to specify a custom function which checks a condition to stop the iterations.

% The MapOverlap skeleton can be configured to handle out-of-bounds accesses by returning the nearest valid value of the input matrix.
% Another distinction can be made regarding iterations and sequences of stencil operations:
% using elements of the \textbf{initial}, user-provided input matrix or using elements of the \textbf{current} step's input matrix, which already was updated during earlier stencil operations.
% The Stencil skeleton can be configured to handle out-of-bounds accesses in both ways, thus offering three possible ways, including the neutral value. 
% For each of them, there is an own kernel function, loading appropriate elements into local memory. 






% Simple stencil computations with a regular stencil shape can easily be expressed using the \mapOverlap skeleton.
% For more complex stencil computations, \eg iterative stencil computations, we introduce the more advanced \stencil skeleton.





% Real-world applications often perform multiple stencil operations in sequence.
% Let us consider the popular \emph{Canny algorithm} which is used for detecting edges in images.
% For the sake of simplicity we consider a simplified version, which applies the following stencil operations in a sequence:
% 1), a noise reduction operation is applied, e.g., a Gaussian filter;
% 2), an edge detection operator like the Sobel filter is applied;
% 3), the so-called non-maximum suppression is performed, where all pixels in the image are colored black except pixels being a local maximum;
% 4), a threshold operation is applied to produce the final result.
% A more complex version of the algorithm performs the edge tracking by hysteresis, as an additional step.
% This results in detecting some weaker edges, but even without this additional step the algorithm usually achieves good results.

% In SkelCL, each single step of the Canny algorithm can be expressed using the Stencil skeleton.
% The last step, threshold operation, does not need access to neighboring elements, as the user threshold function only checks the value of the current pixel.
% Therefore, this step can be expressed using SkelCL's simpler Map skeleton.
% The Stencil skeleton's implementation automatically uses the simpler Map skeleton's implementation when the user specifies a stencil shape which extents are $0$ in all directions.

% \begin{lstlisting}[%
%   caption={Structure of the Canny algorithm as implemented with a sequence of skeletons.},%
%   float=tbp,%
%   label={lst:canny01}]
% Stencil<Pixel(Pixel)> gauss(...);
% Stencil<Pixel(Pixel)> sobel(...);
% Stencil<Pixel(Pixel)> nms(...);
% Stencil<Pixel(Pixel)> threshold(...);

% StencilSequence<Pixel(Pixel)> canny(
%   gauss, sobel, nms, threshold);

% output = canny(1, input);
% \end{lstlisting}

% To implement the Canny algorithm in SkelCL, the single steps can be combined as shown in Listing~\ref{lst:canny01}.
% The individual steps are defined in lines 1--4 and then combined to a sequence of stencils in line 6 and 7.
% During execution (line 9), the stencil operation are performed in the order which is specified when creating the \emph{StencilSequence} object.

% \paragraph{Implementation}
% In order to achieve high performance, our implementations of both the MapOverlap and the Stencil skeleton use the GPU's fast local memory.
% Both implementations perform the same basic steps on the GPU:
% first, the data is loaded from the global memory into the local memory;
% then, the user-defined function is called for every data element by passing a pointer to the element's location in the local memory;
% finally, the result of the user-defined function is copied back into the global memory.

% Although both implementations perform the same basic steps, different strategies are implemented for loading the data from the global into the local memory.

% \begin{figure}
%   \begin{centering}
%     \includegraphics[width=.29\textwidth]{HiStencils/map_overlap}
%     \caption{The MapOverlap skeleton prepares a matrix by copying data on the top and bottom.}
%     \label{fig:preparation}
%     \vspace{-.5em}
%   \end{centering}
% \end{figure}

% The MapOverlap skeleton prepares the input matrix on the CPU before uploading it to the GPU:
% padding elements are appended; they are used to avoid out-of-bounds memory accesses to the top and bottom of the input matrix, as shown in Figure~\ref{fig:preparation}.
% This slightly enlarges the input matrix, but it reduces branching on the GPU due to avoiding some out-of-bound checks.
% In SkelCL a matrix is stored row-wise in memory on the CPU and GPU, therefore, it would be complex and costly to add padding elements on the left and right of the matrix.
% To avoid out-of-bound accesses for these regions, the boundary checks are performed on the GPU.

% The Stencil skeleton has to use a different strategy in order to enable the usage of different padding modes and stencil shapes when using several Stencil skeletons in a sequence.
% As an example, consider two stencil shapes in a sequence where the first shape defines a neutral element $0$ and the second defines a neutral element $1$.
% This cannot be implemented using MapOverlap's implementation strategy.
% Therefore, Stencil does not append padding elements on the CPU, but rather manages all out-of-bounds accesses on the GPU, which slightly increases branching.

% \section{Targeting Multi-GPU Systems (HiStencils)}
% \label{sec:multi_gpu}
% The implicit and automatic support of systems with multiple OpenCL devices is one of the key features of SkelCL.
% By using distributions, SkelCL completely liberates the user from error-prone and low-level explicit programming of data (re)distributions on multiple GPUs. 

% The MapOverlap skeleton uses the overlap distribution with \textit{border regions} in which the elements calculated by a neighboring device are located.
% When it comes to iteratively executing a skeleton, data has to be transferred among devices between iteration steps, in order to ensure that data for the next iteration step is up-to-date.
% As the MapOverlap skeleton does not explicitly support iterations, its implementation is not able to exchange data between devices besides a full down- and upload of the matrix.
% In addition, data exchange has to be performed after each iteration.
% We can enlarge the number of elements in the border regions and perform multiple iteration steps on each device before exchanging data.
% However, this introduces redundant computations, such that a trade-off between data exchange and redundant computations has to be found.
 
% For the Stencil skeleton, the user can specify the number of iterations between \textit{device synchronisations}, where all border regions are updated with elements from the corresponding inner border regions of the neighboring device.
% The border regions are sized by default in such a way that the specified number of iterations can be performed without leading to incorrect results.
% However, there may be cases in which a different number of iterations between device synchronizations may result in better performance.
% Therefore, Stencil offers the user the possibility to specify that number.
% Please note that the implementation of the Stencil skeleton only exchanges elements from the border region and does not perform a full down- and upload of the matrix, as the MapOverlap skeleton does.

% Figure \ref{fig:syncDevices} shows the device synchronization.
% Only the appropriate elements in the inner border region are downloaded and stored as \texttt{std::vector}s in a \texttt{std::vector}.
% Within the outer vector, the inner vectors are swapped pair-wise on the host, so that the inner border regions can be uploaded in order to replace the out-of-date border regions.

% For the first iteration after a device synchronization, there are as many work-items on the GPU active as there are total elements on the device.
% As the first and last rows of the border regions become invalid within an iteration, the corresponding work-items become inactive in the following iteration step.
% This is done by using an offset and by reducing the number of total work-items when launching the OpenCL kernel.
% The Stencil's four kernel functions (one for each out-of-bounds handling mode and one for the adapted Map skeleton) can be used for both single- and multi-GPU systems.
 
% \begin{figure}[tb]
%   \centering
%   \includegraphics[width=\columnwidth]{HiStencils/data_exchange}
%   \caption{\small Device synchronization for three devices. Equally patterned and colored chunks represent the border regions and their matching inner border region. After the download of the appropriate inner border regions, they are swapped pair-wise on the host. Then the inner border regions are uploaded in order to replace the out-of-date border regions.}
%   \label{fig:syncDevices}
%   \vspace{1em}
% \end{figure} 






% allpairs impl




% \vspace{1em}
% We develop the allpairs skeleton within the skeleton library SkelCL~\cite{StKG-12}, which is built on top of OpenCL and targets modern parallel systems with multiple GPUs.
% Currently, five other skeletons are available in SkelCL: \emph{map}, \emph{zip}, \emph{reduce}, \emph{scan}, and \emph{mapOverlap}.
% Skeletons operate on container data types (in particular vectors and matrices) which alleviate the memory management of GPUs:
% data is copied automatically to and from GPUs, instead of manually performing data transfers as required in OpenCL.
% For programming multi-GPU systems, SkelCL offers the application programmer a data distribution mechanism to specify how the data of a container is distributed among the GPUs in the system.
% The container's data can either be assigned to a single GPU, be copied to all GPUs, or be partitioned in equal blocks across the GPUs, possibly with an overlap.
% If the data distribution is changed in the program, the necessary data movements are done automatically by the system~\cite{StKG-12}.

% \begin{lstlisting}[%                                                             
% caption={Matrix multiplication in SkelCL using the \emph{allpairs} skeleton.},%
% float=t,%                                                                       
% numbers=left,%
% label={lst:basic_mm}]
% skelcl::init();
% Allpairs<float(float, float)> mm(
%  "float func(float_matrix_t a, float_matrix_t b) {\
%   float c = 0.0f;\
%   for (int i = 0; i < width(a); ++i) {\
%     c += getElementFromRow(a, i) * getElementFromCol(b, i); }\
%   return c; }");
% Matrix<float> A(n, k); fill(A);
% Matrix<float> B(k, m); fill(B);
% Matrix<float> C = mm(A, B);
% \end{lstlisting}

% Listing~\ref{lst:basic_mm} shows the SkelCL program for computing matrix multiplication using the \emph{allpairs} skeleton;
% the code follows directly from the mathematical formulation (\ref{eq:mat_mult_allpairs}).
% In the first line, the SkelCL library is initialized.
% Skeletons are implemented as classes in SkelCL and customized by instantiating a new object, like in line 2.
% The \texttt{Allpairs} class is implemented as a template class specified with the data types of matrices involved in the computation (\texttt{float(float, float)}).
% This way the implementation can ensure the type correctness by checking the types of the arguments when the skeleton is executed in line 10.
% The customizing function -- specified as a string (line 3 -- 7) -- is passed to the constructor.
% Data types for matrices (\texttt{float\_matrix\_t} in line 3) are defined by the SkelCL implementation and used as arguments of helper functions for accessing elements from both matrices (line 6).
% The transpose of matrix $B$ required by the definition (\ref{eq:mat_mult_allpairs}) is implicitly performed by accessing elements from the columns of $B$ using the helper function \texttt{getElementFromCol}.
% After initializing the two input matrices (line 8 and 9), the calculation is performed in line 10.

% In our SkelCL library, skeletons are implemented by translating them into executable OpenCL code.
% Listing~\ref{lst:basic_impl} shows the OpenCL kernel which is combined with the given customizing function by the implementation of the allpairs skeleton.
% The customizing function (named \texttt{\emph{func}} in Listing~\ref{lst:basic_mm}) is renamed to match the name used in the function call in the implementation (\texttt{USER\_FUNC} in Listing~\ref{lst:basic_impl}).
% In addition, the types used in the predefined OpenCL kernel (\texttt{TYPE\_LEFT}, \texttt{TYPE\_RIGHT}, and \texttt{TYPE\_OUT} in Listing~\ref{lst:basic_impl}) are adjusted to match the actual types of the elements used in the computation (in this case, all three types are \texttt{float}).
% These modifications ensure that a valid OpenCL program performing the allpairs calculation is constructed.
% This generated OpenCL program is executed once for every element of the output matrix $C$.
% In lines 6 -- 7, the implementation prepares variables (\texttt{Am} and \texttt{Bm}) of a predefined data type (\texttt{float\_matrix\_t}) which encapsulate the matrices $A$ and $B$ and passes them to the customizing function which is called in line 9.

% \begin{lstlisting}[%                                                             
% caption={Generic OpenCL kernel used in the implementation of the allpairs skeleton.},%
% float=t,%                                                                       
% numbers=left,%
% label={lst:basic_impl}]
% __kernel void allpairs(const __global TYPE_LEFT*  A,
%                        const __global TYPE_RIGHT* B,
%                              __global TYPE_OUT*   C,
%                              int n, int d, int m) {
%   int col = get_global_id(0); int row = get_global_id(1);
%   float_matrix_t Am; Am.data = A; Am.width = d; Am.row = row;
%   float_matrix_t Bm; Bm.data = B; Bm.width = m; Bm.col = col;
%   if (row < n && col < m)
%     C[row * m + col] = USER_FUNC(Am, Bm); }
% \end{lstlisting}

% To achieve high performance, skeleton implementations must efficiently exploit the complex memory hierarchy of multi-GPU architectures.
% There are two main types of memory in OpenCL: \emph{global} and \emph{local memory}.
% The global memory is typically large but slow; the local memory is small but fast and has similar performance as caches in traditional systems, but has to be programmed manually.
% On modern GPUs, accesses to the global memory are very expensive, taking up to 800 processor cycles, as compared to only few cycles required to access the local memory~\cite{NVIDIA-12}.

% The generic implementation of the allpairs skeleton in Listing~\ref{lst:basic_impl} makes no assumption about the order in which the customizing function (\texttt{USER\_FUNC}) accesses the elements of its two input vectors.
% In this general case, we cannot assume that the two vectors fit entirely into the restricted GPU local memory.
% Therefore, we have to use only the global memory in the generic implementation.
% To improve our implementation of the allpairs skeleton, we restrict the memory access pattern of the customizing function in the next section.





% specialized allpairs

% \begin{lstlisting}[%                                                             
% caption={Matrix multiplication in SkelCL using the specialized \emph{allpairs} skeleton.},%
% float=b,%                                                                       
% numbers=left,%
% label={lst:nested_allpairs}]
% skelcl::init();
% Zip<float(float, float)> mult
%     ("float func(float x, float y) { return x*y; }");
% Reduce<float(float, float)> sum_up
%     ("float func(float x, float y) { return x+y; }");
% Allpairs<float(float, float)> mm(sum_up, mult);
% Matrix<float> A(n, d); fill(A);
% Matrix<float> B(d, m); fill(B);
% Matrix<float> C = mm(A, B);
% \end{lstlisting}

% Listing~\ref{lst:nested_allpairs} shows how the matrix multiplication can be programmed in SkelCL using the allpairs skeleton customized with zip-reduce.
% In line 1, the SkelCL library is initialized.
% In lines 2 and 3, the zip skeleton is defined using multiplication as customizing function and in lines 4 and 5, the reduce skeleton is customized with addition.
% These two customized skeletons are passed to the allpairs skeleton on its creation in line 6.
% The implementation of the allpairs skeleton then uses the two customizing functions of zip and reduce to generate the OpenCL kernel performing the allpairs computation.
% In line 9, the skeleton is executed taking two input matrices and producing the output matrix.
% Note that we create objects of the same \texttt{Allpairs} class when using the generic allpairs implementation (Listing~\ref{lst:basic_impl} line 2) and the specialized implementation (Listing~\ref{lst:nested_allpairs} line 6).
% Depending on which of the overloaded constructors is used, either the generic or the specialized implementation is created.





\subsection{Memory Management Implementation}
\label{section:skelcl-library:memory-management}
In the \SkelCL programming model the user managed its memory using \emph{container data types}.
The two container data types -- vector and matrix -- are implemented as template classes in the \SkelCL Library.
This generic implementation allows for storing data items of any primitive C/\Cpp data type (\eg, \code{int}), as well as user-defined data structures (\code{struct}s).

\paragraph{The SkelCL Vector}
The \SkelCL vector replicates the interface of the vector from the Standard Template Library (\STL), \ie, it can be used as a drop-in replacement of the standard vector.
Internally, a vector comprises pointers to corresponding areas of main memory (accessible by the host) and device memory.
The vector holds one pointer for the host and one pointer for each device available.
Memory on the devices is allocated automatically, according to the distribution of the vector:
while for a single distributed vector only memory on a single device is allocated, for a vector distributed with the copy, block, or overlap distribution memory on all devices is allocated.
The selected distribution obviously also influences how big the buffers allocated on the devices are.

Before the execution of a skeleton, the input vector's implementation ensures that all of its data is available on the devices.
This might result in implicit data transfers from the host memory to device memory.
The data of the output vector is not copied back to the host memory but rather resides in the device memory.
Before every data transfer, the vector implementation checks whether the data transfer is necessary;
only then the data is actually transferred.
Hence, if an output vector is used as the input to another skeleton, no further data transfer is performed.
This \emph{lazy copying} defers data transfers as long as possible or avoids them completely and, thus, minimizes the costly data transfers between host and device.
While all data transfers are performed implicitly by \SkelCL we understand that advanced application developers want fine grained control over the data transfers between host and devices.
For that purpose \SkelCL offers a set of \APIs developers can use to explicitly initiate and control the data transfer to and from the devices.

%In a SkelCL program, a vector object can be created and filled with data like this:
%\vspace{.5em}
%\centerline{\lstinline!Vector<int> vec(size);!}
%\centerline{\lstinline!for (int i = 0; i < vec.size(); ++i) \{ vec[i] = i; \}!}
%\vspace{.5em}

\paragraph{The SkelCL Matrix}
The \SkelCL matrix offers an easy to use interface similar to the interface of the vector.
Data is stored in row-major order and iterators are provided to iterate first over rows and then inside of a single row to access a particular element.
For the copy, block, and overlap distribution the matrix is divided across rows.
A single row is never split across multiple devices, which simplifies the memory management.
Besides offering an interface to access elements on the host, the matrix also offers an interface for accessing elements on the device by using two-dimensional indices.
This frees the application developer from performing cumbersome index calculations manually.

% \subsection{Data distribution}

% SkelCL's vector data type abstracts from memory ranges on multiple GPUs, such that the vector's data is accessible by each GPU.
% However, each GPU may access different parts of a vector or may even not access it at all.
% For example, when implementing work-sharing on multiple GPUs, the GPUs will access disjoint parts of input data, such that copying only a part of the vector to a GPU would be more efficient than copying the whole data to each GPU.

% For specifying partitionings of vectors in multi-GPU systems, the concept of \emph{distribution} is introduced in SkelCL.
% A distribution describes how the vector's data is distributed among the available GPUs.
% It allows the programmer to abstract from the challenges of managing memory ranges which are shared or partitioned across multiple devices: the programmer can think of a distributed vector as of a self-contained entity.

% Figure~\ref{fig:distributions} shows three distributions which are currently implemented in SkelCL and offered to the programmer:
% \emph{single}, \emph{block}, and \emph{copy}.
% If set to \emph{single} distribution (Figure~\ref{fig:distributions:single}), vector's whole data is stored on a single GPU (the first GPU if not specified otherwise).
% With \emph{block} distribution (Figure~\ref{fig:distributions:block}), each GPU stores a contiguous, disjoint part of the vector.
% The \emph{copy} distribution (Figure~\ref{fig:distributions:copy}) copies vector's entire data to each available device.
% A newly created vector can adopt any of these distributions.

% \begin{figure}[tb]
%   \centering
%   \begin{subfigure}{.30\textwidth}
%     \includegraphics[width=\textwidth]{ASHES/singleDistribution}
%     \caption{\emph{single}}
%     \label{fig:distributions:single}
%   \end{subfigure}
%   \hfill
%   \begin{subfigure}{.30\textwidth}
%     \includegraphics[width=\textwidth]{ASHES/copyDistribution}
%     \caption{\emph{copy}}
%     \label{fig:distributions:copy}
%   \end{subfigure}
%   \hfill
%   \begin{subfigure}{.30\textwidth}
%     \includegraphics[width=\textwidth]{ASHES/blockDistribution}
%     \caption{\emph{block}}
%     \label{fig:distributions:block}
%   \end{subfigure}
%   \caption{Distributions of a vector in SkelCL.}
%   \label{fig:distributions}
%   \bigskip
% \end{figure}

% The vector distribution can be changed at runtime either explicitly by the programmer or implicitly by the system.
% A change of distribution implies data exchanges between multiple GPUs and the host, which are performed implicitly by SkelCL.
% These implicit data exchanges are also performed lazily, i.\,e. only if really necessary, as described in Section~\ref{sec:introduction_to_skelcl}.
% Implementing such data transfers in OpenCL manually is a cumbersome task:
% data has to be downloaded to the host before it can be uploaded to other devices, including the corresponding length and offset calculations;
% this results in a lot of low-level code which is completely hidden when using SkelCL.

% A special situation arises when the distribution is changed from the \emph{copy} distribution, where each GPU holds its own full copy of the data.
% In such a case, each GPU may hold a different version of the vector as data modifications are only performed locally on the GPU.
% In order to maintain SkelCL's concept of a self-contained vector, these different versions must be combined using a user-specified function when the distribution is changed.
% If no function is specified, the copy of the first device is taken as the new version of the vector; the copies of the other devices are discarded.

