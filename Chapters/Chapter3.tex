% Chapter 3:

\chapter{High-Level Programming for Multi-GPU Systems}

\label{chapter:skelcl}

In this chapter we address the first main challenge identified in \autoref{chapter:background}: Programmability.
We will see how structured parallel programming significantly simplifies the task of programming for parallel systems.
As throughout the thesis, we will focus on programming of single- and multi-\GPU systems.
Many observations made here are also valid when programming other parallel systems.

We will first motivate the need for high-level abstractions using an real-world \OpenCL application from the field of medical imaging.
Then we introduce the \emph{\SkelCL} programming model and its implementation as a \Cpp library which addresses the lack of high-level abstractions in state of the art \GPU programming models.
The following \autoref{chapter:skelcl-evaluation} will provide several application studies to thoroughly evaluate the usefulness and performance of the abstractions and implementation presented in this chapter.


% ============================================================================ %
% ============================================================================ %
\input{Chapters/Chapter3/need_for_high-level.tex}

% ============================================================================ %
% ============================================================================ %
\input{Chapters/Chapter3/skelcl_programming_model.tex}


% ============================================================================ %
% ============================================================================ %
\input{Chapters/Chapter3/skelcl_library.tex}
























































% BREAK
\newpage







\from{HIPS begin}
\section{SkelCL: An OpenCL-based skeleton library (HIPS)}

\todo{use in the library section}
SkelCL provides a set of basic skeletons.
Two well-known examples are the \texttt{Zip} skeleton combining two vectors element-wise, and the \texttt{Reduce} skeleton combining all elements of a vector using a binary operation (see Section~\ref{sec:skeletons}).
Listing~\ref{lst:dotproduct} shows how a dot product of two vectors is implemented in SkelCL using these two skeletons.
Here, the \texttt{Zip} skeleton is customized by usual multiplication, and the \texttt{Reduce} skeleton is customized by usual addition.

For comparison, an OpenCL-based implementation of a dot product computation provided by NVIDIA requires approximately 68 lines of code (kernel function: 9~lines, host program: 59~lines)~\cite{CUDASDK-10}.

The implementation principles of the SkelCL library are as follows.
SkelCL generates OpenCL code (\emph{kernel functions}) from skeletons which is then compiled by OpenCL at runtime.
User-defined customizing functions passed to the skeletons are merged with pre-implemented skeleton code during code generation.
Since OpenCL is not able to pass function pointers to GPU functions, user-defined functions are passed as strings in SkelCL.

\begin{lstlisting}[%
breakindent=1.5em,%
caption={SkelCL program computing the dot product of two vectors. Arrays \texttt{a\_ptr} and \texttt{b\_ptr} initialize the vectors.},%
float=tbp,%
label={lst:dotproduct}]
int main (int argc, char const* argv[]) {
    SkelCL::init(); /* initialize SkelCL */

    /* create skeletons */
    SkelCL::Reduce<float> sum (                   "float sum (float x,float y){return x+y;}");
    SkelCL::Zip<float>    mult(                   "float mult(float x,float y){return x*y;}");

    /* allocate and initialize host arrays */
    float *a_ptr = new float[ARRAY_SIZE];
    float *b_ptr = new float[ARRAY_SIZE];
    fillArray(a_ptr, ARRAY_SIZE);
    fillArray(b_ptr, ARRAY_SIZE);

    /* create input vectors */
    SkelCL::Vector<float> A(a_ptr, ARRAY_SIZE);
    SkelCL::Vector<float> B(b_ptr, ARRAY_SIZE);

    /* execute skeletons */
    SkelCL::Scalar<float> C = sum( mult( A, B ) );

    /* fetch result */
    float c = C.getValue();
    
    /* clean up */
    delete[] a_ptr;
    delete[] b_ptr;
}
\end{lstlisting}


\subsubsection{Skeletons in SkelCL}

\todo{use in the library section}
Skeletons are higher-order functions because they take so-called \emph{customizing functions} as parametes.
In SkelCL, skeletons expect the customizing function to be a plain string containing the function's source code.
This is merged with the skeleton's own source code to generate source code for an OpenCL kernel.
After compilation, the kernel function is ready for execution.
Compiling the source code every time from source is a time-consuming task, taking up to several hundreds of milliseconds.
For a small kernel, this can be a huge overhead.
Therefore, SkelCL saves already compiled kernels on disk.
They can be loaded later if the same kernel is used again.
For our applications (presented in Section~\ref{sec:studies}), we observed that loading kernels from disk is at least five times faster than building them from source.

SkelCL currently provides four basic skeletons: \texttt{Map}, \texttt{Zip}, \texttt{Reduce}, and \texttt{Scan}.
Each skeleton consumes vectors as input and produces vectors as output.
A skeleton is called by using the name of the skeleton as a function and passing the appropriate number of arguments to it.
This behavior is implemented using the C++ operator overloading feature.

\subsubsection{Passing Additional Arguments to Skeletons}

\todo{use in the library section}

\begin{lstlisting}[%
caption={Passing additional arguments to a \texttt{Map} skeleton.},%
float=tbp,%
label={lst:additional_args}]
Map<float> mult_num("float f(float input, float number) { return input * number }");

Arguments arguments;
arguments.push(5);

mult_num(input, arguments);
\end{lstlisting}

In general, a skeleton's definition dictates how many input arguments can be passed to the skeleton.
The \texttt{Map} skeleton, for example, specifies that the provided function has to be a unary function, such that only one input vector can be passed to the skeleton.
However, not all algorithms easily fit into this strict pattern.

SkelCL allows the user to pass an arbitrary number of arguments to the function called inside of a skeleton:
first, the function definition must be changed, such that it expects additional arguments;
second, the additional arguments have to be passed to the skeleton upon execution.
A simple example is presented in Listing~\ref{lst:additional_args}.
The function definition for the \texttt{Map} skeleton in the listing takes two arguments instead of one, which would be common for \texttt{Map}.
In this example, an additional increment value is passed to the function.
Thus, the \texttt{Map} skeleton can now be used for adding an arbitrary increment to all elements of an input vector, instead of a fixed increment.
The additional argument is packaged into an \texttt{Arguments} object that is passed to the skeleton.
The implementation ensures passing the argument to all kernel instances called during execution.

Arbitrary types can be passed as arguments; a pointer and the size of the type has to be provided.
It is particularly easy to pass vectors as arguments because no information about the size has to be provided.
The arguments will be passed to the skeleton in the same order in which they are added to the \texttt{Arguments} object.
Hence, their order has to resemble the order of parameters in the function definition.
\from{HIPS end}




\from{ASHES begin}
\section{Overview of SkelCL (ASHES)} 
\subsection{Algorithmic Skeletons}

\todo{use in the library section}
% IMPLEMENTATION
To customize a skeleton, the application developer passes the source code of the user-defined function as a plain string to the skeleton.
SkelCL merges the user-defined function's source code with pre-implemented skeleton-specific program code, thus creating a valid OpenCL kernel automatically.
The created kernel is then compiled by the underlying OpenCL implementation before execution.
Thanks to this procedure SkelCL can operate on top of every standard compliant OpenCL implementation and does not require a customized compiler.

In real-world applications (see, e.\,g., Section~\ref{sec:list-mode_OSEM}), user-defined functions often work not only on a skeleton's input vector, but may also take additional inputs.
With only a fixed number of input arguments, traditional skeletons would not be applicable for the implementation of such applications.
The novelty of SkelCL skeletons is that they can accept additional arguments which are passed to the skeleton's user-defined function.
Since SkelCL's skeletons rely on pre-defined OpenCL code, they are by default not compatible with additional arguments.
Therefore, the SkelCL implementation at runtime adapts this code to the function it uses, such that the skeleton passes its additional arguments to the user-defined function.

% EXAMPLE
Listing~\ref{lst:saxpy} shows an example implementation of the \emph{single-precision real-alpha x plus y} (SAXPY) computation -- a commonly used BLAS routine -- in SkelCL.
\begin{lstlisting}[%
  caption={The BLAS $saxpy$ computation using a zip skeleton with additional arguments},%
float=tbp,%
label={lst:saxpy}]
/* create skeleton Y <- a * X + Y */
Zip<float> saxpy (
    "float func(float x, float y, float a)\
        { return a*x+y; }" );

/* create input vectors */
Vector<float> X(SIZE); fillVector(X);
Vector<float> Y(SIZE); fillVector(Y);
float a = fillScalar();

Y = saxpy( X, Y, a );      /* execute skeleton */

print(Y.begin(), Y.end()); /* print results */
\end{lstlisting}
SAXPY is a combination of scalar multiplication of $a$ with vector $X$ followed by vector addition with $Y$.
In the example, the computation is implemented by a zip skeleton:
vectors $X$ and $Y$ are passed as input, while factor $a$ is passed to the user-defined function as an additional argument.
The additional argument is simply appended to the argument list when the skeleton is executed.
Note that all input values of the user-defined function are scalar values rather than vectors or pointers.
Besides scalar values, like shown in the example, vectors can also be passed as additional arguments to a skeleton.
This feature is implemented in SkelCL using variadic templates from the new C++ standard\cite{gregor08, c++11}.

\section{Programming Multi-GPU Systems (ASHES)}

Additional challenges arise when a system comprises multiple GPUs.
In particular, communication and coordination between multiple GPUs and the host have to be implemented.
Many low-level details, like pointer arithmetics and offset calculations, are necessary when using OpenCL or CUDA for this purpose.
In this section, we demonstrate how SkelCL helps the developer to program multi-GPU systems at a high level of abstraction.

% \subsection{Data distribution}

% SkelCL's vector data type abstracts from memory ranges on multiple GPUs, such that the vector's data is accessible by each GPU.
% However, each GPU may access different parts of a vector or may even not access it at all.
% For example, when implementing work-sharing on multiple GPUs, the GPUs will access disjoint parts of input data, such that copying only a part of the vector to a GPU would be more efficient than copying the whole data to each GPU.

% For specifying partitionings of vectors in multi-GPU systems, the concept of \emph{distribution} is introduced in SkelCL.
% A distribution describes how the vector's data is distributed among the available GPUs.
% It allows the programmer to abstract from the challenges of managing memory ranges which are shared or partitioned across multiple devices: the programmer can think of a distributed vector as of a self-contained entity.

% Figure~\ref{fig:distributions} shows three distributions which are currently implemented in SkelCL and offered to the programmer:
% \emph{single}, \emph{block}, and \emph{copy}.
% If set to \emph{single} distribution (Figure~\ref{fig:distributions:single}), vector's whole data is stored on a single GPU (the first GPU if not specified otherwise).
% With \emph{block} distribution (Figure~\ref{fig:distributions:block}), each GPU stores a contiguous, disjoint part of the vector.
% The \emph{copy} distribution (Figure~\ref{fig:distributions:copy}) copies vector's entire data to each available device.
% A newly created vector can adopt any of these distributions.

% \begin{figure}[tb]
%   \centering
%   \begin{subfigure}{.30\textwidth}
%     \includegraphics[width=\textwidth]{ASHES/singleDistribution}
%     \caption{\emph{single}}
%     \label{fig:distributions:single}
%   \end{subfigure}
%   \hfill
%   \begin{subfigure}{.30\textwidth}
%     \includegraphics[width=\textwidth]{ASHES/copyDistribution}
%     \caption{\emph{copy}}
%     \label{fig:distributions:copy}
%   \end{subfigure}
%   \hfill
%   \begin{subfigure}{.30\textwidth}
%     \includegraphics[width=\textwidth]{ASHES/blockDistribution}
%     \caption{\emph{block}}
%     \label{fig:distributions:block}
%   \end{subfigure}
%   \caption{Distributions of a vector in SkelCL.}
%   \label{fig:distributions}
%   \bigskip
% \end{figure}

% The vector distribution can be changed at runtime either explicitly by the programmer or implicitly by the system.
% A change of distribution implies data exchanges between multiple GPUs and the host, which are performed implicitly by SkelCL.
% These implicit data exchanges are also performed lazily, i.\,e. only if really necessary, as described in Section~\ref{sec:introduction_to_skelcl}.
% Implementing such data transfers in OpenCL manually is a cumbersome task:
% data has to be downloaded to the host before it can be uploaded to other devices, including the corresponding length and offset calculations;
% this results in a lot of low-level code which is completely hidden when using SkelCL.

% A special situation arises when the distribution is changed from the \emph{copy} distribution, where each GPU holds its own full copy of the data.
% In such a case, each GPU may hold a different version of the vector as data modifications are only performed locally on the GPU.
% In order to maintain SkelCL's concept of a self-contained vector, these different versions must be combined using a user-specified function when the distribution is changed.
% If no function is specified, the copy of the first device is taken as the new version of the vector; the copies of the other devices are discarded.

\subsection{Skeletons for Multiple GPUs}
\label{sec:multi-gpu_skeletons}

The skeletons of SkelCL possess specific features for working in multi-GPU systems.
They take into account the distribution of their input vectors: each GPU that holds a part or a complete copy of a vector is involved in the execution of the skeleton.
Therefore, all GPUs automatically cooperate in the skeleton execution if its input vector is \emph{block}-distributed, whereas the skeleton is executed on one GPU if the vector distribution is \emph{single}.
If a skeleton's input vector is \emph{copy}-distributed, then all GPUs execute the same skeleton on their own copies.

Vectors can be passed to skeletons either as main inputs or as additional arguments.
For main input vectors, a skeleton-specific default distribution is set automatically by SkelCL, but the programmer can override the defaults, i.\,e., specify a distribution that fits best for the application.
For vectors passed as an additional argument, no meaningful default distribution can be provided by the system, because the access pattern for the vector is determined by the user-defined function.
Therefore, the user has to specify explicitly the distribution for these vectors.

\subsection{Implementation of Skeletons on Multiple GPUs}

\paragraph{Map and zip}
In a multi-GPU setting, each GPU executes the map's unary function on its part of the input vector.
The same holds for the zip skeleton, but it requires both input vectors to have the same distribution, and, in case of the single-distributed vectors, they also have to be stored on the same GPU.
If this requirement is not satisfied, SkelCL automatically changes the input vector distribution to block distribution.
This distribution is also set by default for input vectors with no distribution specified by the user.
Both, the map and zip skeleton, set their output vector distribution to that of their input vectors.

\paragraph{Reduce}
The reduce skeleton automatically performs all necessary synchronization and communication between CPU and GPUs, in three steps:
\begin{enumerate}
 \item Every GPU executes a local reduction for its local part of data;
 \item The results of all GPUs are gathered by the CPU;
 \item The CPU reduces these intermediate results to compute the final result.
\end{enumerate}
The output vector of the reduce skeleton holds only a single element, therefore, the output vector distribution is set to single.

\paragraph{Scan}
An example of the scan skeleton executed on four devices using addition as operation is shown in Figure~\ref{fig:scan}.
The input vector $[1,\ldots,16]$ is distributed using the \emph{block} distribution by default (shown in the top line).
After performing the scan algorithm on all devices (second line of the figure), map skeletons are built implicitly using the marked values and executed on all devices except the first one.
This produces the final result, as shown in the bottom line.

\begin{figure*}[tbp]
    \centering
    \includegraphics[width=.9\textwidth]{ASHES/scan}
    \caption{Scan on four GPUs: (1) All GPUs scan their parts independently.
            (2) map skeletons are created automatically and
             executed to produce the result.}
    \label{fig:scan}
\end{figure*}

The SkelCL implementation of the scan skeleton assumes the GPUs to have a fixed order, such that each GPU (except the first one) has a predecessor:
\begin{enumerate}
 \item Every GPU executes a local scan algorithm for its local part of data;
 \item The results of all GPUs are downloaded to the host;
 \item For each GPU (except the first one), a map skeleton is implicitly created that combines the result of the GPU's predecessors with all elements of its part using the user-defined operation of the scan skeleton;
 \item The newly created map skeletons compute the final results on all GPUs.
\end{enumerate}
The output vector is block-distributed among all GPUs.
\from{ASHES end}




\from{Paraphrase begin}
\section{SkelCL (Paraphrase)}

\subsection{The MapOverlap Skeleton}
Many applications dealing with two-dimensional data perform calculations for every data element taking neighboring data elements into account.
For example, image processing algorithms, like the gaussian blur, calculate a new value for every pixel of an input image using the previous value of the pixel and its surrounding values.

To facilitate the development of such applications, we extend SkelCL by an additional skeleton in combination with a new matrix data type, which is presented in Section~\ref{sec:skelcl:matrix}.
This skeleton can be used with either vector or matrix data type.
We explain the details of the new skeleton for the matrix data type.
\begin{itemize}
  \item The \emph{MapOverlap} skeleton takes two parameters: a function $f$ and an integer value $d$.
   It applies $f$ to each element of an input matrix $m_{in}$ while taking the neighboring elements within the range $[-d, +d]$ in each dimension into account, i.\,e.
  \begin{align*}
m_{out}[i,j]=f\left(
\begin{array}{ccccc}
m_{in}[i-d,j-d] & \hdots & m_{in}[i-d,j] & \hdots & m_{in}[i-d,j+d] \\
\vdots & ~ & \vdots & ~ & \vdots \\
m_{in}[i,j-d] & \hdots & m_{in}[i,j] & \hdots & m_{in}[i,j+d]\\
\vdots & ~ & \vdots & ~ & \vdots \\
m_{in}[i+d,j-d] & \hdots & m_{in}[i+d,j] & \hdots & m_{in}[i+d,j+d] \\
\end{array}
\right)
\end{align*}
\end{itemize}

In the actual source code, the application developer provides the function $f$ which receives a pointer to the element in the middle, $m_{in}[i,j]$.
Listing~\ref{lst:mapoverlap01} shows a simple example of computing the sum of all direct neighboring values using the MapOverlap skeleton.
To access the elements of the input matrix $m_{in}$, function \texttt{get} is used, as provided by SkelCL.
All indices are specified relative to the middle element $m_{in}[i,j]$, therefore, for accessing this element the function call \texttt{get(m\_in, 0, 0)} is used.

The application developer must ensure that only elements in the range specified by the second argument $d$ of the MapOverlap skeleton, are accessed.
In Listing~\ref{lst:mapoverlap01}, range is specified as $d=1$, therefore, only direct neighboring elements are accessed.
To enforce this property, boundary checks are performed at runtime by the \texttt{get} function.
In future work, we plan to avoid boundary checks at runtime by statically proving that all memory accesses are in bounds, as it is the case in the shown example.

Special handling is necessary when accessing elements out of the boundaries of the matrix, e.g., when the item in the top-left corner of the matrix accesses elements above and left of it.
The MapOverlap skeleton can be configured to handle such out-of-bound memory accesses in two possible ways:
1) a specified neutral value is returned;
2) the nearest valid value inside the matrix is returned.
In Listing~\ref{lst:mapoverlap01}, the first option is chosen and $0.0$ is provided as neutral value.

\begin{lstlisting}[%
caption={MapOverlap skeleton computing the sum of all direct neightbors for every element in a matrix},%
float=tbp,%
label={lst:mapoverlap01}]
MapOverlap<float(float)> m("float func(float* m_in){
     float sum = 0.0f;
     for (int i = -1; i < 1; ++i)
       for (int j = -1; j < 1; ++i)
         sum += get(m_in, i, j);
     return sum;
   }", 1, SCL_NEUTRAL, 0.0f);
\end{lstlisting}

Listing~\ref{lst:raw_opencl01} shows how the same simple calculation can be performed in standard OpenCL.
While the amount of lines of code increases by a factor of 2, the complexity of each single line also increases, as follows.
Besides a pointer to the output memory, the width of the matrix has to be provided as parameter.
The correct index has to be calculated for every memory access using an offset and the width of the matrix.
Therefore, knowledge about how the two-dimensional matrix is stored in one-dimensional memory is required.
In addition, manual boundary checks have to be performed to avoid faulty memory accesses.\bigskip

\begin{lstlisting}[%
caption={An OpenCL kernel performing the same calculation as the MapOverlap skeleton shown in Listing~\ref{lst:mapoverlap01}.\bigskip},%
label={lst:raw_opencl01}]
__kernel void sum_up(__global float* m_in,
                     __global float* m_out,
                     int width, int height) {
  int i_off = get_global_id(0); int j_off = get_global_id(1);
  float sum = 0.0f;
  for (int i = i_off - 1; i < i_off + 1; ++i)
    for (int j = j_off - 1; j < j_off + 1; ++j) {
      // perform boundary checks
      if ( i < 0 || i > width || j < 0 || j > height )
        continue;
      sum += m_in[ j * width + i ];     }
  m_out[ j_off * width + i_off ] = sum; }
\end{lstlisting}

SkelCL avoids all these low-level details.
Neither additional parameter, nor index calculations or manual boundary checks are necessary.
In SkelCL, the application developer only provides the source code implementing the steps required by the algorithm.

\subsection{Data distribution on multiple devices}
\label{sec:data_distribution}
The key feature of SkelCL for multi-device systems is that SkelCL's data types abstract from memory ranges on multiple devices, i.\,e. the data is accessible by each device.
However, each device may access different parts of a container (vector or matrix) or may even not access it at all.
For example, when implementing work-sharing on multiple devices, the devices will usually access disjoint parts of input data, such that copying only a part of the container to a device would be more efficient than copying the whole data to each device.

To simplify the specification of partitionings of containers in programs for multi-device systems, SkelCL implements the \emph{distribution} mechanism that describes how a container is distributed among the available devices.
It allows the programmer to abstract from managing memory ranges which are shared or spread across multiple devices:
the programmer can think of a distributed container as of a self-contained entity.

Four kinds of distribution are currently implemented in SkelCL and offered to the programmer:
\emph{block}, \emph{copy}, \emph{single} and \emph{overlap} (see Figure~\ref{fig:distributions}).
With \emph{block} distribution (Figure~\ref{fig:distributions:block}), each device stores a contiguous, disjoint part of the container.
The \emph{copy} distribution (Figure~\ref{fig:distributions:copy}) copies container's entire data to each available device.
In case of \emph{single} distribution (Figure~\ref{fig:distributions:single}), container's whole data is stored on a single device (the first device by default).

Together with the matrix data type, we introduce a new distribution called \emph{overlap} (Figure~\ref{fig:distribution:overlap}).
The overlap distribution splits the matrix into one chunk for each device, similarly to the block distribution.
In addition to the block distribution, the following holds for an overlap-distributed matrix:
each chunk consists of a number of continuous rows, and, a parameter -- the \emph{overlap size} -- specifies the number of rows at the edges of a chunk which are copied to the two neighboring devices.
Figure~\ref{fig:distribution:overlap} illustrates the overlap distribution:
Device 0 receives the top chunk ranging from the top row to the middle, while device 1 receives the second chunk ranging from the middle row to the bottom.
The marked regions are the \emph{overlap regions} which are available on both devices.

The \emph{overlap} distribution is automatically selected as distribution by the MapOverlap skeleton, to ensure that every device has access to the neighboring elements as needed by the MapOverlap skeleton.

A programmer can set the distribution of containers explicitly, or every skeleton selects a default distribution for its input and output containers otherwise.
Container's distribution can be changed at runtime.
A change of distribution implies data exchanges between multiple devices and the host, which are performed by SkelCL implicitly and lazily, as described above.
Implementing such data transfers in the standard OpenCL is a cumbersome task:
data has to be downloaded to the host before it can be uploaded to other devices, including the corresponding length and offset calculations;
this results in a lot of low-level code which is completely hidden when using SkelCL.
\from{Paraphrase end}



\from{HLPP begin}
\section{The Allpairs Skeleton and its Implementation (HLPP)}
\label{sec:allpairs_skeleton}

\label{sec:formal_def}
We define the allpairs computation pattern for two sets of entities, each entity represented by a vector of length $d$.
Let the cardinality of the first set be $n$ and the cardinality of the second set be $m$.
We model the first set as a $n\times d$ matrix $A$ and the second set as a $m\times d$ matrix $B$.
The allpairs computation yields an output matrix $C$ of size $n\times m$ as follows:
$c_{i, j} = A_i \oplus B_j$, where $A_i$ and $B_j$ are row vectors of $A$ and $B$, correspondingly:
$A_i = [A_{i,1}, \cdots, A_{i, d}]$, $B_j = [B_{j,1}, \cdots, B_{j,d}]$, and $\oplus$ is a binary operator defined as vectors.

\begin{definition}
  \label{def:allpairs}
  Let $A$ be a $n\times d$ matrix, $B$ be a $m\times d$ matrix, and $C$ be a $n\times m$ matrix, with their elements $a_{i,j}$, $b_{i,j}$, and $c_{i,j}$ respectively.
  The algorithmic skeleton \emph{allpairs} with customizing binary function $\oplus$ is defined as follows:
  \[
    \allpairs(\oplus)\left(
      \left[ \begin{array}{ccc}%
 	      a_{1,1} & \cdots & a_{1,d}\\[.25em]
       	\vdots & & \vdots\\[.25em]
       	a_{n,1} & \cdots & a_{n,d}%
     	\end{array}\right],%
      \left[ \begin{array}{ccc}%
       	b_{1,1} & \cdots & b_{1,d}\\[-.25em]
       	\cdot & & \cdot\\[-.75em]
       	\cdot & & \cdot\\[-.25em]
       	b_{m,1} & \cdots & b_{m,d}%
   	\end{array}\right]\right)%
  	\eqdef 
      \left[ \begin{array}{ccc}%
 	      c_{1,1} & \cdots & c_{1,m}\\[.25em]
       	\vdots & & \vdots\\[.25em]
        c_{n,1} & \cdots & c_{n,m}%
   	\end{array}\right]
  \]
  where elements $c_{i,j}$ of the $n\times m$ matrix $C$ are calculated as follows:
  \[
	  c_{i,j} = \DottedVector{a_{i,1}}{a_{i,d}} \oplus \DottedVector{b_{j,1}}{b_{j,d}}
  \]
\end{definition}

Figure~\ref{fig:allpairs_access_a} illustrates this definition:
the element $c_{2,3}$ of matrix $C$ marked as \circled{3} is computed by combining the second row of $A$ marked as \circled{1} with the third row of $B$ marked as \circled{2} using the binary operator $\oplus$.
Figure~\ref{fig:allpairs_access_b} shows the same computation with the transposed matrix $B$.
\begin{figure}[b]
  \centering
	\includegraphics[width=0.44\textwidth]{HLPP/allpairs_access_pattern_alternative}
	\includegraphics[width=0.44\textwidth]{HLPP/allpairs_access_pattern}
  \caption{The allpairs computation. Left: element $c_{2,3}$
    (\protect\tikz[baseline=(char.base)]\protect\node[shape=circle,draw,inner sep=1pt] (char) {3};)
    is computed by combining the second row of $A$
    (\protect\tikz[baseline=(char.base)]\protect\node[shape=circle,draw,inner sep=1pt] (char) {1};)
    with the third row of $B$
    (\protect\tikz[baseline=(char.base)]\protect\node[shape=circle,draw,inner sep=1pt] (char) {2};)
    using the binary operator $\oplus$. Right: the transposed matrix $B$ is used.}
  \label{fig:allpairs_access}
\end{figure}

\vspace{6em}
Let us consider two example applications which can be expressed by customizing the allpairs skeleton with a particular function $\oplus$.

\paragraph{Example 1:}
The Manhattan distance (or $L_1$ distance) is a measure of distance which is used in many applications.
In general, it is defined for two vectors, $v$ and $w$, of equal length $d$, as follows:
\begin{equation}
  \label{eq:man_dist}
  \ManDist(v, w) = \sum_{k=1}^d | v_k - w_k | 
\end{equation}
In~\cite{DaDQR-09}, the so-called Pairwise Manhattan Distance (\emph{PMD}) is studied as a fundamental operation in hierarchical clustering for data analysis.
\emph{PMD} is obtained by computing the Manhattan distance for every pair of rows of a given matrix.
This computation for arbitrary matrix $A$ can be expressed using the allpairs skeleton customized with the Manhattan distance defined in (\ref{eq:man_dist}):
\begin{equation}
  \PMD(A) = \mbox{\emph{allpairs}}(\mbox{\emph{ManDist}})\left(A, A\right)
\end{equation}
The $n\times n$ matrix computed by the customized skeleton contains the Manhattan distance for every pair of rows of the input $n\times d$ matrix $A$.

\paragraph{Example 2:}
Matrix multiplication is a basic linear algebra operation, which is a building block of many scientific applications.
An $n\times d$ matrix $A$ is multiplied by a $d\times m$ matrix $B$, producing a $n\times m$ matrix $C=A\times B$ whose element $c_{i,j}$ is computed as the dot product of the $i$th row of $A$ with the $j$th column of $B$.
The dot product of two vectors $a$ and $b$ of length $d$ is computed as:
\begin{equation}
  \dotProduct (a,b) = \sum_{k=1}^d a_k \cdot b_k
\end{equation}
The matrix multiplication can be expressed using the allpairs skeleton as:
\begin{equation}
  A\times B = \allpairs(\dotProduct)\left(A, B^T\right)
  \label{eq:mat_mult_allpairs}
\end{equation}
where $B^T$ is the transpose of matrix $B$.
We will use the matrix multiplication as our running example for the allpairs skeleton throughout the paper.

\vspace{1em}
We develop the allpairs skeleton within the skeleton library SkelCL~\cite{StKG-12}, which is built on top of OpenCL and targets modern parallel systems with multiple GPUs.
Currently, five other skeletons are available in SkelCL: \emph{map}, \emph{zip}, \emph{reduce}, \emph{scan}, and \emph{mapOverlap}.
Skeletons operate on container data types (in particular vectors and matrices) which alleviate the memory management of GPUs:
data is copied automatically to and from GPUs, instead of manually performing data transfers as required in OpenCL.
For programming multi-GPU systems, SkelCL offers the application programmer a data distribution mechanism to specify how the data of a container is distributed among the GPUs in the system.
The container's data can either be assigned to a single GPU, be copied to all GPUs, or be partitioned in equal blocks across the GPUs, possibly with an overlap.
If the data distribution is changed in the program, the necessary data movements are done automatically by the system~\cite{StKG-12}.

\begin{lstlisting}[%                                                             
caption={Matrix multiplication in SkelCL using the \emph{allpairs} skeleton.},%
float=t,%                                                                       
numbers=left,%
label={lst:basic_mm}]
skelcl::init();
Allpairs<float(float, float)> mm(
 "float func(float_matrix_t a, float_matrix_t b) {\
  float c = 0.0f;\
  for (int i = 0; i < width(a); ++i) {\
    c += getElementFromRow(a, i) * getElementFromCol(b, i); }\
  return c; }");
Matrix<float> A(n, k); fill(A);
Matrix<float> B(k, m); fill(B);
Matrix<float> C = mm(A, B);
\end{lstlisting}

Listing~\ref{lst:basic_mm} shows the SkelCL program for computing matrix multiplication using the \emph{allpairs} skeleton;
the code follows directly from the mathematical formulation (\ref{eq:mat_mult_allpairs}).
In the first line, the SkelCL library is initialized.
Skeletons are implemented as classes in SkelCL and customized by instantiating a new object, like in line 2.
The \texttt{Allpairs} class is implemented as a template class specified with the data types of matrices involved in the computation (\texttt{float(float, float)}).
This way the implementation can ensure the type correctness by checking the types of the arguments when the skeleton is executed in line 10.
The customizing function -- specified as a string (line 3 -- 7) -- is passed to the constructor.
Data types for matrices (\texttt{float\_matrix\_t} in line 3) are defined by the SkelCL implementation and used as arguments of helper functions for accessing elements from both matrices (line 6).
The transpose of matrix $B$ required by the definition (\ref{eq:mat_mult_allpairs}) is implicitly performed by accessing elements from the columns of $B$ using the helper function \texttt{getElementFromCol}.
After initializing the two input matrices (line 8 and 9), the calculation is performed in line 10.

In our SkelCL library, skeletons are implemented by translating them into executable OpenCL code.
Listing~\ref{lst:basic_impl} shows the OpenCL kernel which is combined with the given customizing function by the implementation of the allpairs skeleton.
The customizing function (named \texttt{\emph{func}} in Listing~\ref{lst:basic_mm}) is renamed to match the name used in the function call in the implementation (\texttt{USER\_FUNC} in Listing~\ref{lst:basic_impl}).
In addition, the types used in the predefined OpenCL kernel (\texttt{TYPE\_LEFT}, \texttt{TYPE\_RIGHT}, and \texttt{TYPE\_OUT} in Listing~\ref{lst:basic_impl}) are adjusted to match the actual types of the elements used in the computation (in this case, all three types are \texttt{float}).
These modifications ensure that a valid OpenCL program performing the allpairs calculation is constructed.
This generated OpenCL program is executed once for every element of the output matrix $C$.
In lines 6 -- 7, the implementation prepares variables (\texttt{Am} and \texttt{Bm}) of a predefined data type (\texttt{float\_matrix\_t}) which encapsulate the matrices $A$ and $B$ and passes them to the customizing function which is called in line 9.

\begin{lstlisting}[%                                                             
caption={Generic OpenCL kernel used in the implementation of the allpairs skeleton.},%
float=t,%                                                                       
numbers=left,%
label={lst:basic_impl}]
__kernel void allpairs(const __global TYPE_LEFT*  A,
                       const __global TYPE_RIGHT* B,
                             __global TYPE_OUT*   C,
                             int n, int d, int m) {
  int col = get_global_id(0); int row = get_global_id(1);
  float_matrix_t Am; Am.data = A; Am.width = d; Am.row = row;
  float_matrix_t Bm; Bm.data = B; Bm.width = m; Bm.col = col;
  if (row < n && col < m)
    C[row * m + col] = USER_FUNC(Am, Bm); }
\end{lstlisting}

To achieve high performance, skeleton implementations must efficiently exploit the complex memory hierarchy of multi-GPU architectures.
There are two main types of memory in OpenCL: \emph{global} and \emph{local memory}.
The global memory is typically large but slow; the local memory is small but fast and has similar performance as caches in traditional systems, but has to be programmed manually.
On modern GPUs, accesses to the global memory are very expensive, taking up to 800 processor cycles, as compared to only few cycles required to access the local memory~\cite{NVIDIA-12}.

The generic implementation of the allpairs skeleton in Listing~\ref{lst:basic_impl} makes no assumption about the order in which the customizing function (\texttt{USER\_FUNC}) accesses the elements of its two input vectors.
In this general case, we cannot assume that the two vectors fit entirely into the restricted GPU local memory.
Therefore, we have to use only the global memory in the generic implementation.
To improve our implementation of the allpairs skeleton, we restrict the memory access pattern of the customizing function in the next section.


\section{The Specialized Allpairs Skeleton (HLPP)}
\label{sec:opt_allpairs_skeleton}
In this section, we first analyze the memory access pattern of the matrix multiplication and then observe that this pattern can also be found in some other allpairs computations.
We, therefore, define a specialized version of the allpairs skeleton, which is suitable for applications having this pattern, and show how it can be implemented more efficiently than the generic skeleton.

\subsection{The memory access pattern of the matrix multiplication}
Figure~\ref{fig:single_memory_access} shows the memory access pattern of the matrix multiplication for $4\times 4$ matrices.
\begin{figure}[t]
  \centering
  \includegraphics[width=0.4\textwidth]{HLPP/single_memory_access}
  \caption{Memory access pattern of the matrix multiplication $A\times B = C$.}
  \label{fig:single_memory_access}
\end{figure}
To compute the element \circled{R} of the result matrix $C$, the first row of matrix $A$ and the first column of matrix $B$ are needed.
In the skeleton-based code, these two vectors are used by the customizing function (which is the dot product) for pairwise computations:
the two elements marked as \circled{1} are multiplied and the intermediate result is stored;
then, the next elements (marked as \circled{2}) are multiplied and the result is added to the intermediate result, and so forth.
Let us estimate the number of global memory accesses for computing an element of the matrix multiplication in the generic implementation (Listing~\ref{lst:basic_impl}).
One global memory read access for every element of both input vectors is performed, and a single global memory write access is required to write the result into the output matrix.
Therefore, $n\cdot m\cdot (d + d + 1)$ global memory accesses are performed in total, where $n$ and $m$ are the height and width of matrix $C$ and $d$ is the width of $A$ and the height of $B$.

Obviously, the customizing function of the pairwise Manhattan distance (Example 1 in Section~\ref{sec:allpairs_skeleton}) follows the same memory access pattern as matrix multiplication.
To find a common representation for a customizing function with this pairwise access pattern, we describe it as a combination of two well-known algorithmic skeletons: \emph{zip} and \emph{reduce}.

The \emph{zip} skeleton combines two input vectors by applying its customizing function ($\odot$) pairwise, producing the result vector:
\[ \zip\ (\odot)\ \DottedVector{a_1}{a_n}\ \DottedVector{b_1}{b_n}\ =\ \DottedVector{a_1 \odot b_1}{a_n \odot b_n} \]

The \emph{reduce} skeleton transforms an input vector into a scalar value by repeatedly applying its binary associative customizing operator ($\oplus$):
\[ \reduce\ (\oplus)\ \DottedVector{a_1}{a_n}\ =\ a_1 \oplus a_2 \oplus \cdots \oplus a_n \]

It is possible to sequentially compose these two customized skeletons.
For two functions $f: X \to Y$ and $g: Y\to Z$, the \emph{sequential composition} denoted by $g \circ f: X \to Z$ means that $f$ is applied first and then $g$ is applied to the return value of $f$ as input: $(g\circ f)(x) = g(f(x))$.
Our customized skeletons are functions with types that allow their composition as follows:
\begin{eqnarray*}
  (reduce\ (\oplus) \circ zip\ (\odot) ) \DottedVector{a_1}{a_n}\ \DottedVector{b_1}{b_n} &=& \\
  reduce\ (\oplus) \left( zip\ (\odot) \DottedVector{a_1}{a_n}\ \DottedVector{b_1}{b_n} \right) &=& (a_1 \odot b_1) \oplus \cdots \oplus (a_n \odot b_n)
\end{eqnarray*}
This composition of the two customized skeletons yields a function which takes two input vectors and produces a single scalar value:
\begin{equation}
  zipReduce\ (\oplus, \odot)\ a\ b = 
  \left( reduce\ (\oplus) \circ zip\ (\odot) \right)\ a\ b
\end{equation}

Following the definition of {\zipReduce}, we can express the customizing function of the Manhattan distance as follows.
We use the binary operator $a \ominus b = |a - b|$ as customizing function for zip, and addition as customizing function for the reduce skeleton:
\begin{eqnarray*}
    ManDist(a, b) = \sum_{i=1}^{n} | a_i - b_i | &=&
    (a_1 \ominus b_1) + \cdots + (a_n \ominus b_n) \\
    &=& zipReduce(+, \ominus)\ \DottedVector{a_1}{a_n}\ \DottedVector{b_1}{b_n}
\end{eqnarray*}

Similarly, we can express the dot product (which is the customizing function of matrix multiplication) as a zip-reduce composition, by using multiplication for customizing zip and addition for customizing the reduce skeleton:
\begin{eqnarray*}
  \dotProduct(a, b) = \sum_{i = 1}^{n} a_i \cdot b_i &=& (a_1 \cdot b_1) + \cdots + (a_n \cdot b_n) \\
  &=& zipReduce(+, \cdot)\ \DottedVector{a_1}{a_n}\ \DottedVector{b_1}{b_n}
\end{eqnarray*}

We can now specialize the generic Definition~\ref{def:allpairs} by employing the sequential composition of the customized reduce and zip skeletons for customizing the allpairs skeleton.
From here on, we refer to this specialization as the allpairs skeleton \emph{customized with zip-reduce}.

While not every allpairs computation can be expressed using the specialization, many real-world problems can.
In addition to the matrix multiplication and the pairwise Manhattan distance examples are the pairwise computation of the Pearson correlation coefficient~\cite{DaDQR-09} and estimation of Mutual Informations~\cite{DaSSK-04}.
The composition of zip and reduce is well known in the functional programming world.
Google's popular MapReduce programming model has been inspired by a similar composition of the \emph{map} and reduce skeletons; see~\cite{La-07}~for the relation of MapReduce to functional programming.

\begin{lstlisting}[%                                                             
caption={Matrix multiplication in SkelCL using the specialized \emph{allpairs} skeleton.},%
float=b,%                                                                       
numbers=left,%
label={lst:nested_allpairs}]
skelcl::init();
Zip<float(float, float)> mult
    ("float func(float x, float y) { return x*y; }");
Reduce<float(float, float)> sum_up
    ("float func(float x, float y) { return x+y; }");
Allpairs<float(float, float)> mm(sum_up, mult);
Matrix<float> A(n, d); fill(A);
Matrix<float> B(d, m); fill(B);
Matrix<float> C = mm(A, B);
\end{lstlisting}

Listing~\ref{lst:nested_allpairs} shows how the matrix multiplication can be programmed in SkelCL using the allpairs skeleton customized with zip-reduce.
In line 1, the SkelCL library is initialized.
In lines 2 and 3, the zip skeleton is defined using multiplication as customizing function and in lines 4 and 5, the reduce skeleton is customized with addition.
These two customized skeletons are passed to the allpairs skeleton on its creation in line 6.
The implementation of the allpairs skeleton then uses the two customizing functions of zip and reduce to generate the OpenCL kernel performing the allpairs computation.
In line 9, the skeleton is executed taking two input matrices and producing the output matrix.
Note that we create objects of the same \texttt{Allpairs} class when using the generic allpairs implementation (Listing~\ref{lst:basic_impl} line 2) and the specialized implementation (Listing~\ref{lst:nested_allpairs} line 6).
Depending on which of the overloaded constructors is used, either the generic or the specialized implementation is created.

\subsection{Implementation of the specialized allpairs skeleton}
By expressing the customizing function of the allpairs skeleton as a zip-reduce composition, we provide additional semantic information about the memory access pattern of the customizing function to the skeleton implementation, thus allowing for improving the performance.
Our idea of optimization is based on the OpenCL programming model that organizes \emph{work-items} (i.\,e., threads executing a kernel) in \emph{work-groups} which share the same GPU local memory.
By loading data needed by multiple work-items of the same work-group into the local memory, we can avoid repetitive accesses to the global memory.

\begin{figure}[b]
  \centering
  \includegraphics[width=.4\textwidth]{HLPP/memory_access}
  \caption{Implementation schema of the specialized allpairs skeleton.}
  \label{fig:memory_access}
\end{figure}
For the allpairs skeleton with the zip-reduce customizing function, we can adopt the implementation schema for GPUs~\cite{SaA-13}, as shown in Figure~\ref{fig:memory_access}.
We allocate two arrays in the local memory, one of size $r\times k$ ($r=2$, $k=3$ in Figure~\ref{fig:memory_access}) for elements of $A$ and one of size $k\times c$ ($c=3$ in Figure~\ref{fig:memory_access}) for elements of $B$.
A work-group consisting of $c\times r$ work-items computes $s$ blocks ($s=2$ in Figure~\ref{fig:memory_access}) of the result matrix $C$.
In Figure~\ref{fig:memory_access}, the two blocks marked as \circled{7} and \circled{8} are computed by the same work-group as follows.
In the first iteration, the elements of blocks \circled{1} and \circled{2} are loaded into the local memory and combined following the zip-reduce pattern.
The obtained intermediate result is stored in block \circled{7}.
Then, the elements of block \circled{3} are loaded and combined with the elements from \circled{2} which still reside in the local memory.
The intermediate result is stored in block \circled{8}.
In the second iteration, the algorithm continues in the same manner with blocks \circled{4}, \circled{5}, and \circled{6}, but this time, the elements of the blocks are also combined with the intermediate results of the first iteration, which are stored in blocks \circled{7} and \circled{8}.
The advantage of computing multiple blocks by the same work-group is that we keep the elements of $B$ in the local memory when computing the intermediate results, i.\,e., we do not reload block \circled{2} twice for the computation of blocks \circled{7} and \circled{8}.

Every element loaded from the global memory is used by multiple work-items:
e.\,g., the upper left element of block \circled{1} is loaded only once from the global memory, but used three times:
in the computation of the upper left, upper middle, and upper right elements of \circled{7}.
In general, every element loaded from $A$ is reused $c$ times, and every element from $B$ is reused $r\cdot s$ times.
As the intermediate results are stored in the global memory of matrix $C$, we perform two additional memory accesses (read/write) for every iteration, i.\,e., $2\cdot \frac{d}{k}$ in total.
Therefore, instead of $n\cdot m\cdot (d + d + 1)$ global memory accesses necessary when not using the local memory, only $n\cdot m\cdot (\frac{d}{r\cdot s} + \frac{d}{c} + 2\cdot \frac{d}{k})$ global memory accesses are performed.
By increasing the parameters $s$ and $k$, or the number of work-items in a work-group ($c$ and $r$), more global memory accesses can be saved.
However, the work-group size is limited by the GPU hardware.
While the parameters can be chosen independently of the matrix sizes, we have to consider the amount of available local memory.
\cite{SaA-13}~discusses how suitable parameters can be found by performing runtime experiments.

\begin{figure}[b]
  \centering
  \includegraphics[width=.3\textwidth]{HLPP/multi_gpu}
  \caption{Data distributions used for a system with two GPUs: matrices $A$ and $C$ are block distributed, matrix $B$ is copy distributed.}
  \label{fig:multi_gpu}
\end{figure}

\section{The Allpairs Skeleton using Multiple GPUs (HLPP)}
\label{sec:multi_gpu}
The allpairs skeleton can be efficiently implemented not only on systems with a single GPU, but on multi-GPU systems as well.
The SkelCL library provides four \emph{data distributions} which specify how a container data type (vector or matrix) is distributed among multiple GPUs~\cite{StKG-12}.
We use two of them in our multi-GPU implementation of the allpairs skeleton, as shown in Figure~\ref{fig:multi_gpu}:
Matrix $B$ is \emph{copy} distributed, i.\,e., it is copied entirely to all GPUs in the system.
Matrix $A$ and $C$ are \emph{block} distributed, i.\,e., they are row-divided into as many equally-sized blocks as GPUs are available;
each block is copied to its corresponding GPU.
Following these distributions, each GPU computes one block of the result matrix $C$.
In the example with two GPUs shown in Figure~\ref{fig:multi_gpu}, the first two rows of $C$ are computed by GPU 1 and the last two rows by GPU 2.
The allpairs skeleton automatically selects these distributions; therefore, no changes to the already discussed implementations of the matrix multiplication are necessary for using multiple GPUs.
\from{HLPP end}


\from{PaCT begin}



\subsection{The MapOverlap Skeleton}
\label{sec:skelcl:mapoverlap}
% ------------------------------------------------------------------------------
Many numerical and image processing applications dealing with two-dimensional data perform calculations for a particular data element (e.\,g., a pixel) taking neighboring data elements into account.
To facilitate the development of such applications, we define in SkelCL a skeleton that can be used with both vector and matrix data type; we explain the details for the matrix data type.
\begin{itemize}
  \item The \emph{MapOverlap} skeleton takes two parameters: a unary function $f$ and an integer value $d$.
   It applies $f$ to each element of an input matrix $m_{in}$ while taking the neighboring elements within the range $[-d, +d]$ in each dimension into account, i.\,e.
  \begin{align*}
m_{out}[i,j]=f\left(
\begin{array}{ccccc}
m_{in}[i-d,j-d] & \hdots & m_{in}[i-d,j] & \hdots & m_{in}[i-d,j+d] \\
\vdots & ~ & \vdots & ~ & \vdots \\
m_{in}[i,j-d] & \hdots & m_{in}[i,j] & \hdots & m_{in}[i,j+d]\\
\vdots & ~ & \vdots & ~ & \vdots \\
m_{in}[i+d,j-d] & \hdots & m_{in}[i+d,j] & \hdots & m_{in}[i+d,j+d] \\
\end{array}
\right)
\end{align*}
\end{itemize}

In the actual source code, the application developer provides the function $f$ which receives a pointer to the element in the middle, $m_{in}[i,j]$.

\begin{lstlisting}[%
caption={MapOverlap skeleton computing the sum of all direct neightbors for every element in a matrix},%
float=bp,%
label={lst:mapoverlap01}]
MapOverlap<float(float)> m("float func(float* m_in){
float sum = 0.0f;
for (int i = -1; i < 1; ++i)
	for (int j = -1; j < 1; ++i)
 		sum += get(m_in, i, j); return sum;
}", 1, SCL_NEUTRAL, 0.0f);
\end{lstlisting}


Listing~\ref{lst:mapoverlap01} shows a simple example of computing the sum of all direct neighboring values using the MapOverlap skeleton.
To access the elements of the input matrix $m_{in}$, function \texttt{get} is provided by SkelCL.
All indices are specified relative to the middle element $m_{in}[i,j]$; therefore, for accessing this element the function call \texttt{get(m\_in, 0, 0)} is used.
The application developer must ensure that only elements in the range specified by the second argument $d$ of the MapOverlap skeleton, are accessed.
In Listing~\ref{lst:mapoverlap01}, range is specified as $d=1$, therefore, only direct neighboring elements are accessed.
To enforce this property, boundary checks are performed at runtime by the \texttt{get} function.
In future work, we plan to avoid boundary checks at runtime by statically proving that all memory accesses are in bounds, as it is the case in the shown example.


Special handling is necessary when accessing elements out of the boundaries of the matrix, e.g., when the item in the top-left corner of the matrix accesses elements above and left of it.
The MapOverlap skeleton can be configured to handle such out-of-bound memory accesses in two possible ways:
1) a specified neutral value is returned;
2) the nearest valid value inside the matrix is returned.
In Listing~\ref{lst:mapoverlap01}, the first option is chosen and $0.0$ is provided as neutral value.

\begin{lstlisting}[%
caption={An OpenCL kernel performing the same calculation as the MapOverlap skeleton shown in Listing~\ref{lst:mapoverlap01}.},%
float=tbp,%
label={lst:raw_opencl01}]
__kernel void sum_up(__global float* m_in,
                     __global float* m_out,
                     int width, int height) {
  int i_off = get_global_id(0); 
  int j_off = get_global_id(1);
  float sum = 0.0f;
  for (int i = i_off - 1; i < i_off + 1; ++i)
    for (int j = j_off - 1; j < j_off + 1; ++j) {
      // perform boundary checks
      if ( i < 0 || i > width || j < 0 || j > height )
        continue;
      sum += m_in[ j * width + i ];     }
  m_out[ j_off * width + i_off ] = sum; }
\end{lstlisting}


Listing~\ref{lst:raw_opencl01} shows how the same simple calculation can be performed in standard OpenCL.
While the amount of lines of code increases by a factor of 2, the complexity of each single line also increases:
1) Besides a pointer to the output memory, the width of the matrix has to be provided as parameter; 2) the correct index has to be calculated for every memory access using an offset and the width of the matrix, i.\,e.
knowledge about how the two-dimensional matrix is stored in one-dimensional memory is required.
3) In addition, manual boundary checks have to be performed to avoid faulty memory accesses. 

SkelCL avoids all these low-level details.
Neither additional parameter, nor index calculations or manual boundary checks are necessary.

\subsection{The Allpairs Skeleton}
\label{sec:all-pairs_skeleton}

\emph{All-pairs computations} occur in a variety of applications, ranging from pairwise Manhattan distance computations used in bioinformatics~\cite{DaDQR-09} to N-Body simulations used in physics~\cite{ArSV-09}.
All these applications follow a common computational scheme:
for two sets of entities, the same computation is performed independently for all pairs of entities from the first set combined with entities from the second set.
An entity is usually described by a $d$-dimensional vector.

We define the all-pairs computation scheme for two sets of $n$ and $m$ entities, each entity represented by a $d$-dimensional vector.
We represent the sets as an $n\times d$ matrix $A$ and an $m\times d$ matrix $B$.
The all-pairs computation yields an output matrix $C$ of size $n\times m$ as follows:
$C_{i, j} = A_i \oplus B_j$, where $A_i$ and $B_j$ are rows of $A$ and $B$, correspondingly:
$A_i = [A_{i,1}, \cdots, A_{i, d}]$, $B_j = [B_{j,1}, \cdots, B_{j,d}]$,
and $\oplus$ is a binary function applied to every pair of rows from $A$ and $B$.

Figure~\ref{fig:allpairs_access} illustrates this definition:
the element marked as \circled{3} of matrix $C$ is computed by combining the second row of $A$ marked as \circled{1} with the third row of $B$ marked as \circled{2} using the binary operator $\oplus$.
\begin{figure}[tb]
  \centering
  \includegraphics[width=0.4\textwidth]{PaCT/allpairs_access_pattern_alternative}
  \caption{The allpairs computation: Element $c_{2,3}$ 
    (\protect\tikz[baseline=(char.base)]\protect\node[shape=circle,draw,inner sep=1pt] (char) {3};)
    is computed by combining the second row of $A$
    (\protect\tikz[baseline=(char.base)]\protect\node[shape=circle,draw,inner sep=1pt] (char) {1};)
    with the third row of $B$
    (\protect\tikz[baseline=(char.base)]\protect\node[shape=circle,draw,inner sep=1pt] (char) {2};)
    using the binary operator $\oplus$}
  \label{fig:allpairs_access}
\end{figure}

For formally defining the all-pairs skeleton, let $d$, $m$ and $n$ be positive numbers.
  Let $A$ be a $n\times d$ matrix, $B$ be a $m\times d$ matrix and $C$ be a $n\times m$ matrix with their entries $a_{i,j}$, $b_{i,j}$ and $c_{i,j}$ respectively.
  Let $\oplus$ be a binary function on vectors. % $\oplus: A \times B \to C$.
  The algorithmic skeleton $allpairs$ is defined as follows:
  \[
	  allpairs(\oplus)\left(\DottedMatrix{a_{1,1}}{a_{1,d}}{a_{n,1}}{a_{n,d}},%
	                           \DottedMatrix{b_{1,1}}{b_{1,d}}{b_{m,1}}{b_{m,d}}\right)%
  	\eqdef \DottedMatrix{c_{1,1}}{c_{1,m}}{c_{n,1}}{c_{n,m}}
  \]
  with entries $c_{i,j}$ of the computed $n\times m$ matrix $C$ defined as:
  \[
	  c_{i,j} = \DottedVector{a_{i,1}}{a_{i,d}} \oplus \DottedVector{b_{j,1}}{b_{j,d}}
  \]

To illustrate the definition, we show how matrix multiplication can be expressed using the allpairs skeleton.

\paragraph{Example 1:}
The matrix multiplication is a basic linear algebra operation, which is a building block of many scientific applications.
A $n\times d$ matrix $A$ is multiplied by a $d\times m$ matrix $B$, producing a $n\times m$ matrix $C=A\times B$ whose element $C_{i,j}$ is computed as the dot product of the $i$th row of $A$ with $j$th column of $B$.
The dot product of two vectors $a$ and $b$ of length $d$ is computed as:
\begin{equation}
  dotProduct(a,b) = \sum_{k=1}^d a_k \cdot b_k
\end{equation}
The matrix multiplication can be expressed using the allpairs skeleton as:
\begin{equation}
  A\times B = allpairs(dotProduct)\left(A, B^T\right)
  \label{eq:mat_mult_allpairs}
\end{equation}
where $B^T$ is the transpose of matrix $B$.
\from{PaCT end}






\from{HiStencils begin}
\section{The SkelCL Skeleton Library (HiStencils)}
\label{sec:skelcl}

\section{New Skeletons for Stencils (HiStencils)}
\label{sec:stencil}

The idea of our approach is that while the stencil operation varies for different applications, the overall structure of stencil computations stays the same.
Therefore, stencil computations can be implemented as a skeleton which is customized by the application developer with a particular stencil operation and particular stencil shape.

To simplify the development of stencil applications, we introduce two specialized skeletons in SkelCL: \emph{MapOverlap} and \emph{Stencil}.
While MapOverlap supports simple stencil computations, the Stencil skeleton provides support for more complex stencil computations with more complex stencil shapes and (possibly) iterative execution.

Listing~\ref{lst:mapoverlap01} shows the implementation of the Sobel edge detection using the \emph{MapOverlap} skeleton.
The MapOverlap skeleton applies a given function $func$ (defined in lines 2--6) to each element of an input matrix $in_{img}$ while taking the neighboring elements within the range $[-d, +d]$ in each dimension into account.
Here, $d$ is the second parameter (line 7) and two additional parameters define how the skeleton handles out-of-bound memory accesses (line 8).
A helper function (\code{get}) is used to easily access the neighboring elements.
The indexes are specified relative to the current element, e.\,g. to access the element on the left the function call \code{get(in, -1, 0)} is used.

Special handling is necessary when accessing elements out of the boundaries of the matrix, e.g., when the item in the top-left corner of the matrix accesses elements above and left of it.
The MapOverlap skeleton can be configured to handle such out-of-bound memory accesses in two possible ways:
1) a specified neutral value is returned;
2) the nearest valid value inside the matrix is returned.
In Listing~\ref{lst:mapoverlap01}, the first option is chosen and $0$ is provided as neutral value.

\begin{lstlisting}[%
caption={Implementation of Sobel edge detection using the MapOverlap skeleton},%
float=tbp,%
label={lst:mapoverlap01}]
MapOverlap<char(char)> sobel(
 "char func(const char* in_img) {
    char ul = get(in_img, -1, -1);
    ...
    char lr = get(in_img, +1, +1);
    return computeSobel(ul,..., lr);}",
 1, Padding::NEUTRAL, 0);

output = sobel(input);
\end{lstlisting}

Simple stencil computations with a regular stencil shape can easily be expressed using the MapOverlap skeleton.
For more complex stencil computations, e.\,g. iterative stencils, we introduce the more advanced \emph{Stencil} skeleton.
\paragraph{The MapOverlap Skeleton}

Listing~\ref{lst:stencil01} shows the implementation of an iterative stencil application simulating heat transfer.
This application simulates heat spreading from one location and flowing throughout a two-dimensional simulation space.
\begin{lstlisting}[%
caption={Implementation of heat simulation using the Stencil skeleton},%
float=tbp,%
label={lst:stencil01}]
Stencil<char(char)> heatSim(
 "char func(const char* in) {
    char lt = get(in, -1, -1);
    char lm = get(in, -1,  0);
    char lb = get(in, -1, +1);
    return computeHeat(lt, lm, lb); }",
 StencilShape(1, 0, 1, 1),
 Padding::NEUTRAL, 255);

output = heatSim(100, input);
\end{lstlisting}


The application developer specifies the function (line 2--6) describing the computation and, therefore, the stencil shape, as well as the stencil shape's extents (line 7) and the out-of-bound handling (line 8).
The stencil shape's extents are specified using four values for each of the directions:
up, right, down, left.
In the example in Listing~\ref{lst:stencil01}, the heat flows from left to right, therefore, no accesses to elements to the right are necessary and the stencil space's extents are specified accordingly (note the $0$ in line 7 representing the extent to the right).
Figure~\ref{fig:stencilShape} illustrates this situation: the dark gray element is updated by using the values from the left.
The specified stencil shape's extent is highlighted in light gray.
In our current implementation, the user has to explicitly specify the stencil shape's extents, which is necessary for performing the out-of-bound handling on the GPU.
In future work, we plan to automatically infer the stencil shape's extents
from the customizing function using source code analysis in order to free the user from specifying this information explicitly.
\begin{figure}
  \begin{centering}
    \includegraphics[width=.18\textwidth]{HiStencils/heat_transfer}
    \caption{Stencil shape for heat transfer simulation}
    \label{fig:stencilShape}
    \vspace{-.5em}
  \end{centering}
\end{figure}

Many stencil applications apply a stencil multiple times for a fixed number of iterations, or until a certain condition is met.
For example, to iterate the heat transfer simulation for one hundred steps, we specify the number of iterations to perform when executing the skeleton (line 10).
In the future, we plan to allow the user to specify a custom function which checks a condition to stop the iterations.

The MapOverlap skeleton can be configured to handle out-of-bounds accesses by returning the nearest valid value of the input matrix.
Another distinction can be made regarding iterations and sequences of stencil operations:
using elements of the \textbf{initial}, user-provided input matrix or using elements of the \textbf{current} step's input matrix, which already was updated during earlier stencil operations.
The Stencil skeleton can be configured to handle out-of-bounds accesses in both ways, thus offering three possible ways, including the neutral value. 
For each of them, there is an own kernel function, loading appropriate elements into local memory. 

\paragraph{The Stencil Skeleton}

\paragraph{Sequence of Stencil Operations}
Many real-world applications perform different stencil operations in a sequence.
Let us consider the popular \emph{Canny algorithm} which is used for detecting edges in images.
For the sake of simplicity we consider a simplified version, which applies the following stencil operations in a sequence:
first, a noise reduction operation is applied, e.g., a Gaussian filter;
second, an edge detection operator like the Sobel filter is applied;
third, the so-called non-maximum suppression is performed, where all pixels in the image are colored black except pixels being a local maximum;
finally, a threshold operation is applied to produce the final result.
A more complex version of the algorithm performs the edge tracking by hysteresis, as an additional step.
This results in detecting some weaker edges, but even without this
additional step the algorithm usually achieves good results.

In SkelCL, each single step of the Canny algorithm can be expressed using the Stencil skeleton.
The last step, threshold operation, does not need access to neighboring elements, as the user threshold function only checks the value of the current pixel.
Therefore, this step can be expressed using SkelCL's simpler Map skeleton.
The Stencil skeleton's implementation automatically uses the simpler Map skeleton's implementation when the user specifies a stencil shape which extents are $0$ in all directions.

\begin{lstlisting}[%
  caption={Structure of the Canny algorithm as implemented with a sequence of skeletons.},%
  float=tbp,%
  label={lst:canny01}]
Stencil<Pixel(Pixel)> gauss(...);
Stencil<Pixel(Pixel)> sobel(...);
Stencil<Pixel(Pixel)> nms(...);
Stencil<Pixel(Pixel)> threshold(...);

StencilSequence<Pixel(Pixel)> canny(
  gauss, sobel, nms, threshold);

output = canny(1, input);
\end{lstlisting}

To implement the Canny algorithm in SkelCL, the single steps can be combined as shown in Listing~\ref{lst:canny01}.
The individual steps are defined in lines 1--4 and then combined to a sequence of stencils in line 6 and 7.
During execution (line 9), the stencil operation are performed in the order which is specified when creating the \emph{StencilSequence} object.

\paragraph{Implementation}
In order to achieve high performance, our implementations of both the MapOverlap and the Stencil skeleton use the GPU's fast local memory.
Both implementations perform the same basic steps on the GPU:
first, the data is loaded from the global memory into the local memory;
then, the user-defined function is called for every data element by passing a pointer to the element's location in the local memory;
finally, the result of the user-defined function is copied back into the global memory.

Although both implementations perform the same basic steps, different strategies are implemented for loading the data from the global into the local memory.

\begin{figure}
  \begin{centering}
    \includegraphics[width=.29\textwidth]{HiStencils/map_overlap}
    \caption{The MapOverlap skeleton prepares a matrix by copying data on the top and bottom.}
    \label{fig:preparation}
    \vspace{-.5em}
  \end{centering}
\end{figure}

The MapOverlap skeleton prepares the input matrix on the CPU before uploading it to the GPU:
padding elements are appended; they are used to avoid out-of-bounds memory accesses to the top and bottom of the input matrix, as shown in Figure~\ref{fig:preparation}.
This slightly enlarges the input matrix, but it reduces branching on the GPU due to avoiding some out-of-bound checks.
In SkelCL a matrix is stored row-wise in memory on the CPU and GPU, therefore, it would be complex and costly to add padding elements on the left and right of the matrix.
To avoid out-of-bound accesses for these regions, the boundary checks are performed on the GPU.

The Stencil skeleton has to use a different strategy in order to enable the usage of different padding modes and stencil shapes when using several Stencil skeletons in a sequence.
As an example, consider two stencil shapes in a sequence where the first shape defines a neutral element $0$ and the second defines a neutral element $1$.
This cannot be implemented using MapOverlap's implementation strategy.
Therefore, Stencil does not append padding elements on the CPU, but rather manages all out-of-bounds accesses on the GPU, which slightly increases branching.

\section{Targeting Multi-GPU Systems (HiStencils)}
\label{sec:multi_gpu}
The implicit and automatic support of systems with multiple OpenCL devices is one of the key features of SkelCL.
By using distributions, SkelCL completely liberates the user from error-prone and low-level explicit programming of data (re)distributions on multiple GPUs. 

The MapOverlap skeleton uses the overlap distribution with \textit{border regions} in which the elements calculated by a neighboring device are located.
When it comes to iteratively executing a skeleton, data has to be transferred among devices between iteration steps, in order to ensure that data for the next iteration step is up-to-date.
As the MapOverlap skeleton does not explicitly support iterations, its implementation is not able to exchange data between devices besides a full down- and upload of the matrix.
In addition, data exchange has to be performed after each iteration.
We can enlarge the number of elements in the border regions and perform multiple iteration steps on each device before exchanging data.
However, this introduces redundant computations, such that a trade-off between data exchange and redundant computations has to be found.
 
For the Stencil skeleton, the user can specify the number of iterations between \textit{device synchronisations}, where all border regions are updated with elements from the corresponding inner border regions of the neighboring device.
The border regions are sized by default in such a way that the specified number of iterations can be performed without leading to incorrect results.
However, there may be cases in which a different number of iterations between device synchronizations may result in better performance.
Therefore, Stencil offers the user the possibility to specify that number.
Please note that the implementation of the Stencil skeleton only exchanges elements from the border region and does not perform a full down- and upload of the matrix, as the MapOverlap skeleton does.

Figure \ref{fig:syncDevices} shows the device synchronization.
Only the appropriate elements in the inner border region are downloaded and stored as \texttt{std::vector}s in a \texttt{std::vector}.
Within the outer vector, the inner vectors are swapped pair-wise on the host, so that the inner border regions can be uploaded in order to replace the out-of-date border regions.

For the first iteration after a device synchronization, there are as many work-items on the GPU active as there are total elements on the device.
As the first and last rows of the border regions become invalid within an iteration, the corresponding work-items become inactive in the following iteration step.
This is done by using an offset and by reducing the number of total work-items when launching the OpenCL kernel.
The Stencil's four kernel functions (one for each out-of-bounds handling mode and one for the adapted Map skeleton) can be used for both single- and multi-GPU systems.
 
\begin{figure}[tb]
	\centering
	\includegraphics[width=\columnwidth]{HiStencils/data_exchange}
	\caption{\small Device synchronization for three devices. Equally patterned and colored chunks represent the border regions and their matching inner border region. After the download of the appropriate inner border regions, they are swapped pair-wise on the host. Then the inner border regions are uploaded in order to replace the out-of-date border regions.}
	\label{fig:syncDevices}
  \vspace{1em}
\end{figure} 
\from{HiStencils end}

