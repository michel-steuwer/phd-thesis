% Chapter 3:

\chapter{High-Level Programming for Multi-GPU Systems}

\label{chapter:skelcl}

In this chapter we address the first main challenge identified in \autoref{chapter:background}: Programmability.
We will see how structured parallel programming significantly simplifies the task of programming for parallel systems.
As throughout the thesis, we will focus on programming of single- and multi-\GPU systems.
Many observations made here are also valid when programming other parallel systems.

We will first motivate the need for high-level abstractions using an real-world \OpenCL application from the field of medical imaging.
Then we introduce the \emph{\SkelCL} programming model and its implementation as a \Cpp library which addresses the lack of high-level abstractions in state of the art \GPU programming models.
The following \autoref{chapter:skelcl-evaluation} will provide several application studies to thoroughly evaluate the usefulness and performance of the abstractions and implementation presented in this chapter.


% ============================================================================ %
% ============================================================================ %
\input{Chapters/Chapter3/need_for_high-level.tex}

% ============================================================================ %
% ============================================================================ %
\input{Chapters/Chapter3/skelcl_programming_model.tex}


% ============================================================================ %
% ============================================================================ %
\input{Chapters/Chapter3/skelcl_library.tex}
























































% BREAK
\newpage







\from{HIPS begin}
\section{SkelCL: An OpenCL-based skeleton library (HIPS)}

\todo{use in the library section}
SkelCL provides a set of basic skeletons.
Two well-known examples are the \texttt{Zip} skeleton combining two vectors element-wise, and the \texttt{Reduce} skeleton combining all elements of a vector using a binary operation (see Section~\ref{sec:skeletons}).
Listing~\ref{lst:dotproduct} shows how a dot product of two vectors is implemented in SkelCL using these two skeletons.
Here, the \texttt{Zip} skeleton is customized by usual multiplication, and the \texttt{Reduce} skeleton is customized by usual addition.

For comparison, an OpenCL-based implementation of a dot product computation provided by NVIDIA requires approximately 68 lines of code (kernel function: 9~lines, host program: 59~lines)~\cite{CUDASDK-10}.

The implementation principles of the SkelCL library are as follows.
SkelCL generates OpenCL code (\emph{kernel functions}) from skeletons which is then compiled by OpenCL at runtime.
User-defined customizing functions passed to the skeletons are merged with pre-implemented skeleton code during code generation.
Since OpenCL is not able to pass function pointers to GPU functions, user-defined functions are passed as strings in SkelCL.

\begin{lstlisting}[%
breakindent=1.5em,%
caption={SkelCL program computing the dot product of two vectors. Arrays \texttt{a\_ptr} and \texttt{b\_ptr} initialize the vectors.},%
float=tbp,%
label={lst:dotproduct}]
int main (int argc, char const* argv[]) {
    SkelCL::init(); /* initialize SkelCL */

    /* create skeletons */
    SkelCL::Reduce<float> sum (                   "float sum (float x,float y){return x+y;}");
    SkelCL::Zip<float>    mult(                   "float mult(float x,float y){return x*y;}");

    /* allocate and initialize host arrays */
    float *a_ptr = new float[ARRAY_SIZE];
    float *b_ptr = new float[ARRAY_SIZE];
    fillArray(a_ptr, ARRAY_SIZE);
    fillArray(b_ptr, ARRAY_SIZE);

    /* create input vectors */
    SkelCL::Vector<float> A(a_ptr, ARRAY_SIZE);
    SkelCL::Vector<float> B(b_ptr, ARRAY_SIZE);

    /* execute skeletons */
    SkelCL::Scalar<float> C = sum( mult( A, B ) );

    /* fetch result */
    float c = C.getValue();
    
    /* clean up */
    delete[] a_ptr;
    delete[] b_ptr;
}
\end{lstlisting}


\subsubsection{Skeletons in SkelCL}

\todo{use in the library section}
Skeletons are higher-order functions because they take so-called \emph{customizing functions} as parametes.
In SkelCL, skeletons expect the customizing function to be a plain string containing the function's source code.
This is merged with the skeleton's own source code to generate source code for an OpenCL kernel.
After compilation, the kernel function is ready for execution.
Compiling the source code every time from source is a time-consuming task, taking up to several hundreds of milliseconds.
For a small kernel, this can be a huge overhead.
Therefore, SkelCL saves already compiled kernels on disk.
They can be loaded later if the same kernel is used again.
For our applications (presented in Section~\ref{sec:studies}), we observed that loading kernels from disk is at least five times faster than building them from source.

SkelCL currently provides four basic skeletons: \texttt{Map}, \texttt{Zip}, \texttt{Reduce}, and \texttt{Scan}.
Each skeleton consumes vectors as input and produces vectors as output.
A skeleton is called by using the name of the skeleton as a function and passing the appropriate number of arguments to it.
This behavior is implemented using the C++ operator overloading feature.

\subsubsection{Passing Additional Arguments to Skeletons}

\todo{use in the library section}

\begin{lstlisting}[%
caption={Passing additional arguments to a \texttt{Map} skeleton.},%
float=tbp,%
label={lst:additional_args}]
Map<float> mult_num("float f(float input, float number) { return input * number }");

Arguments arguments;
arguments.push(5);

mult_num(input, arguments);
\end{lstlisting}

In general, a skeleton's definition dictates how many input arguments can be passed to the skeleton.
The \texttt{Map} skeleton, for example, specifies that the provided function has to be a unary function, such that only one input vector can be passed to the skeleton.
However, not all algorithms easily fit into this strict pattern.

SkelCL allows the user to pass an arbitrary number of arguments to the function called inside of a skeleton:
first, the function definition must be changed, such that it expects additional arguments;
second, the additional arguments have to be passed to the skeleton upon execution.
A simple example is presented in Listing~\ref{lst:additional_args}.
The function definition for the \texttt{Map} skeleton in the listing takes two arguments instead of one, which would be common for \texttt{Map}.
In this example, an additional increment value is passed to the function.
Thus, the \texttt{Map} skeleton can now be used for adding an arbitrary increment to all elements of an input vector, instead of a fixed increment.
The additional argument is packaged into an \texttt{Arguments} object that is passed to the skeleton.
The implementation ensures passing the argument to all kernel instances called during execution.

Arbitrary types can be passed as arguments; a pointer and the size of the type has to be provided.
It is particularly easy to pass vectors as arguments because no information about the size has to be provided.
The arguments will be passed to the skeleton in the same order in which they are added to the \texttt{Arguments} object.
Hence, their order has to resemble the order of parameters in the function definition.
\from{HIPS end}




\from{ASHES begin}
\section{Overview of SkelCL (ASHES)} 
\subsection{Algorithmic Skeletons}

\todo{use in the library section}
% IMPLEMENTATION
To customize a skeleton, the application developer passes the source code of the user-defined function as a plain string to the skeleton.
SkelCL merges the user-defined function's source code with pre-implemented skeleton-specific program code, thus creating a valid OpenCL kernel automatically.
The created kernel is then compiled by the underlying OpenCL implementation before execution.
Thanks to this procedure SkelCL can operate on top of every standard compliant OpenCL implementation and does not require a customized compiler.

In real-world applications (see, e.\,g., Section~\ref{sec:list-mode_OSEM}), user-defined functions often work not only on a skeleton's input vector, but may also take additional inputs.
With only a fixed number of input arguments, traditional skeletons would not be applicable for the implementation of such applications.
The novelty of SkelCL skeletons is that they can accept additional arguments which are passed to the skeleton's user-defined function.
Since SkelCL's skeletons rely on pre-defined OpenCL code, they are by default not compatible with additional arguments.
Therefore, the SkelCL implementation at runtime adapts this code to the function it uses, such that the skeleton passes its additional arguments to the user-defined function.

% EXAMPLE
Listing~\ref{lst:saxpy} shows an example implementation of the \emph{single-precision real-alpha x plus y} (SAXPY) computation -- a commonly used BLAS routine -- in SkelCL.
\begin{lstlisting}[%
  caption={The BLAS $saxpy$ computation using a zip skeleton with additional arguments},%
float=tbp,%
label={lst:saxpy}]
/* create skeleton Y <- a * X + Y */
Zip<float> saxpy (
    "float func(float x, float y, float a)\
        { return a*x+y; }" );

/* create input vectors */
Vector<float> X(SIZE); fillVector(X);
Vector<float> Y(SIZE); fillVector(Y);
float a = fillScalar();

Y = saxpy( X, Y, a );      /* execute skeleton */

print(Y.begin(), Y.end()); /* print results */
\end{lstlisting}
SAXPY is a combination of scalar multiplication of $a$ with vector $X$ followed by vector addition with $Y$.
In the example, the computation is implemented by a zip skeleton:
vectors $X$ and $Y$ are passed as input, while factor $a$ is passed to the user-defined function as an additional argument.
The additional argument is simply appended to the argument list when the skeleton is executed.
Note that all input values of the user-defined function are scalar values rather than vectors or pointers.
Besides scalar values, like shown in the example, vectors can also be passed as additional arguments to a skeleton.
This feature is implemented in SkelCL using variadic templates from the new C++ standard\cite{gregor08, c++11}.

\section{Programming Multi-GPU Systems (ASHES)}

Additional challenges arise when a system comprises multiple GPUs.
In particular, communication and coordination between multiple GPUs and the host have to be implemented.
Many low-level details, like pointer arithmetics and offset calculations, are necessary when using OpenCL or CUDA for this purpose.
In this section, we demonstrate how SkelCL helps the developer to program multi-GPU systems at a high level of abstraction.

% \subsection{Data distribution}

% SkelCL's vector data type abstracts from memory ranges on multiple GPUs, such that the vector's data is accessible by each GPU.
% However, each GPU may access different parts of a vector or may even not access it at all.
% For example, when implementing work-sharing on multiple GPUs, the GPUs will access disjoint parts of input data, such that copying only a part of the vector to a GPU would be more efficient than copying the whole data to each GPU.

% For specifying partitionings of vectors in multi-GPU systems, the concept of \emph{distribution} is introduced in SkelCL.
% A distribution describes how the vector's data is distributed among the available GPUs.
% It allows the programmer to abstract from the challenges of managing memory ranges which are shared or partitioned across multiple devices: the programmer can think of a distributed vector as of a self-contained entity.

% Figure~\ref{fig:distributions} shows three distributions which are currently implemented in SkelCL and offered to the programmer:
% \emph{single}, \emph{block}, and \emph{copy}.
% If set to \emph{single} distribution (Figure~\ref{fig:distributions:single}), vector's whole data is stored on a single GPU (the first GPU if not specified otherwise).
% With \emph{block} distribution (Figure~\ref{fig:distributions:block}), each GPU stores a contiguous, disjoint part of the vector.
% The \emph{copy} distribution (Figure~\ref{fig:distributions:copy}) copies vector's entire data to each available device.
% A newly created vector can adopt any of these distributions.

% \begin{figure}[tb]
%   \centering
%   \begin{subfigure}{.30\textwidth}
%     \includegraphics[width=\textwidth]{ASHES/singleDistribution}
%     \caption{\emph{single}}
%     \label{fig:distributions:single}
%   \end{subfigure}
%   \hfill
%   \begin{subfigure}{.30\textwidth}
%     \includegraphics[width=\textwidth]{ASHES/copyDistribution}
%     \caption{\emph{copy}}
%     \label{fig:distributions:copy}
%   \end{subfigure}
%   \hfill
%   \begin{subfigure}{.30\textwidth}
%     \includegraphics[width=\textwidth]{ASHES/blockDistribution}
%     \caption{\emph{block}}
%     \label{fig:distributions:block}
%   \end{subfigure}
%   \caption{Distributions of a vector in SkelCL.}
%   \label{fig:distributions}
%   \bigskip
% \end{figure}

% The vector distribution can be changed at runtime either explicitly by the programmer or implicitly by the system.
% A change of distribution implies data exchanges between multiple GPUs and the host, which are performed implicitly by SkelCL.
% These implicit data exchanges are also performed lazily, i.\,e. only if really necessary, as described in Section~\ref{sec:introduction_to_skelcl}.
% Implementing such data transfers in OpenCL manually is a cumbersome task:
% data has to be downloaded to the host before it can be uploaded to other devices, including the corresponding length and offset calculations;
% this results in a lot of low-level code which is completely hidden when using SkelCL.

% A special situation arises when the distribution is changed from the \emph{copy} distribution, where each GPU holds its own full copy of the data.
% In such a case, each GPU may hold a different version of the vector as data modifications are only performed locally on the GPU.
% In order to maintain SkelCL's concept of a self-contained vector, these different versions must be combined using a user-specified function when the distribution is changed.
% If no function is specified, the copy of the first device is taken as the new version of the vector; the copies of the other devices are discarded.

\subsection{Skeletons for Multiple GPUs}
\label{sec:multi-gpu_skeletons}

The skeletons of SkelCL possess specific features for working in multi-GPU systems.
They take into account the distribution of their input vectors: each GPU that holds a part or a complete copy of a vector is involved in the execution of the skeleton.
Therefore, all GPUs automatically cooperate in the skeleton execution if its input vector is \emph{block}-distributed, whereas the skeleton is executed on one GPU if the vector distribution is \emph{single}.
If a skeleton's input vector is \emph{copy}-distributed, then all GPUs execute the same skeleton on their own copies.

Vectors can be passed to skeletons either as main inputs or as additional arguments.
For main input vectors, a skeleton-specific default distribution is set automatically by SkelCL, but the programmer can override the defaults, i.\,e., specify a distribution that fits best for the application.
For vectors passed as an additional argument, no meaningful default distribution can be provided by the system, because the access pattern for the vector is determined by the user-defined function.
Therefore, the user has to specify explicitly the distribution for these vectors.

\subsection{Implementation of Skeletons on Multiple GPUs}

\paragraph{Map and zip}
In a multi-GPU setting, each GPU executes the map's unary function on its part of the input vector.
The same holds for the zip skeleton, but it requires both input vectors to have the same distribution, and, in case of the single-distributed vectors, they also have to be stored on the same GPU.
If this requirement is not satisfied, SkelCL automatically changes the input vector distribution to block distribution.
This distribution is also set by default for input vectors with no distribution specified by the user.
Both, the map and zip skeleton, set their output vector distribution to that of their input vectors.

\paragraph{Reduce}
The reduce skeleton automatically performs all necessary synchronization and communication between CPU and GPUs, in three steps:
\begin{enumerate}
 \item Every GPU executes a local reduction for its local part of data;
 \item The results of all GPUs are gathered by the CPU;
 \item The CPU reduces these intermediate results to compute the final result.
\end{enumerate}
The output vector of the reduce skeleton holds only a single element, therefore, the output vector distribution is set to single.

\paragraph{Scan}
An example of the scan skeleton executed on four devices using addition as operation is shown in Figure~\ref{fig:scan}.
The input vector $[1,\ldots,16]$ is distributed using the \emph{block} distribution by default (shown in the top line).
After performing the scan algorithm on all devices (second line of the figure), map skeletons are built implicitly using the marked values and executed on all devices except the first one.
This produces the final result, as shown in the bottom line.

\begin{figure*}[tbp]
    \centering
    \includegraphics[width=.9\textwidth]{ASHES/scan}
    \caption{Scan on four GPUs: (1) All GPUs scan their parts independently.
            (2) map skeletons are created automatically and
             executed to produce the result.}
    \label{fig:scan}
\end{figure*}

The SkelCL implementation of the scan skeleton assumes the GPUs to have a fixed order, such that each GPU (except the first one) has a predecessor:
\begin{enumerate}
 \item Every GPU executes a local scan algorithm for its local part of data;
 \item The results of all GPUs are downloaded to the host;
 \item For each GPU (except the first one), a map skeleton is implicitly created that combines the result of the GPU's predecessors with all elements of its part using the user-defined operation of the scan skeleton;
 \item The newly created map skeletons compute the final results on all GPUs.
\end{enumerate}
The output vector is block-distributed among all GPUs.
\from{ASHES end}
