\section{Related work}

\from{HIPS begin}
\subsection{HIPS}
% ------------------------------------------------------------------------------

There are a number of other projects aiming at high-level GPU programming.

\emph{SkePU}~\cite{EnmyrenKe10} uses container classes and algorithmic skeletons to ease multi-GPU computing.
Although SkePU and SkelCL have been developed independently, both projects share some concepts:
SkePU provides a vector class similar to SkelCL's \texttt{Vector} class, but unlike SkelCL it does not support different kinds of data distribution on multi-GPU systems.
SkePU and SkelCL both provide a map and a reduce skeleton.
However, SkelCL additionally provides the \texttt{Zip} and \texttt{Scan} skeleton, while SkePU supports two additional variants of the map skeleton.
Unlike SkelCL, which allows for an arbitrary number of arguments, in SkePU the user-defined functions are restricted to a fixed skeleton-specific number of arguments.
Currently, SkePU is the only project other than SkelCL that supports data-parallel computations on multi-GPU systems.
SkelCL provides a more flexible memory management than SkePU, as data transfers can be expressed by changing data distribution settings.
Only this flexibility provides the best performance for our second case study (Section~\ref{sec:list-mode_OSEM}) and similar applications.
Both projects differ significantly in the way how functions are passed to skeletons.
While functions are defined as plain strings in SkelCL, SkePU uses a macro language, which brings some serious drawbacks.
For example, it is not possible to call mathematical functions like sin or cos inside a function generated by a SkelPU macro,
because these functions are either named differently in all three target programming models or might even be missing entirely.
The same holds for functions and keywords related to performance tuning, e.\,g., the use of local memory.
SkelCL does not suffer from these drawbacks because it relies on OpenCL and thus can be executed on a variety of devices.

\emph{CUDPP}~\cite{SenguptaHZO07} is a C++ library based on CUDA.
It provides data-parallel algorithm primitives similar to skeletons.
These primitives can be configured using only a predefined set of operations, whereas
skeletons in SkelCL are true higher-order functions, which accept any user-defined function.
CUDPP does not simplify data management, because data still has to be exchanged between CPU and GPU explicitly.
There is also no support for multi-GPU applications.

\emph{Thrust}~\cite{BellHo2011} is an open-source library by NVIDIA.
It provides two vector types similar to the vector type of the C++ Standard Template Library.
While these types refer to vectors stored in CPU or GPU memory, respectively, SkelCL's vector data type provides a unified abstraction for CPU and GPU memory.
Thrust also contains data-parallel implementations of higher-order functions, similiar to SkelCL's skeletons.
SkelCL adopts several of Thrust's ideas, but it is not limited to CUDA-capable devices and supports multiple GPUs.

Unlike SkelCL, \emph{PGI Acccelerator}~\cite{PGI-10} and \emph{HMPP}~\cite{HMPP-09} are compiler-based approaches to GPU programming, similar to the popular OpenMP~\cite{OpenMP-08}.
The programmer uses compiler directives to mark regions of code to be executed on a GPU.
A compiler generates executable code for the GPU, based on the used directives.
Although source code for low-level details like memory allocation or data exchange is generated by the compiler, these operations still have to be specified explicitly by the programmer using suitable compiler directives.
We consider these approaches low-level, as they do not perform data transfer automatically to shield the programmer from low-level details.
\from{HIPS end}

\from{ASHES begin}
\subsection{ASHES}
SkelCL is a runtime, library-based approach for GPU programming, unlike compiler-based approaches, e.\,g., \emph{HMPP}~\cite{HMPP-07} and the \emph{PGI Accelerator compilers}~\cite{PGI-10}.
It offers the user a consistent high-level API while still allowing the programmer to use all features of the underlying OpenCL standard.

There are other library-based approaches for high-level GPU programming.

\emph{SkePU}~\cite{EnmyrenKe10} is a skeleton-based framework for multi-core CPUs and multi-GPU systems.
An architecture-independent macro language is used which, however, makes architecture-specific optimizations impossible, like the use of local memory in OpenCL.
SkelCL avoids this drawback by building on top of OpenCL.
SkePU provides memory abstractions similar to SkelCL, but does not support different data distributions on multi-GPU systems as in SkelCL:
vectors are always distributed to all available devices, with no possibility of data exchanges between devices.
Therefore, our list-mode OSEM application which heavily relies on multiple data exchanges between devices cannot be implemented efficiently using SkePU.

\emph{Thrust}~\cite{BellHo2011} provides an interface similar to the C++ Standard Template Library (STL).
For data management, two distinct dynamic containers, a \texttt{host\_vector} and a \texttt{device\_vector}, can be used like STL vectors for managing host and device memory respectively.
% Data is copied to a device by assigning a \texttt{host\_vector} to a \texttt{device\_vector}.
In addition, Thrust offers common parallel algorithms, including searching, sorting, reductions, and transformations.
Thrust is based on CUDA, therefore, restricting the user to NVIDIA GPUs.

\emph{GPUSs}~\cite{AyBILMQ-09} is an implementation of the Star Superscalar model for multi-GPU systems.
While SkelCL is focused on data parallelism, GPUSs provides simple task parallelism;
annotations are used for data transfers between host and GPU.
SkelCL offers a higher level of memory abstraction: communication is specified implicitly by a distribution scheme instead of individual data transfers.

Rabhi and Gorlatch~\cite{RaG-03} present different approaches of skeletal programming for parallel as well as distributed systems.
Gonz\'{a}lez-V\'{e}lez and Leyton~\cite{GoL-10} provide an overview of available skeleton frameworks.
\from{ASHES end}

\from{ICCS begin}
\subsection{ICCS}
A considerable amount of work exists in the filed of algorithmic skeletons;
for an overview we refer to~\cite{gc11}.
There are several related approaches to raise the level of program abstraction in GPU programming.
While SkelCL can be used for programming multiple OpenCL capable GPUs, the CUDA-based \emph{Thrust}~\cite{BellHo2011} library simplifies programming only for a single NVIDIA GPU.
As SkelCL, \emph{SkePU}~\cite{EnK-10} is a skeleton library targeting multi-GPU systems.
In contrast to our work which is based entirely on the portable OpenCL, SkePU is implemented with multiple back-ends which restrict the application developer to the back-ends' smallest common set of functions and, thus, prevents the user from applying optimizations, like using the fast local GPU memory.
\from{ICCS end}

\from{HLPP begin}
\subsection{HLPP}
Considerable theoretical as well as practical research has been conducted in the field of algorithmic skeletons since its introduction in the late 1980s.
Due to lack of space, we refer to~\cite{GoC-11} for an overview of skeletal programming and~\cite{gl10} for a recent survey of skeleton libraries for clusters and multi-core CPUs.
Our contribution to skeletal programming is the introduction and efficient implementation of a new algorithmic skeleton for performing allpairs computations.
As other skeletons, the allpairs skeleton can be used as a basic building block by application developers who do not have to be experts in GPU computing or parallel programming in general.

In previous work, efficient parallel implementations of allpairs computations on modern parallel processors were studied (e.\,g., multi-core CPUs~\cite{AroraShVu2009}, the Cell processor~\cite{WirawanSK09}, and GPUs~\cite{ChangDeQuRo2009}) in the context of specific applications.
In contrast to~\cite{SarjeAl2013}, which presents an efficient implementation scheme of allpairs computations for GPUs, we abstract the computation as an algorithmic skeleton and offer its efficient implementation to application developers as part of the SkelCL skeleton library.

The evaluation of the programming effort shows that the allpairs skeleton allows to express many applications considerably shorter and at a higher level of abstraction, as compared to using OpenCL or library implementations like BLAS.
The performance comparison shows that by making information about the memory access pattern available to the implementation, we can considerably improve the performance by efficiently using the fast GPU local memory.

Several current approaches address simplifying GPU programming.
As SkelCL, also SkePU~\cite{EnmyrenKe10} and Muesli~\cite{ErK-12} are skeleton libraries targeting multi-GPU systems.
In contrast to our work, which is based entirely on the portable OpenCL, Muesli is implemented using NVIDIA's CUDA and SkePU is implemented with multiple back-ends which restrict the application developer to the back-ends' smallest common set of functions.
While SkelCL can be used for programming multiple OpenCL-capable GPUs, the CUDA-based Thrust~\cite{BellHo2011} library simplifies programming only for a single NVIDIA GPU.
\from{HLPP end}

\from{PaCT begin}
\subsection{PaCT}
There are a number of other projects aiming at high-level GPU programming.

\emph{SkePU}~\cite{EnmyrenKe10} provides a vector class similar to our \texttt{Vector} class, but unlike SkelCL it does not support different kinds of data distribution on multi-GPU systems.
SkelCL provides a more flexible memory management than SkePU, as data transfers can be expressed by changing data distribution settings.
Both approaches differ significantly in the way how functions are passed to skeletons.
While functions are defined as plain strings in SkelCL, SkePU uses a macro language, which brings some serious drawbacks.
For example, it is not possible to call mathematical functions like sin or cos inside a function generated by a SkelPU macro,
because these functions are either named differently in all of their three target programming models (CUDA, OpenCL, OpenMP) or might even be missing entirely.
The same holds for functions and keywords related to performance tuning, e.\,g., the use of local memory.
SkelCL does not suffer from these drawbacks because it relies on OpenCL and thus can be executed on a variety of GPUs and other accelerators.

\emph{CUDPP}~\cite{SenguptaHZO07} provides data-parallel algorithm primitives similar to skeletons.
These primitives can be configured using only a predefined set of operations, whereas
skeletons in SkelCL are true higher-order functions which accept any user-defined function.
CUDPP does not simplify data management, because data still has to be exchanged between CPU and GPU explicitly.
There is also no support for multi-GPU applications.

\emph{Thrust}~\cite{BellHo2011} provides two vector types similar to the vector type of the C++ Standard Template Library.
While these types refer to vectors stored in CPU or GPU memory, respectively, SkelCL's vector data type provides a unified abstraction for CPU and GPU memory.
Thrust also contains data-parallel implementations of higher-order functions, similiar to SkelCL's skeletons.
SkelCL adopts several of Thrust's ideas, but it is not limited to CUDA-capable GPUs and supports multiple GPUs.

Unlike SkelCL, \emph{OpenACC}~\cite{OpenACC}, \emph{PGI Acccelerator}~\cite{PGI-10}, and \emph{HMPP}~\cite{HMPP-09} are compiler-based approaches to GPU programming, similar to the popular OpenMP~\cite{OpenMP-08}.
The programmer uses compiler directives to mark regions of code to be executed on a GPU.
A compiler generates executable code for the GPU, based on the used directives.
Although source code for low-level details like memory allocation or data exchange is generated by the compiler, these operations still have to be specified explicitly by the programmer using suitable compiler directives.
We consider these approaches low-level, as they do not perform data transfer automatically to shield the programmer from low-level details and parallelism is still expressed explicitly.
\from{PaCT end}

\from{HiStencils begin}
\subsection{HiStencils}
Several approaches aiming at simplifying GPU programming exist.
\emph{SkePU}~\cite{EnmyrenKe10} provides a vector class similar to our \texttt{Vector} class, but unlike SkelCL it does not support different kinds of data distribution on multi-GPU systems.
SkelCL provides a more flexible memory management than SkePU, as data transfers can be expressed by changing data distribution settings.
\emph{Thrust}~\cite{BellHo2011} provides two vector types similar to the vector type of the C++ Standard Template Library.
While these types refer to vectors stored in CPU or GPU memory, respectively, SkelCL's vector data type provides a unified abstraction for CPU and GPU memory.
Thrust also contains data-parallel implementations of higher-order functions, similiar to SkelCL's skeletons.
SkelCL adopts several of Thrust's ideas, but it is not limited to CUDA-capable GPUs and supports multiple GPUs.
Both SkePU and Thrust provide no explicit support for stencil computations.

Several projects focus on stencil computations on GPUs.
PATUS~\cite{PATUS} is a code generation and tuning framework for stencil computations.
It can generate optimized code for multicore processors and a single GPU.
PARTANS~\cite{PARTANS} is a code generation and autotuning framework for stencil computations on multiple GPUs. 
It automatically distributes and optimizes stencil computations on multiple GPUs, by searching for optimal parameters for a given hardware architecture.
These specialized domain-specific approaches can only be applied to stencil computations, whereas SkelCL is a general-purpose approach.
\from{HiStencils end}


\from{PACT begin}
\subsection{PACT}

\paragraph{Algorithmic Patterns}
Algorithmic patterns or skeletons~\cite{cole88skeleton} have been around for more than two decades.
The Map-Reduce~\cite{dean08mapreduce} framework from Google for instance allows programmers to express computation using two operators; map and reduce.
Since the original framework was developed, many researchers have looked at the problem of optimizing these operations for different type of hardware.
Paraprox~\cite{Samadi14Paraprox} for instance uses automatic detection of algorithmic patterns to apply optimization at the expense of accuracy.
Compared to our approach, most prior works rely on hardware-specific implementations to achieve high performance.
Conversely, we automatically generate implementations using our fine-grain hardware patterns combined with our rule rewriting system.

\paragraph{Functional Approaches for GPU Code generation}
Accelerate is a functional domain specific language built within Haskell to support GPU acceleration~\cite{chakravarty11accelerating,mcdonell13optimising}.
Recently, Nvidia has released technical reports on NOVA~\cite{collins13nova}, a new functional language target at code generation for GPUs, and Copperhead~\cite{catanzaro11copperhead}, a data parallel language embedded in Python.
Nova shares many concepts from Accelerate and Copperhead and offers familiar data parallel patterns.
HiDP~\cite{zhang13hidp} is a hierarchical data parallel language which maps computations to the OpenCL programming model similar to our approach.
All these projects rely on analysis of user code or hand-tuned versions of high-level algorithmic patterns.
In contrast, our approach uses rewrite rules and low-level hardware patterns to produce high-performance code in a portable way.

\paragraph{Rewrite-rules for Optimizations}
Rewrite rules have been used very early as a way to automate the optimization process of functional programs~\cite{jones01playing}.
Similar to us, Spiral~\cite{pueschel05spiral,Spampinato13LGen} also uses rewrite rules to optimize signal processing programs and was more recently adapted to other applications such as linear algebra.
Chellappa et al. show how Spiral can be used to automatically parallelize code for the IBM Cell BE processor~\cite{Chellappa09computer}.
In contrast our rules and hardware patterns are expressed at a much finer level, allowing for highly specialized and optimized code generation.

\paragraph{High-level Code Generation for GPUs} 
A large body of work has explored how to automatically generate high performance code for GPUs.
Dataflow programming models such as IBM's LiquidMetal~\cite{dubach12compiling} or StreamIt~\cite{thies02streamit} have been used to automatically produce GPU code with OpenCL or CUDA~\cite{hormati11sponge,huynh12scalable,udupa09software}.
Directive based approach have also been used such as OpenMP to CUDA~\cite{lee09openmp}, OpenACC to OpenCL~\cite{reyes12openaccgpu}, or hiCUDA~\cite{han11hicuda} which translates sequential C code to CUDA.
Other works include the automatic mapping of data-parallel programs to the IBM Cell processor~\cite{Wang09automatic}.
Delite~\cite{brown11heterogeneous,chafi11domain}, a system that enables the creation of domain-specific languages, can also target mutlicore CPUs or GPUS.
Unfortunately, all these approaches do not provide full performance portability since the mapping of the application assumes a fixed platform and the optimizations and implementations are targeted at a specific device.
X10~\cite{charles05x10}, a language for high performance computing, can also be used to program GPUs~\cite{cunningham11gpu}.
However, the programming style remains low-level since the programmer has to express in X10 the same low-level operations found in CUDA or OpenCL.

Recently, researchers have looked at generating efficient GPU code for loops using the polyhedral framework~\cite{verdoolaege13polyhedral}.
This framework enables the exploration of various optimizations such as use of local memory or vectorization.
Finally, Petabricks~\cite{ansel09petabricks} takes a different approach by letting the programmer specify different algorithms implementations.
The compiler and runtime then choose the most suitable one based on an adaptive mechanism and can produce OpenCL code~\cite{phothilimthana13portable}.
Compared to our work, they generate optimized code by relying on static analysis.
Our code generator does not make any decisions nor perform any analysis since the optimization process happens at a higher level within our rewrite rules.
\from{PACT end}

