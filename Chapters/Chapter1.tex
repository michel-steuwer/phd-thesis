% Chapter 1: Introduction

\chapter{Introduction} % Chapter title

\label{ch:introduction} % For referencing the chapter elsewhere, use \autoref{ch:name}

Computer architectures have radically changed in the last 20 years with the introduction of multi- and many-core designs in almost all areas of computing:
from mobile devices and desktop computers to data centers and high-performance computing systems.
Furthermore, recently novel types of specialized architectures, especially \emph{Graphics Processing Units} (\GPUs), have been developed featuring large amounts of processing units for accelerating computational intensive applications.
Parallelism and specialization are seen by many computer architects as two crucial answers to the challenge of increasing performance and at the same time improving the energy efficiency~\cite{}.

% The development of mainstream programming languages and models have not kept pace with the hardware development.
The modern multi- and many-core processors are often still programmed with programming languages developed in the 1980s and 1990s, like \Cpp or Java, or even older languages like C (from the 1970s) or Fortran (from the 1950s).
These languages have a simplified view of the underlying hardware, often more or less directly resembling the Von Neumann architecture.
Multi-core processors are programmed with low-level libraries providing \emph{threads} where the programmer explicitly controls the computations on each core executing in parallel with the other cores.
This style of programming has turned out to be extremely difficult, as threads running concurrently on distinct cores can modify shared data leading to serious problems like deadlocks, race conditions, and non-determinism.
Even if programmers develop correct parallel implementations, optimizing these implementations for modern parallel processors is a challenging task even for experienced programmers.
Due to the lack of better programming systems, programmers are forced to manually develop low-level hardware-specific implementations optimized towards a particular hardware architecture to achieve high performance, which limits portability.

This thesis describes our work to address these issues.
In the first part of the thesis, we present a high-level programming model and its implementation which is designed to simplify the programming of modern parallel processors, especially systems comprising multiple \GPUs with many cores.
In the second part, we discuss a system which generates portable high-performance code for different modern parallel processors, particularly a multi-core \CPU and two different types of \GPUs, from a single high-level program representation.
In the final part of the thesis, we outline how these two systems can be combined in the future to obtain a programming system which simplifies programming and achieves high performance on different hardware architectures.

\noindent
We will start our discussion with an introduction on programming modern parallel processors.
From this we will identify the two main challenges we address in this thesis:
the \emph{programmability} challenge and the \emph{performance portability} challenge.
We will then list our contributions before presenting the outline for the rest of the thesis.

\section{Multi-Core Processors and their Programming}

% processors become parallel
Traditionally, the performance of microprocessors has been mainly increased by boosting the clock frequency, optimizing the execution flow, and improving caches~\cite{Sutter2005}.
Processor development drastically changed around 10 years ago, as shown in \autoref{fig:cpuTrend}.
While the number of transistors continues to grow according to Moore's Law~\cite{Moore1998}, the clock frequency and power consumption hit a plateau around 2005.
\begin{figure}
  \centering
  \includegraphics[width=.9\linewidth]{Figures/intelCPUTrend}
  \caption[Development of Intel Desktop \CPUs over time]{Development of Intel Desktop \CPUs over time. Inspired by~\cite{Sutter2005}}
  \label{fig:cpuTrend}
\end{figure}
The reason for this lies in multiple physical effects, especially the physical limit of signal speed and increased heat development,  mainly due to the increased power consumption, both of which limit further increasing of the clock frequency.
The predominant answer to this challenge has been the introduction of multiple distinct \emph{cores} within one processor which can operate independently and concurrently.
% parallelism is now explicit to the programmer
Parallelism has been exploited in computer architectures for a long time at the bit level and the instruction level, but different than before with the introduction of multiple cores, this new \emph{thread-level parallelism} has been made explicit to the programmer.
This has profound effects on the design and development of software running on modern multi-core processors.
% programming with threads is a terrible idea
Most programming languages allow programmers to exploit multiple cores by the means of \emph{threads}.
A thread is a sequence of instructions defined by the programmer which can be scheduled by the runtime system to be executed on a particular core.
Multiple threads can be executed concurrently and the programmer is responsible to use synchronization primitives, like locks or semaphores, for coordinating the execution of multiple threads.
Programming with threads is widely regarded as extremely difficult~\cite{Lee06} mainly because multiple threads operate on a shared memory region.
Even when programs are constructed carefully subtle problems can arise which are hard to detect but can have severe consequences.
Nevertheless, threads are still the de-facto standard for programming multi-core \CPUs.

\bigskip

\noindent
% still moores law, no dennards scaling
% new hardware trend driven by energy efficiency, specialization, a.k.a heterogeneity, most prominent example: GPUs
While Moore's law still holds and transistor counts increase by further shrinking the transistor size, a related observation, known as Dennard scaling~\cite{DennardRiBaLe1974}, has broken down.
Dennard scaling suggested that power consumption is proportional to the area used for transistors on the chip.
Combined with Moore's law this meant, that the energy efficiency of processors doubled roughly every 18 month.
The primary reason for the breakdown of Dennard scaling around 2005 were physical effects appearing at small scale, especially current leakage and increased heat development.
This has led to architectures which particularly focus on their energy efficiency, the most prominent example of such architectures are modern graphics processing units (\GPUs).
Originally developed for accelerating the rendering of complex graphics and 3D scenes, \GPU architectures have been recently generalized to support more types of computations.
Some people refer to this development using the term general-purpose computing on graphics processing units (\GPGPU).

Technically \GPU architectures are multi-core architectures like modern multi-core \CPUs, but each individual core on a \GPU typically has dozens or hundreds of functional units which can perform computations in parallel following the \emph{Single Instruction, Multiple Data} (SIMD) principle.
These types of architectures are optimized towards a high throughput of computations, therefore, they focus on performing a large amount of operations in parallel and feature no, or only small, caches to prevent or mitigate latencies of the memory:
if a thread stalls waiting for the memory, another thread takes over and keeps the core busy.
For multi-core \CPUs switching between threads is more expensive, therefore, \CPUs are instead optimized to avoid long latencies when accessing the memory with a deep cache hierarchy and advanced architectural features, like long pipelines and out-of-order execution, all of which are designed to keep each core busy.

\pagebreak
\section{The Programmability Challenge}

% programming on the CPU is a nightmare
Developing efficient programs for modern parallel processors is a challenging task.
The current approach for programming multi-core \CPUs is using multithreading, \ie, explicitly managing the individual execution paths (called \emph{threads}) in a single program.
Threads communicate via a shared memory, where all threads have the possibility to modify any data item.
This fundamental principle gives the programmer full control over the interaction of the threads, but greatly complicates the reasoning about the programs behavior.
The extensive and explicit control implies a low level of abstraction, where the programmer deals with many details of the execution explicitly.
This makes the development of programs using threads complex and certain types of bugs likely, like deadlocks and race conditions, where the computed result depends on the order the threads are executed in.
This ultimately leads to extensive problems during the development cycle as well as possible non-deterministic and undesirable behavior at runtime.

% not better on the GPU
Programming \GPUs requires a different programming approach, as it is obviously infeasible to manage thousands of threads explicitly.
\CUDA~\cite{CUDAProgrammingGuide} and \OpenCL~\cite{OpenCL} are the two most popular approaches for programming \GPUs.
Both approaches are quite similar and require the programmer to write a function, called \emph{kernel}, which will be executed in parallel on the \GPU.
The kernel usually contains source code for identifying the thread executing the particular kernel instance which is used to coordinate on which data each thread operate on.
While this \emph{Single Program, Multiple Data} (SPMD) programming model makes the management of threads feasible, it does not address most problems associated with the low-level programming using threads.
For example, explicit synchronization of threads is still required and problems like deadlocks and race conditions can still easily occur.
Furthermore, \GPU programming currently involves extensive boilerplate code to manage the execution on the \GPU, including the compilation of the kernel (in \OpenCL), transferring data to and from the \GPU, and launching the kernel by explicit specifying the number of threads to be started in parallel.

% There is a challenge: Programmability
All of this leads to our first central research \emph{challenge of programmability}:
to design a programming approach which significantly raises the level of abstraction and, thus, greatly simplifies the development process of parallel applications, without sacrificing high-performance on multi-core \CPUs and \GPUs.
We will underpin the necessity to address this challenge with an extensive study of state-of-the-art \GPU programming using a concrete application example in \autoref{chapter:skelcl}.
In \autoref{part:skelcl} of this thesis, we introduce, discuss, and evaluate a novel high-level programming model which addresses this challenge.

\pagebreak
\section{The Performance Portability Challenge}

% extracting performance is hard and not always predictable
Developing functionally correct applications for multi-core \CPUs or \GPUs is very challenging as discussed in the preceding subsection.
Unfortunately, the development of high-performing optimized applications is even more demanding -- even for experienced programmers.
Applying optimizations often requires substantial restructuring of existing code.
Optimizations are often applied ad-hoc, following certain ``rules of thumb'' derived from small-scale profiling and experiments as well as previously gained experience.
There exists no widely established approach for performing optimizations more systematically.
Especially, the performance implications of an assumed optimization are currently not predictable:
sometimes supposed optimizations lead to actual slowdowns in certain circumstances.

% compilers do a good job on low-level stuff, but fail for high-level optimizations
Modern optimizing compilers perform very sophisticated low-level optimizations, including various loop optimizations and optimizations based on data-flow analysis.
For all these type of optimizations, the compiler first has to recover parts of the high-level program semantics using code analysis before applying the optimization.
As these analyses quickly become very complex, compilers are ultimately limited in their optimization capabilities.
High-level optimizations which might restructure the entire source code are often out of the reach of low-level compiler optimizations and, therefore, require explicit and often extensive manual refactoring.
The state-of-the-art optimizing compilers are lacking the high-level semantic information necessary for performing this kind of optimizations.

% optimizations are hardware-specific
Many optimizations exploit particular hardware-specific features of the target architecture and are, thus, inherently not portable.
As in current low-level programing models the programmers often encode these optimizations directly, programmers are confronted with the trade-off of either fully optimize their code for performance -- at the expense of code portability -- or implement a portable version -- at the expense of missing out on the highest performance possible.
Nowadays, programmers often choose a middle ground by introducing conditionals and branches in their code to select optimizations for the given hardware, while maintaining portability.
This, obviously, complicates the development process and decreases maintainability.

% There is a challenge: Performance Protability
This leads to our second central research \emph{challenge of performance portability}:
to design a systematic approach that generates an optimized, hardware-specific code for different target architectures from a single portable, high-level program representation.
We will confirm the lack of portability of performance in current state-of-the-art programming approaches using a study comparing optimizations on three different hardware architectures in \autoref{chapter:codeGeneration}.
In \autoref{part:codeGeneration} of this thesis, we introduce, discuss, and evaluate a novel systematic code generation approach which addresses this challenge.


\section{Contributions of this Thesis}

This thesis makes the following four major contributions:

\subsection*{\hspace{2em}For addressing the programmability challenge:}

\begin{description}
  \item[A High-level programming model for mutli-\GPU systems]\hfill\\[-1em]
    We present the design and implementation of our high-level programming model \SkelCL for programming multi-\GPU systems.
    \SkelCL offers three high-level features for simplifying programming of \GPUs: container data types, algorithmic skeletons, and distributions.
    We discuss our \Cpp library implementation of the \SkelCL programming model which is deeply integrated with modern \Cpp features.
    Finally, we evaluate the level of abstraction and runtime performance of \SkelCL using a set of application studies.

  \item[Two novel algorithmic skeletons]\hfill\\[.25em]
    We introduce two novel algorithmic skeletons specialized towards \emph{allpairs} and \emph{stencil} computations.
    We present their formal definitions and discuss possible use cases in real-world applications.
    For the allpairs skeleton we identify and formally define an optimization rule, which enables an optimized implementation on \GPUs.
    We discuss and evaluate efficient implementations for both skeletons on a multi-\GPU system.
\end{description}

\subsection*{\hspace{2em}For addressing the performance portability challenge:}

\begin{description}
  \item[A formal system for rewriting pattern-based programs]\hfill\\[-1em]
    We introduce a novel system comprising a set of high-level and low-level patterns along with a set of rewrite rules.
    The rewrite rules express high-level algorithmic and low-level optimization choices which can be systematically applied to transform a program expressed with our functional patterns.
    We prove the soundness of our approach by showing that applying the rewrite rules does not change the program semantics.

  \item[A code generator offering performance portability]\hfill\\[.25em]
    Based on our formal system of rewrite rules, we present a novel code generator which generates from a single pattern-based program highly efficient \OpenCL code for three different hardware platforms:
    one multi-core \CPU and two different \GPU architectures.
    We discuss the design of our code generator and evaluate the performance of the generated \OpenCL code as compared against highly optimized library codes.
\end{description}

\section{Outline of this Thesis}
The remainder of this thesis is structured as follows.

\subsection*{Part I -- Introduction}

\paragraph{\autoref{chapter:background}} provides an introduction into the main aspects of programming modern multi-core \CPUs and \GPUs.
We will start with the hardware where we mainly focus on \GPUs, discuss in detail their architectures, the state-of-the-art programming approaches, and optimization opportunities.
We will then shift our focus and introduce the algorithmic skeleton parallel programming model including its origins in functional programming.


\subsection*{Part II -- \SkelCL}

\paragraph{\autoref{chapter:skelcl}} addresses the programmability challenge identified in this chapter by introducing our novel \SkelCL high-level programming model targeted towards multi-\GPU systems and its implementation as a \Cpp library.
We will start by further motivating the need for high-level abstractions to simplify the software development process.
Then we present \SkelCL's three high-level features:
1) parallel container data types, 2) algorithmic skeletons, and 3) data distributions.
The combination of these features greatly simplifies the programming of multi-\GPU applications.
Finally, we will discuss the \SkelCL library implementation and its integration within \Cpp.

\paragraph{\autoref{chapter:skelcl-evaluation}} evaluates the \SkelCL programming model and library using multiple application studies.
We will start with discussing typical benchmark applications like the Mandelbrot set computation and popular benchmarks from linear algebra, including matrix multiplication.
We then continue with discussing more advanced applications from different areas of computing, including image processing, medical imaging, and physics.
We evaluate the improvement of programmability and the performance of \SkelCL as compared with implementations written using the state-of-the-art \OpenCL programming approach.


\subsection*{Part III -- Code Generation using Patterns}

\paragraph{\autoref{ch:fifth}} addresses the performance portability challenge identified in this chapter by introducing a novel approach for generating highly optimized hardware-specific code for different target architectures from a single portable, high-level program representation.
We will start the chapter with a study showing that optimizations in \OpenCL are not portable across different hardware architectures.
This emphasizes the motivation for performance portability in order to preserve maintainability while increasing performance.
We continue by discussing the design of our code generation approach consisting of a set of parallel patterns combined with a set of rewrite rules.
The rewrite rules allow to systematically rewrite pattern-based programs without changing their semantics which we will show formally.
Finally, we discuss how the rewrite rules can be systematically applied to optimize pattern-based programs for a particular hardware architecture.

\paragraph{\autoref{ch:sixth}} evaluates our code generation approach using a set of benchmark applications.
We evaluate the performance of the code generated using our approach and compare it against highly tuned library code on three different hardware architectures.

\subsection*{Part IV -- Summary \& Conclusions}

\paragraph{\autoref{ch:seventh}} summarizes Part I and Part II and describes their relation.
We will discuss how \SkelCL, presented in Part II, could be combined in the future with the code generator, presented in Part III, to obtain a system offering both high-level abstractions to simplify programming and portable, high performance on different hardware architectures.

\paragraph{\autoref{ch:eighth}} concludes the thesis with a comparison against related work and discussing possible future extensions of our research.

