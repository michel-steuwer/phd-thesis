% Chapter 1: Introduction

\chapter{Introduction} % Chapter title

\label{ch:introduction} % For referencing the chapter elsewhere, use \autoref{ch:name} 

\section{Introduction}

\from{HIPS begin}
\subsection{HIPS}
Modern \emph{Graphics Processing Units} (GPUs) provide large numbers of processing units and a high memory bandwidth.
To enable \emph{General-Purpose Computation on GPU} (GPGPU), new programming models have been introduced, the two most popular approaches being CUDA and OpenCL~\cite{KiHw-10,OpenCL-10}.
While both programming models are similar, CUDA is a proprietary software developed by NVIDIA, whereas OpenCL is an open industry standard.

Programming GPUs with OpenCL (and CUDA as well) still remains a difficult task because it is a low-level programming model:
Data has to be transferred explicitly from the system's main memory (accessible by the CPU) to the GPU's memory and back.
Moreover, memory allocation and deallocation also has to be explicitly controlled by the programmer.
All this results in a lot of low-level boilerplate code in GPU programs.

In this paper, we present \emph{SkelCL} --- a library which introduces an easy-to-use, high-level approach for GPU programming.
SkelCL provides an abstract vector data type to perform data exchange between CPU and GPU implicitly.
Pre-implemented communication and computation patterns (a.k.a. \emph{algorithmic skeletons}) based on this data type hide boilerplate code from the programmer inside the skeletons' implementations.
Because SkelCL is based on OpenCL, it is not bound to a specific hardware and can be executed on any OpenCL-capable device.
\from{HIPS end}

\from{ASHES begin}
\subsection{ASHES}

The two popular programming approaches for systems with \emph{Graphics Processing Units} (GPUs) --- CUDA and OpenCL~\cite{OpenCL-11,CUDA-12,KiHw-10} --- work at a low level of abstraction.
They require the programmer to explicitly manage the GPU's memory, including data allocations and transfers to/from the system's main memory.
This leads to long, complex and, therefore, error-prone code.
The emerging multi-GPU systems are particularly challenging for the application developer:
they additionally require explicit data exchanges between individual GPUs, including low-level pointer arithmetics and offset calculations, as well as an adaption of algorithms for execution on multiple GPUs.

In this paper, we introduce SkelCL --- a library for high-level GPU programming.
SkelCL comprises two major abstraction mechanisms: a set of algorithmic skeletons~\cite{Cole-89} and an abstract vector data type.
Skeletons offer several frequently used pre-implemented patterns of parallel communication and computation to the application developer~\cite{RaG-03}.
The vector data type enables implicit data transfers between the system's main memory and GPUs.
Using these abstractions, SkelCL frees the programmer from writing low-level and error-prone boilerplate code when programming multi-GPU systems, and still provides all useful features of the OpenCL standard.

The focus of this paper, as compared to our introductory article~\cite{StKG-11}, is the set of specific features and mechanisms of SkelCL for programming multi-GPU systems on a high level of abstraction.
We describe how SkelCL implements skeletons on multi-GPU systems and show how SkelCL's concept of \emph{data distribution} frees the user from low-level memory management, without sacrificing performance in real-world applications that use multiple GPUs.

\from{ASHES end}

\from{Paraphrase begin}
\subsection{Paraphrase}
Application programming for GPUs (Graphics Processing Units) is complex and error-prone.
The popular programming models for systems with GPUs -- CUDA and OpenCL~\cite{CUDA-12,KiHw-10,OpenCL-11} -- require the programmer to explicitly manage GPU's memory, including (de)allocations and data transfers to/from the system's main memory.
This leads to lengthy, low-level, complex and thus error-prone code.
For emerging systems with multiple GPUs, CUDA and OpenCL additionally require an explicit implementation of data exchange between GPUs and separate management of each GPU.
This includes low-level pointer arithmetics and offset calculations, as well as explicit program execution on each GPU.
Neither CUDA nor OpenCL offer specific support for such systems, which makes their programming even more complex.

In this paper, we first briefly describe our SkelCL library~\cite{StKG-11} for high-level single- and multi-GPU computing.
It offer pre-implemented recurring computation patterns (\emph{skeletons}) for simplified GPU programming.
In addition, the application developer is freed from memory management, which is done implicitly in SkelCL.

As a specific contribution of the paper, we add a new two-dimensional data type and an additional skeleton to SkelCL.
The \emph{matrix} data type complements the one-dimensional vector data type for working with two-dimensional data, e.g., matrices or images.
The new \emph{MapOverlap} skeleton executes a given function on every element of the input data, using also the values of its neighboring elements.
Another novel contribution of this paper is the data distribution mechanism for simplifying two-dimensional data processing on systems with multiple GPUs.
Finally, we present an application case study using the matrix data type and the MapOverlap skeleton -- Sobel edge detection for 2D images -- and report experimental results using SkelCL.

The paper is organized as follows.
First we briefly introduce the basics of SkelCL in Section~\ref{sec:skelcl} using our previous work~\cite{StKG-11}.
The MapOverlap skeleton is then introduces in Section~\ref{sec:skelcl:mapoverlap} and the matrix data type in Section~\ref{sec:skelcl:matrix}.
In Section~\ref{sec:application_study} we demonstrate how both are used together to implement an application from the area of image processing: the Sobel edge detection.
We report experimental results and we evaluate performance and usability of our approach.
Section~\ref{sec:conclusion} concludes the paper and compares our approach to related work.
\from{Paraphrase end}


\from{ICCS begin}
\subsection{ICCS}
Modern high-performance computer systems increasingly employ \emph{Graphics Processing Units} (GPUs) and other accelerators.
The state-of-the-art application development for systems with GPUs is cumbersome and error-prone, because GPUs are programmed using relatively low-level models like CUDA~\cite{CUDA-11} or OpenCL~\cite{OpenCL-10}.
These programming approaches require the programmer to explicitly manage GPU's memory (including memory (de)allocations, and data transfers to/from the system's main memory), and to explicitly specify parallelism in the computation.
This leads to lengthy, low-level, complex and thus error-prone code.
For multi-GPU systems, programming with CUDA and OpenCL is even more complex, as both approaches require an explicit implementation of data exchange between GPUs, as well as separate management of each GPU, including low-level pointer arithmetics and offset calculations.

In this paper, we describe the SkelCL (Skeleton Computing Language) -- a high-level programming model for parallel systems with multiple GPUs.
The model is based on the OpenCL standard and extends it with three novel, high-level mechanisms:
\begin{itemize}
  \item[1)] \emph{parallel skeletons}: pre-implemented high-level patterns of parallel computation and communication which can be customized and combined to express application-specific parallelism;
  \item[2)] \emph{parallel container data types}: collections of data (e.\,g., vectors and matrices) that are managed automatically across all GPUs in the system;
  \item[3)] \emph{data (re)distributions}: an automatic mechanism for describing data distributions and re-distributions among the GPUs of the target system.
\end{itemize}

The paper describes how the SkelCL model is used for programming a sample real-world application from the area of medical imaging, and how the model is implemented as the SkelCL programming library, using C++.
Our focus is on programming methodology;
therefore, we motivate our work using one typical imaging application and then study it in great detail throughout the paper.
\from{ICCS end}

\from{HLPP begin}
\subsection{HLPP}
Algorithmic skeletons enable application developers to program parallel systems at a high level of abstraction~\cite{Cole-04}.
Skeletons abstract recurring patterns of parallel programming and provide efficient parallel implementations for these patterns.
The application developer uses skeletons by providing application-specific code (so-called customizing function) which customizes the skeletons' behavior.
Skeletons thus allow the application developer to focus on the structure of the algorithms, rather than on their implementation details.

We develop SkelCL~\cite{StKG-12} -- a skeleton library for programming modern many-core architectures, especially systems with Graphics Processing Units (GPUs).
Programming these systems using the current low-level approaches like CUDA~\cite{NVIDIA-12} and OpenCL~\cite{OpenCL1.2} is challenging: parallelism has to be expressed explicitly by defining and executing parallel \emph{kernels}, and memory management is cumbersome, as data has to be moved manually to and from the GPU memory.
By providing skeletons on container data types, SkelCL alleviates programming of systems with GPUs:
parallelism is expressed implicitly, using skeletons, and memory management is performed automatically by the SkelCL implementation built on top of OpenCL.
The especially tricky programming of multi-GPU systems is greatly simplified by SkelCL's data distribution mechanism which automatically moves data between multiple GPUs.

In this paper, we aim at \emph{allpairs computations} which occur in a variety of applications, ranging from matrix multiplication and pairwise Manhattan distance computations in bioinformatics~\cite{DaDQR-09} to N-Body simulations in physics~\cite{ArSV-09}.
These applications share a common computational pattern:
for two sets of entities, the same computation is performed independently for all pairs in which entities from the first set are combined with entities from the second set.
Previous work discussed specific allpairs applications and their parallel implementations on multi-core CPUs~\cite{ArSV-09}, the Cell processor~\cite{WiSK-09}, and GPUs~\cite{DaDQR-09,SaA-13};
it demonstrated that developing well-performing implementations for allpairs is challenging, especially on GPU systems, as it requires exploiting their complex memory hierarchy and assumes a deep knowledge of the target hardware architecture.

The contributions and structure of this paper are as follows.
To enable application developers to efficiently use the allpairs computational pattern, we formally define the allpairs skeleton and provide its generic parallel implementation in OpenCL (Section~\ref{sec:allpairs_skeleton}).
We optimize the use of the GPU memory hierarchy by implementing a specialized version of the allpairs skeleton with a customizing function that follows a certain memory access pattern (Section~\ref{sec:opt_allpairs_skeleton}).
We show that our implementations can be used for single- and multi-GPU systems (Section~\ref{sec:multi_gpu}).
We evaluate the runtime performance and the programming effort of the allpairs skeleton within the SkelCL library, using matrix multiplication as application study (Section~\ref{sec:experiments}).
We discuss related work and conclude in Section~\ref{sec:conclusion}.
\from{HLPP end}

\from{PaCT begin}
\subsection{PaCT}
Modern high-performance computer systems become increasingly heterogeneous as they comprise in addition to multi-core processors, also \emph{Graphics Processing Units} (GPUs), Cell processors, FPGA, and other accelerating devices, usually called \emph{accelerators}.
The state-of-the-art application programming for systems with GPUs is cumbersome and error-prone, because GPUs are programmed using explicit, low-level programming approaches like CUDA~\cite{CUDA-11} or OpenCL~\cite{OpenCL-10}.
These approaches require the programmer to explicitly manage GPU's memory (including memory (de)allocations, and data transfers to/from the system's main memory), and explicitly specify parallelism in the computation.
This leads to lengthy, low-level, complicated and, thus, error-prone code.
For multi-GPU systems, programming with CUDA and OpenCL is even more complex, as both approaches require an explicit implementation of data exchange between the GPUs, as well as disjoint management of each GPU, including low-level pointer arithmetics and offset calculations.

In this paper, we describe the \emph{SkelCL} (Skeleton Computing Language) -- our high-level programming approach for parallel systems with multiple GPUs.
The SkelCL programming model is based on the OpenCL standard and enhances it with three high-level mechanisms:
\begin{itemize}
  \item[1)] \emph{parallel container data types}: collections of data (in particular, vectors and matrices) that are managed automatically on all GPUs in the system;
  \item[2)] \emph{data (re)distributions}: an automatic mechanism for specifying in the application program suitable data distributions and re-distributions among the GPUs of the target system:
  \item[3)] \emph{parallel skeletons}: pre-implemented high-level patterns of parallel computation and communication which can be customized to express application-specific parallelism, and combined to a large high-level code.
\end{itemize}
The structure of the paper is as follows. 
In Section \ref{sec:requirements} we formulate the requirements to a high-level programming approach for GPU systems, following from the analysis of compute-intensive applications. 
Section \ref{sec:skelcl} describes in detail our SkelCL approach. 
In Section \ref{sec:studies} we report experimental evaluation of our approach regarding both programming effort and performance. 
We compare to related work and conclude in Section in Section \ref{sec:conclusion}. 
\from{PaCT end}

\from{HiStencils begin}
\subsection{HiStencils}
Stencil computations play an important role in a number of different application domains including time-intensive scientific simulations, image processing and others.
Modern manycore architectures with Graphics Processing Units (GPUs) and other accelerators provide potentially tremendous computing power for challenging applications including stencil computations.

However, the current programming approaches for manycore architectures are low level, the most popular examples being OpenCL~\cite{OpenCL} and CUDA~\cite{CUDA}.
These approaches require the programmer to explicitly manage GPU's memory (including memory (de)allocations and data transfers to/from the system's main memory) and explicitly specify parallelism in the computation.
This leads to lengthy, low-level, complicated and, thus, error-prone code.
For multi-GPU systems, programming with CUDA and OpenCL is even more complex, as both approaches require an explicit implementation of data exchange between the GPUs, as well as disjoint management of each GPU, including low-level pointer arithmetics and offset calculations.
When implementing stencil computations, additional challenges arise, like handling out-of-bound memory accesses and achieving high performance by making efficient use of the fast but small local GPU memory.

In this paper, we present our SkelCL~\cite{StG-13b} approach to high-level, manycore programming, and we describe how it simplifies stencil programming and achieves competitive performance on multi-GPU systems.
SkelCL extends the OpenCL standard by three high-level mechanisms:
\begin{itemize}
  \item[1)] computations are easily expressed using pre-implemented parallel patterns (a.k.a. \emph{skeletons});
  \item[2)] memory management is simplified using \emph{container data types} for vectors and matrices;
  \item[3)] data movement in multi-GPU systems are handled automatically by SkelCL's \emph{(re)distribution mechanism}.
\end{itemize}

For stencil computations, we extend SkelCL with two specialized skeletons: MapOverlap for simple stencil computations, and Stencil for more complex, in particular iterative, stencil computations.
\from{HiStencils end}

\from{PACT begin}
\subsection{PACT}
Computing systems have now become extremely complex and diversified in part due to the recent emergence of GPUs (Graphic Processing Units) for general purpose computation.
The drawback of such systems is the extreme difficulty of extracting performance, requiring a deep understanding of the hardware.
This means that software written and tuned for today systems needs to be adapted frequently to keep pace with ever changing hardware.

Over the years, a wide range of languages, language extensions and frameworks have emerged for programming GPUs and other parallel devices. 
The two most common low-level languages are CUDA and OpenCL, both directly exposing hardware features.
Directive based languages such as Cilk~\cite{blumofe95cilk}, Open\-ACC~\cite{reyes12openaccgpu} and OpenMP~\cite{lee09openmp} or libraries like Intel TBB~\cite{reinders07inteltbb} have been proposed to reduce the complexity of developing code for multicore CPUs and GPUs.
These approaches simplify writing applications, but still require the programmer to tune the application for a particular hardware device.
As a result, \emph{performance portability} remains elusive; code optimized for one device might only achieve a fraction of the performance on a different device, as shown by our results.

Several high-level programming models have been proposed to address this issue.
Petabricks~\cite{phothilimthana13portable} allows the programmer to express different algorithm implementations and automatically picks the best one using auto-tuning.
It can also automatically identify and translate algorithms that are suitable for GPU execution.
Higher-level dataflow programming language such as StreamIt~\cite{hormati11sponge} or LiquidMetal~\cite{dubach12compiling} have been designed with a similar goal in mind.
Both languages use dedicated backend compiler for different hardware targets such as GPUs.
A recent trend is the adoption of a more functional programming approach. 
For instance, Nvidia's NOVA~\cite{collins13nova} language exposes algorithmic patterns such as \pat{map} or \pat{reduce} as function primitives recognized by the backend compiler.
While definitively a step in the right direction, all these approaches still rely on hard-coded device-specific implementations or heuristics.
When hardware changes occur, the backend compiler has to be re-tuned or re-engineered.% in the presence of a new hardware features.

The root of the problem lies in the gap in the system stack between high-level algorithmic concepts on the one hand and low-level hardware paradigms on the other.
To bridge this gap, we propose a novel approach based on algorithmic pattern and low-level hardware pattern composition and a set of rewrite rules.
While similar in spirit to Spiral~\cite{pueschel05spiral}, our approach relies on fine grain hardware patterns representing CPUs and GPUs hardware features exposed by the OpenCL programming model.
The rewrite rules are used to automatically derive semantically equivalent low-level expressions from high-level algorithm expressions written by the programmer.
Once derived, we automatically generate high performance OpenCL code based on these expressions.
This makes the code generator very simple since all decisions related to optimizations are handled during the automatic rewriting process.

The power of our approach lies in the rewrite rules, written once by an expert system designer.
These rules encode the different algorithmic choices and low-level optimizations.
The rewrite rules play the dual role of enabling the composition of algorithmic patterns and enabling the lowering of these patterns onto the low-level hardware paradigms.
This results in a clear separation of concerns between high-level algorithmic patterns and low-level hardware paradigms.
Producing high performance code is achieved by automatically searching the implementation space defined by applying the rewrite rules.

This paper demonstrates the applicability of our approach on a map-reduce style of patterns.
Map and reduce are common performance critical programming patterns used as building blocks for many algorithms~\cite{asanovic06thelandscape}.
We compare our approach with highly-tuned linear algebra functions extracted from the state-of-the-art libraries and with larger type of benchmarks such as BlackScholes.
Our performance is competitive with highly-tuned implementations of the BLAS linear algebra library such as Nvidia GPU CUBLAS~\cite{cuBLAS}, AMD GPU clBLAS~\cite{clBlas} and ATLAS~\cite{whaley98atlas} on the CPU.
Interestingly, this is achieved without specializing the compiler towards a specific hardware target.
Instead, we automatically build and search the implementation space by expressing our high-level algorithmic patterns as a composition of low-level hardware patterns.
Our paper makes the following key contributions:
\vspace{0.2em}
\begin{itemize}
  \item design of \textbf{low-level hardware patterns} that represents the OpenCL programming model; \vspace{0pt}
  \item develop a powerful set of \textbf{algorithmic and low-level rules} that expresses algorithmic and optimization choices; \vspace{0pt}
  \item demonstrate that \textbf{performance portability} can be achieved via an automated search of possible implementations, leading to performance in par with the best hand-tuned versions. \vspace{0pt}
\end{itemize}
\from{PACT end}


% \section{Introduction} end


\from{PaCT begin}
\section{Requirements to a High-Level Programming Model (PaCT)}
To simplify programming for a system with multiple GPUs, the following high-level abstraction are desirable:

\paragraph{Parallel container data types}
Compute-intensive applications typically operate on a (possibly big) set of data items.
Managing memory hierarchy of multi-GPU systems explicitly is complex and error-prone because low-level details, like offset calculations, have to be programmed manually.
A high-level programming model should be able to make collections of data automatically accessible to all GPUs in the target system and it should provide an easy-to-use interface for the application developer.

\paragraph{Distribution and redistribution mechanisms}
To achieve scalability of applications on systems comprising multiple GPUs, it is crucial to decide how the application's data are distributed across all available GPUs.
Applications often require different distributions for their computational steps.
Distributing and re-distributing data between GPUs in OpenCL is cumbersome because data transfers have to be managed manually and performed via the CPU. 
Therefore, it is important for a high-level programming model to allow both for describing the data distribution and for changing the distribution at runtime, such that the system takes care of the necessary data movements.

\paragraph{Recurring patterns of parallelism}
While the concrete operations performed in an application are (of course) application-specific, the general structure of parallelization often follows some common parallel patterns that are reused in different applications.
For example, operations can be performed for every entry of an input vector, which is a well-known pattern of data-parallel programming, or two vectors are combined element-wise into an output vector, which is again a common pattern of parallelism.
It would be, therefore, desirable to express the high-level structure of an application using pre-defined common patterns, rather than describing the parallelism explicitly in much detail.
\from{PaCT end}

