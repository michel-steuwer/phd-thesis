\section{Rewrite Rules}
\label{section:rules}

This section introduces our set of rewrite rules that transform high-level expressions written using our algorithmic patterns into semantically equivalent expressions.
One goal of our approach is to keep each rule as simple as possible and only express one fundamental concept at a time.
For instance the vectorization rule, as we will see, is the only rule expressing the vectorization concept.
This is different from most previous library or compiler approaches that would provide or produce a special vectorized version of different algorithmic patterns such as map or reduce.
The advantage of our approach lies in the power of composition:
many rules can be applied successively to produce expressions that compose hardware concepts or optimizations and that are provably correct by construction.

Similarly to our patterns, we distinguish between algorithmic and \OpenCL-specific rules.
Algorithmic rules produce derivations that represent different algorithmic choices.
Our \OpenCL-specific rules map expressions to \OpenCL patterns.
Once the expression is lowered and consist only of \OpenCL patterns, it is possible to produce \OpenCL code for each single pattern straightforwardly with our code generator as described in the following section.

We write $a \rightarrow b$ for a rewrite rule which allows to replace the occurrence of $a$ in an expression with $b$.
Sometimes multiple rewrites are valid then we write $a \rightarrow b | c$ to indicate the choice to replace $a$ wither with $b$ or $c$.

We will first define each rule and give an informal description which should give some intuition of that the rules ``make sense'', \ie, that applying the rule does not change the programs semantics.
In \autoref{section:rules:proofs} we provide proof for the correctness of each rules, before we discuss how the rules can be applied to rewrite expressions in the following section.

%\paragraph{Syntax and Rule Derivation}
%Some rules can only be activated if certain conditions are true.
%We use the syntax $[pre:post]$ to represent the pre and post conditions of a rule.
%The pre condition $pre$ corresponds to the list of conditions that must be true for the rule to be applied.
%The post condition $post$ is set for any function bound to the pattern.
%The $\overline{\rule[-.3\baselineskip]{0pt}{1.5ex}\hspace{1.5ex}}$, $\wedge$, and $\vee$ corresponds to the logical \emph{not}, \emph{and}, and \emph{or} operators respectively.

%We use leftmost derivation when applying the rules, which means that the leftmost non-terminal is always derived first.

\newenvironment{rerule}[1]%
{\begin{equation}\begin{array}{#1}\ignorespaces}%
{\end{array}\end{equation}%
\ignorespacesafterend}

\newenvironment{rerule*}[1]%
{\begin{equation*}\begin{array}{#1}\ignorespaces}%
{\end{array}\end{equation*}%
\ignorespacesafterend}


\newcommand{\comment}[1] {%
\{\text{\small #1}\}%
}

\subsection{Algorithmic Rules}
\label{section:rules:algo}

Each algorithmic rule formulates a true statement of the relationship of multiple algorithmic patterns.
Applying the rule allows to rewrite an expression and, by doing so, explore different implementations.
As the algorithmic rules are separated from the  \OpenCL rules, these rules can explore valid implementations regardless of their concrete implementation in a concrete low-level programming model like \OpenCL.

\paragraph{Identity}
The identity rule in \autoref{eq:algo:identity} specifies that it is always valid to compose any function $f$ with the identity function \emph{id}.
As we always operate on arrays, we technically compose $f$ with $\map\ id$.
%
\begin{rerule}{lclcl}
  f & \rightarrow & f \circ \map\ \textit{id} & | & \map\ \textit{id} \circ f
  \label{eq:algo:identity}
\end{rerule}
%
The \textit{id} function can act as a copy operation; this is, \eg, useful for expressing copies of an array to local memory when composed with the \toLocal \OpenCL pattern: $\toLocal\ (\map\ \textit{id})$.

\begin{proof}%[Proof]
  We first show the first option of the rule:
  \begin{align*}
    (f \circ \map\ \textit{id})\ xs
      &= f\ (\map\ \textit{id}\ xs)\\
      & \comment{definition of \map}\\
      &= f\ ([\textit{id}\ x_1, \textit{id}\ x_2, \ldots, \textit{id}\ x_n])\\
      & \comment{definition of \textit{id}}\\
      &= f\ xs
  \end{align*}
  Now we show the second option of the rule:
  \begin{align*}
    (\map\ \textit{id} \circ\ f)\ xs
      &= \map\ \textit{id}\ (f\ xs)\\
      & \comment{definition of \map}\\
      &= [\textit{id}\ (f\ xs)_1, \textit{id}\ (f\ xs)_2, \ldots, \textit{id}\ (f\ xs)_n]\\
      & \comment{definition of \textit{id}}\\
      &= f\ xs
  \end{align*}
\end{proof}


\paragraph{Iterate decomposition}
The rule in \autoref{eq:algo:iterate} expresses the fact that an iteration can be decomposed into several iterations.
%
\begin{rerule}{lcl}
  \iterateN\ (m+n)\ f
    & \rightarrow &
      \iterateN\ m\ f
        \circ \iterateN\ n\ f\\
  \iterateN\ 1\ f & \rightarrow & f
  \label{eq:algo:iterate}
\end{rerule}

\begin{proof}
  We first show the second option first:
  \begin{align*}
      & \comment{definition of \iterateN}\\
    \iterateN\ 1\ f\ xs
      &= \iterateN\ (1-1)\ f\ (f\ xs)\\
      & \comment{definition of \iterateN}\\
      &= f\ xs
  \end{align*}
  Now we proof the first option of the rule by induction over the natural number $n$.
  We start with the base case $n=0$:
  \begin{align*}
    (\iterateN\ m\ f \circ \iterateN\ 0\ f)\ xs
      &= \iterateN\ m\ f\ (\iterateN\ 0\ f\ xs)\\
      & \comment{definition of \iterateN}\\
      &= \iterateN\ m\ f\ xs
  \end{align*}
  We finish with the induction step $n-1 \rightarrow n$:
  \begin{align*}
    (\iterateN\ m\ f \circ \iterateN\ n\ f)\ xs
      &= \iterateN\ m\ f\ (\iterateN\ n\ f\ xs)\\
      & \comment{definition of \iterateN}\\
      &= \iterateN\ m\ f\ (\iterateN\ (n-1)\ f\ (f\ xs))\\
      & \comment{definition of $\circ$}\\
      &= (\iterateN\ m\ f\ \circ \iterateN\ (n-1)\ f) (f\ xs)\\
      & \comment{induction hypothesis}\\
      &= \iterateN\ (m+n-1)\ f\ (f\ xs)\\
      & \comment{definition of \iterateN}\\
      &= \iterateN\ (m+n)\ f\ xs
  \end{align*}
\end{proof}


\paragraph{Reorder commutativity}
The following \autoref{eq:algo:reorder} shows that if the data can be reordered arbitrarily, as indicated by the \reorder pattern, then, it does not matter if we apply a function $f$ to each element before or after the reordering.
%
\begin{rerule}{lcl}
  \map\ f \circ \reorder
    & \rightarrow & \reorder \circ \map\ f\\
  \reorder \circ \map\ f
    & \rightarrow & \map\ f \circ \reorder
  \label{eq:algo:reorder}
\end{rerule}

\begin{proof}
  We start by looking at the expression $\map\ f \circ \reorder$:
  \begin{align*}
    (\map\ f \circ \reorder)\ xs
      &= \map\ f\ (\reorder\ xs)\\
      & \comment{definition of \reorder}\\
      &= \map\ f\ [x_{\sigma(1)}, \ldots, x_{\sigma(n)}]\\
      & \comment{definition of \map}\\
      &= [f\ x_{\sigma(1)}, \ldots, f\ x_{\sigma(n)}]
  \end{align*}
  Now we investigate the expression $\reorder \circ \map\ f$:
  \begin{align*}
    (\reorder \circ \map\ f)\ xs
      &= \reorder\ (\map\ f\ xs)\\
      & \comment{definition of \map}\\
      &= \reorder\ [f\ x_1, \ldots, f\ x_n]\\
      &= \reorder\ [y_1, \ldots, y_n]\qquad \text{with } y_i = f\ x_i\\
      & \comment{definition of \reorder}\\
      &= [y_{\sigma(1)}, \dots, y_{\sigma(n)}]\\
      & \comment{definition of $y_i$}\\
      &= [f\ x_{\sigma(1)}, \ldots, f\ x_{\sigma(n)}]
  \end{align*}
  As both expression we started with can be simplified to the same expression they are equal and, therefore, both rules are correct.
\end{proof}

\paragraph{Split-join}
The split-join rule expressed by \autoref{eq:algo:splitjoin} partitions a map into two maps.
%
\begin{rerule}{lcl}
  \map\ f
    & \rightarrow &
      \join \circ \map\ (\map\ f) \circ \splitN\ n
  \label{eq:algo:splitjoin}
\end{rerule}
%
This allows us to nest map patterns in each other and, thus, maps the computation to the thread hierarchy of the \OpenCL programming model:
using the \OpenCL-specific rules (discussed in \autoref{section:rules:opencl}) we can rewrite $\map\ (\map\ f)$ for example to $\mapWorkgroup\ (\mapLocal\ f)$.
This is an expression we have seen in our motivation example (\autoref{fig:codeex}) for mapping a computation to \OpenCL work-group and work-item.

\begin{proof}
  We start from the right-hand side:
  \begin{align*}
    &(\join \circ \map\ (\map\ f) \circ \splitN\ n)\ xs\\
    &\qquad = \join\ (\map\ (\map\ f)\ (\splitN\ n\ xs))\\
    &\qquad \comment{definition of \splitN}\\
    &\qquad = \join\ (\map\ (\map\ f)\ [[x_1, \ldots, x_n], \ldots, [x_{m-n+1}, \ldots, x_m]])\\
    &\qquad \comment{definition of \map}\\
    &\qquad = \join\ [\map\ f\ [x_1, \ldots, x_n], \ldots, \map\ f\ [x_{m-n+1}, \ldots, x_m]]\\
    &\qquad \comment{definition of \map}\\
    &\qquad = \join\ [[f\ x_1, \ldots, f\ x_n], \ldots, [f\ x_{m-n+1}, \ldots, f\ x_m]]\\
    &\qquad \comment{definition of \join}\\
    &\qquad = [f\ x_1, \ldots, \ldots, f\ x_m]\\
    &\qquad \comment{definition of \map}\\
    &\qquad = \map\ f\ xs
  \end{align*}
\end{proof}

\paragraph{Reduction}
We seek to express the reduction function as a composition of other primitive functions, which is a fundamental aspect of our work.
From the algorithmic point of view, we first define a partial reduction pattern \partRed:
\begin{definition}
  \label{definition:pattern:parReduce}
  Let $\vec{x}$ be an array of size $n$ with elements $x_i$ where $0 < i \leq n$.
  Let $\oplus$ be an associative and commutative binary customizing operator with the identity element $\id_\oplus$.
  Let $m$ and $l$ be integer values where $n$ is evenly divisible by $l$ and $m = \frac{n}{l}$.
  The \partRed pattern is then defined as follows:
  \begin{align*}
    \partRed\ (\oplus)\ \id_\oplus\ l\ [x_1, x_2, \dots, x_n] \eqdef \vec{y}
  \end{align*}
  where
  \begin{align*}
    \reduce\ (\oplus)\ \id_\oplus\ \vec{y} = \reduce\ (\oplus)\ \id_\oplus\ \vec{x}
  \end{align*}
  %\begin{align*}
  %  &\partRed\ (\oplus)\ \id_\oplus\ l\ [x_1, x_2, \dots, x_n] \eqdef\\
  %  &\qquad [x_1 \oplus \dots \oplus x_l,\ x_{l+1} \oplus \dots \oplus x_{2l},\ \dots ,\ x_{n-l+1} \oplus \dots \oplus x_n]
  %\end{align*}
  The types of $(\oplus)$, $\id_\oplus$, $\vec{x}$, and \partRed are as follows:
  \begin{align*}
    (\oplus) &: ((\alpha, \alpha) \rightarrow \alpha),\\
    \id_\oplus &: \alpha,\\
    l &: \text{int},\\
    \vec{x} &: [\alpha]_n,\\
    \partRed\ (\oplus)\ \id_\oplus\ \vec{x} &: [\alpha]_m
  \end{align*}
\end{definition}
\noindent
This partial reduction reduces an array of $n$ elements to an array of $m$ elements.
The reduction can be expressed as a partial reduction combined with a full reduction as shown in \autoref{eq:algo:red}.
This rule ensures that we end up with one unique element.
And somewhat reflects our definition of \partRed.
% Another possible derivation consists in iterating a partial reduction until a full reduction is achieved (this is what $\infty$ represents).
%Note that our definition of \textit{reduce} remains correct since the result of a partial reduction is always composed with the reduction to ensure we end up with one unique element.
%
\begin{rerule}{lcl}
  \reduce\ f\ z
    & \rightarrow &
      \reduce\ f\ z \circ \partRed\ f\ z
  \label{eq:algo:red}
\end{rerule}

\begin{proof}
  We start from the right-hand side:
  \begin{align*}
    &(\reduce\ (\oplus)\ \id_\oplus \circ \partRed\ (\oplus)\ id_\oplus\ l)\ xs\\
    &\qquad = \reduce\ (\oplus)\ \id_\oplus\ (\partRed\ (\oplus)\ id_\oplus\ l\ xs)\\
    &\qquad \comment{definition of \partRed}\\
    &\qquad = \reduce\ (\oplus)\ \id_\oplus\ ys\\
    &\qquad\quad \text{where}\\
    &\qquad \reduce\ (\oplus)\ \id_\oplus\ ys = \reduce\ (\oplus)\ \id_\oplus\ xs
  \end{align*}
\end{proof}

\paragraph{Partial Reduction}
\autoref{eq:algo:part-red} shows the rewrite rules for the partial reduction.
%
\begin{rerule}{lcl}
  \partRed\ f\ z
    & \rightarrow &
      \reduce\ f\ z\\
    & | &
      \partRed\ f\ z \circ \reorder\\
    & | &
      \join \circ \map\ (\partRed\ f\ z) \circ \splitN\ n\\
    & | &
      \iterateN\ n\ (\partRed\ f\ z)
  \label{eq:algo:part-red}
\end{rerule}
%
The first possible derivation for partial reduction leads to the full reduction.
The next possible derivation expresses the fact that it is possible to reorder the elements to be reduced, expressing the commutativity property we demand in our definition of reduction (see \autoref{definition:pattern:reduce}).
The third derivation is actually the only place where parallelism is expressed in the definition of our reduction pattern.
This rule expressed the fact that it is valid to partition the input elements first and then reduce them independently.
Finally, the last possible derivation expresses the notion that it is possible to perform a partial reduction in an iterative process by repetitively applying the same partial reduction function.
\todo{sg: konkreter sagen was die Sache ist.}
This concept is very important when considering how the reduction function is typically implemented on \GPUs, as we saw in our discussion of the parallel reduction implementations shown in \autoref{lst:reduce0}--\ref{lst:reduce6}.

\begin{proof}
  We start with the first rule $\partRed\ (\oplus)\ \id_\oplus\ l \rightarrow \reduce\ (\oplus)\ \id_\oplus$:
  \begin{align*}
    &(\partRed\ (\oplus)\ \id_\oplus\ l)\ [x_1, \ldots, x_n]\\
    &\qquad \comment{choose $l=n$}\\
    &\qquad \Rightarrow (\partRed\ (\oplus)\ \id_\oplus\ n)\ [x_1, \ldots, x_n]\\
    &\qquad \comment{definition of \partRed}\\
    &\qquad = [y]\\
    &\qquad\quad \text{where}\\
    &\qquad \reduce\ (\oplus)\ \id_\oplus\ [y] = \reduce\ (\oplus)\ \id_\oplus\ [x_1, \ldots, x_n]
  \end{align*}
\end{proof}
\begin{proof}
  Second rule $\partRed\ (\oplus)\ \id_\oplus\ l \rightarrow \partRed\ (\oplus)\ \id_\oplus\ l \circ \reorder$.
  We start with the right side:
  \begin{align*}
    &(\partRed\ (\oplus)\ \id_\oplus\ l \circ \reorder)\ xs\\
    &\qquad = \partRed\ (\oplus)\ \id_\oplus\ l\ (\reorder\ xs)\\
    &\qquad \comment{definition of \reorder}\\
    &\qquad = \partRed\ (\oplus)\ \id_\oplus\ l\ [x_{\sigma(1)}, \ldots, x_{\sigma(n)}]\\
    &\qquad \comment{definition of \partRed}\\
    &\qquad = ys_\sigma\\
    &\qquad\quad \text{where}\\
    &\qquad \reduce\ (\oplus)\ \id_\oplus\ ys_\sigma = \reduce\ (\oplus)\ \id_\oplus\ [x_{\sigma(1)}, \ldots, x_{\sigma(n)}]
  \end{align*}
  We continue with the left side:
  \begin{align*}
    &(\partRed\ (\oplus)\ \id_\oplus\ l)\ xs\\
    &\qquad = ys\\
    &\qquad\quad \text{where}\\
    &\qquad \reduce\ (\oplus)\ \id_\oplus\ ys = \reduce\ (\oplus)\ \id_\oplus\ xs
  \end{align*}
  It remains to be shown, that\\ $ \reduce\ (\oplus)\ \id_\oplus\ xs = \reduce\ (\oplus)\ \id_\oplus\ [x_{\sigma(1)}, \ldots, x_{\sigma(n)}]$:
  \begin{align*}
      & \comment{definition of \reduce}\\
    (\reduce\ (\oplus)\ \id_\oplus\ xs) &= [x_1 \oplus \dots \oplus x_n]\\
      & \comment{accociativity of $\oplus$}\\
      & = [x_{\sigma(1)} \oplus \dots \oplus x_{\sigma(n)}]\\
      & \comment{definition of \reduce}\\
      & = (\reduce\ (\oplus)\ \id_\oplus\ [x_{\sigma(1)} \oplus \dots \oplus x_{\sigma(n)}])
  \end{align*}
\end{proof}


\paragraph{Simplification Rules}
\autoref{eq:algo:simpl} shows our simplification rules.
They express the fact that consecutive \splitN-\join pairs and \asVector-\asScalar pairs can be eliminated.
%
\todo{sg: wo ist $\epsilon$ definiert?}
\begin{rerule}{lcl}
  \join \circ \splitN\ n        & \rightarrow & \id\\
  %\splitN\ n \circ \join \circ\ f\ \circ \splitN\ n        & \rightarrow & \textit{id}\\
  \asScalar \circ \asVector\ n & \rightarrow & \id
  \label{eq:algo:simpl}
\end{rerule}
\todo{redefined join and asScalar with $n$ to support: split n o join and asVector n o asScalar as well?!}

\paragraph{Fusion Rules}
Finally, our fusion rules are shown in \autoref{eq:algo:fusion}.
%
\begin{rerule}{lcl}
  \map\ f \circ \map\ g
    & \rightarrow & \map\ (f \circ g)\\
  \reduceSeq\ f\ z \circ \mapSeq\ g
    & \rightarrow & \\
  {\hspace{3em}}
  \reduceSeq\
    \big(\ \lambda\ (acc,x)\ .
      &\hspace{-.75em} f\ acc\ (g\ x)&\hspace{-.75em}\big)\ z
      % only reduce sequential is valid because non-associativity!!!
  \label{eq:algo:fusion}
\end{rerule}
%
The first rule fuses the functions applied by two consecutive maps.
The second rule fuses the map-reduce pattern by creating a lambda function that is the result of merging functions $f$ and $g$ from the original reduction and map,, respectively.
This rule only applies to the sequential \reduce pattern since this is the only implementation not requiring the associativity property required by the more generic \reduce pattern.
% When generating code, these rules in effect allow us to fuse the implementation of the different functions and avoid having to store temporary results.
The functional programming community has studied more generic rules for fusion~\cite{CouttsLeSt2007,JonesToHo2001}.
However, as we currently focus on a restricted set of patterns, our simpler fusion rules have, so far, proven to be sufficient.

\paragraph{Summary}
\autoref{fig:algoRules} gives an overview of all algorithmic rules defined in this subsection.
The rules allow us to formalize different algorithmic implementation strategies:
the rewrite rules regarding the \reduce pattern (\autoref{fig:algo:red}), for example, specify that an iterative implementation of the reduction as well as a divide-and-conquer style implementation are possible.

The split-join rule (\autoref{fig:algo:splitjoin}) allows a divide-and-conquer style implementation of the \map pattern.
This eventually enables different parallel implementations which we can express with \OpenCL, as we will see in the next subsection.

The rules presented here are by no means complete and can easily be extended to express more possible implementations.
When adding new patterns to the system, this set of rules have to be extended as well.

In the next subsection we will discuss \OpenCL specific rewrite rules which allow us to map patter implementations to the low-level \OpenCL concepts.

\newlength{\ruleSpace}
\setlength{\ruleSpace}{1em}
\begin{figure}[p]
\centering
\begin{subfigure}[b]{1\linewidth}
  \begin{mdframed}
    \vspace{-\bigskipamount}
    \begin{rerule*}{lclcl}
          f & \rightarrow & f \circ \map\ \textit{id} & | & \map\ \textit{id} \circ f
    \end{rerule*}
  \end{mdframed}
  \vspace{-1em}
  \caption{Identity}
  \label{fig:algo:identity}
\end{subfigure}

\vspace{\ruleSpace}
\begin{subfigure}[b]{1\linewidth}
  \begin{mdframed}
    \vspace{-\bigskipamount}
    \begin{rerule*}{lcl}
      \iterateN\ (m+n)\ f & \rightarrow & \iterateN\ m\ f \circ \iterateN\ n\ f\\
      \iterateN\ 1\ f & \rightarrow & f
    \end{rerule*}
  \end{mdframed}
  \vspace{-1em}
  \caption{Iterate decomposition}
  \label{fig:algo:iterate}
\end{subfigure}

\vspace{\ruleSpace}
\begin{subfigure}[b]{1\linewidth}
  \begin{mdframed}
    \vspace{-\bigskipamount}
    \begin{rerule*}{lcl}
      \map\ f \circ \reorder
        & \rightarrow & \reorder \circ \map\ f\\
      \reorder \circ \map\ f
        & \rightarrow & \map\ f \circ \reorder\\
    \end{rerule*}
  \end{mdframed}
  \vspace{-1em}
  \caption{Reorder commutativity}
  \label{fig:algo:reorder}
\end{subfigure}

\vspace{\ruleSpace}
\begin{subfigure}[b]{1\linewidth}
  \begin{mdframed}
    \vspace{-\bigskipamount}
    \begin{rerule*}{lcl}
      \map\ f
        & \rightarrow &
          \join \circ \map\ (\map\ f) \circ \splitN\ n
    \end{rerule*}
  \end{mdframed}
  \vspace{-1em}
  \caption{Split-join}
  \label{fig:algo:splitjoin}
\end{subfigure}

\vspace{\ruleSpace}
\begin{subfigure}[b]{1\linewidth}
  \begin{mdframed}
    \vspace{-\bigskipamount}
    \begin{rerule*}{lcl}
      \reduce\ f\ z
        & \rightarrow &
          \reduce\ f\ z \circ \partRed\ f\ z\\
      \partRed\ f\ z
        & \rightarrow &
          \reduce\ f\ z\\
        & | &
          \partRed\ f\ z \circ \reorder\\
        & | &
          \join
            \circ \map\ (\partRed\ f\ z)
            \circ \splitN\ n\\
        & | &
          \iterateN\ n\ (\partRed\ f\ z)\\
    \end{rerule*}
  \end{mdframed}
  \vspace{-1em}
  \caption{Reduction}
  \label{fig:algo:red}
\end{subfigure}


\vspace{\ruleSpace}
\begin{subfigure}[b]{1\linewidth}
  \begin{mdframed}
    \vspace{-\bigskipamount}
    \begin{rerule*}{lcl}
      \join \circ \splitN\ n
            & \rightarrow & \textit{id}\\
      \asScalar \circ \asVector\ n
            & \rightarrow & \textit{id}\\
    \end{rerule*}
  \end{mdframed}
  \vspace{-1em}
  \caption{Simplification rules}
  \label{fig:algo:simpl}
\end{subfigure}

\vspace{\ruleSpace}
\begin{subfigure}[b]{1\linewidth}
  \begin{mdframed}
    \vspace{-\bigskipamount}
    \begin{rerule*}{lcl}
      \map\ f \circ \map\ g
        & \rightarrow & \map\ (f \circ g)\\
      \reduceSeq\ f\ z \circ \mapSeq\ g
        & \rightarrow & \\
      {\hspace{3em}}
      \reduceSeq\
        \big(\ \lambda\ acc,x\ .
          &\hspace{-.75em} f\ acc\ (g\ x)&\hspace{-.75em}\big)\ z\\
          % only reduce sequential is valid because non-associativity!!!
    \end{rerule*}
  \end{mdframed}
  \vspace{-1em}
  \caption{Fusion rules}
  \label{fig:algo:fusion}
\end{subfigure}

\caption{Overview of our algorithmic rewrite rules.}
\label{fig:algoRules}
\end{figure}





\subsection{OpenCL-Specific Rules}
\label{section:rules:opencl}

In this section, we discuss our \OpenCL-specific rules that are used to apply \OpenCL optimizations and to lower high-level algorithmic concepts down to \OpenCL-specific ones.
The code generation process is described separately in the next section.

\paragraph{Maps}
The rule in \autoref{eq:low:map} is used to produce the \OpenCL-specific map patterns that match the thread hierarchy of \OpenCL.
\todo{sg: bitte in der ganzen Diss genau/vorsichtig schauen was/wo ``implementation'' bedeutet, so dass sie nicht in 5 verschiedenen Bedeutungen benutzt wird!}
%
\begin{rerule}{lclcl}
  \map\ f
    & \rightarrow & \mapWorkgroup\ f\\
    & | & \mapLocal\ f\\
    & | & \mapGlobal\ f\\
    & | & \mapWarp\ f\\
    & | & \mapLane\ f\\
    & | & \mapSeq\ f
  \label{eq:low:map}
\end{rerule}
%
Our \todoU{implementation}{sg: von was?} \todoU{maintains}{sg: wo?} \todoU{context information}{sg: was ist das?} to ensure that the \OpenCL thread hierarchy is respected.
\todo{sg: man sieht (??)}
For instance, it is only legal to nest a \mapLocal inside a \mapWorkgroup and it is not valid to nest a \mapGlobal or another \mapWorkgroup inside a \mapWorkgroup.

\paragraph{Reduction}
There is only one rule for lowering to \OpenCL (\autoref{eq:low:red}), which expresses the fact that the only implementation known to the code generator is a sequential reduction.
Parallel implementations are defined at a higher level by composition of other algorithmic patterns.
Most existing high-performance compilers treat the reduction directly as an \todoU{irreducible}{sg: was bedeutet das genau?} primitive operation.
With our approach it is possible to explore different implementations for the reduction by simply applying different rules.
%
\begin{rerule}{lcl}
  \reduce\ f\ z & \rightarrow & \reduceSeq\ f\ z
  \label{eq:low:red}
\end{rerule}


\paragraph{Reorder}
\autoref{eq:low:stride} presents the rule that reorders elements of an array.
In our current \todoU{implementation}{sg: von was?}, we support two types of reordering:
no reordering, represented by the \textit{id} function, and reordering with a certain stride $s$: $\reorderStride\ s$
As described earlier, the major use case for the stride reorder is to enable coalesced memory accesses.
%
\begin{rerule}{lcl}
  \reorder & \rightarrow & \reorderStride\ s\\
                & | & \textit{id}
  \label{eq:low:stride}
\end{rerule}
%
\todoU{Other types}{sg: unklar} of reordering functions could be implemented easily within this \todoU{framework}{sg: wo ist Framework, woraus besteht es?} such \todoU{as user-defined}{sg: unklar} reorder function.

\paragraph{Local and Global Memory}
\autoref{eq:low:mem} contains two rules that enable \GPU local memory usage.
%
\begin{rerule}{lcl}
  \mapLocal\ f & \rightarrow & \toGlobal\ (\mapLocal\ f)\\
  \mapLocal\ f & \rightarrow & \toLocal\ (\mapLocal\ f)
  \label{eq:low:mem}
\end{rerule}
%
They express the fact that the result of a \mapLocal can always be stored in local memory or back in global memory.
This holds since \todoU{a \mapLocal always exists within a \mapWorkgroup}{sg: wie wird das garantiert?} for which the local memory is defined in \OpenCL.
These rules allow us to describe how the data is mapped to the \GPU memory hierarchy.



\paragraph{Vectorization}
\autoref{eq:algo:vect} shows the vectorization rule.
%
\begin{rerule}{lcl}
  \map\ f
    & \rightarrow &
      \asScalar
        \circ \map\ (\vect\ n\ f)
        \circ \asVector\ n
  \label{eq:algo:vect}
\end{rerule}
%
Vectorization is achieved by using the \asVector and corresponding \asScalar patterns which change the element type of an array and adjust the length accordingly.
This rule is only allowed to be applied once to a given $\map\ f$ pattern.
This constrain can easily be checked by looking at the function's $f$ type, \ie, if it is a vector type, the rule cannot be applied.
% Another set of rules, not shown here for space reason, is used to propagate the \vect function recursively within $f$.
The \vect pattern is used to produce a vectorized version of the customizing function $f$.
Note that the vector width $n$ has to match for the vectorization of the function and the modification of the array.

\paragraph{Summary}
\autoref{fig:lowRules} shows an overview of the \OpenCL-specific rewrite rules.
Each rule formalizes a different implementation or optimization strategy in \OpenCL.

\begin{itemize}
  \item The map rules (\autoref{fig:low:map}) describe the usage of the \OpenCL thread hierarchy with work-items and work-groups.

  \item The reduce rule (\autoref{fig:low:red}) specifies the simple sequential implementation of reduction in \OpenCL, the parallel reduction is implemented in terms of other patterns as we saw in \autoref{section:rules:algo}.

  \item The stride access rule (\autoref{fig:low:stride}) enables coalesced memory access, which is crucial for performance as we saw in \autoref{section:reduce:case-study}.

  \item The local memory rule (\autoref{fig:low:mem}) allows the usage of the fast local memory.
  We saw the benefits of using the local memory when evaluating the matrix multiplication expressed using the \allpairs pattern in \autoref{chapter:skelcl-evaluation}.

  \item Finally, the vectorization rule (\autoref{fig:low:vect}) enables vectorization, which is a key optimization for the Intel \CPU architectures as we will see in \autoref{chapter:codeGeneration-evaluation}.
\end{itemize}

As for the algorithmic rules, the \OpenCL-specific rules presented here are not complete and do not cover all possible optimizations in \OpenCL.
Nevertheless, we will see in \autoref{chapter:codeGeneration-evaluation}, that these rules are a good starting set for generating efficient \OpenCL code.

\begin{figure}[t]
\centering
\begin{subfigure}[b]{1\linewidth}
  \begin{mdframed}
    \vspace{-\bigskipamount}
    \begin{rerule*}{lclcl}
      \map\ f
        & \rightarrow &
          \mapWorkgroup\ f & | & \mapLocal\ f\\
        & | &
          \mapWarp\ f      & | & \mapLane\ f\\
        & | &
          \mapGlobal\ f    & | & \mapSeq\ f\\
    \end{rerule*}
  \end{mdframed}
  \vspace{-1em}
  \caption{Map}
  \label{fig:low:map}
\end{subfigure}

\vspace{\ruleSpace}
\begin{subfigure}[b]{1\linewidth}
  \begin{mdframed}
    \vspace{-\bigskipamount}
    \begin{rerule*}{lcl}
      \reduce\ f\ z
        & \rightarrow &
          \reduceSeq\ f\ z
    \end{rerule*}
  \end{mdframed}
  \vspace{-1em}
  \caption{Reduction}
  \label{fig:low:red}
\end{subfigure}

\vspace{\ruleSpace}
\begin{subfigure}[b]{1\linewidth}
  \begin{mdframed}
    \vspace{-\bigskipamount}
    \begin{rerule*}{lclcl}
      \reorder
        & \rightarrow &
          \reorderStride\ s & | & \textit{id}
    \end{rerule*}
  \end{mdframed}
  \vspace{-1em}
  \caption{Stride accesses or normal accesses}
  \label{fig:low:stride}
\end{subfigure}

\vspace{\ruleSpace}
\begin{subfigure}[b]{1\linewidth}
  \begin{mdframed}
    \vspace{-\bigskipamount}
    \begin{rerule*}{lcl}
      \mapLocal\ f
        & \rightarrow &
          \toGlobal\ (\mapLocal\ f)\\
      \mapLocal\ f
        & \rightarrow & \toLocal\ (\mapLocal\ f)\\
    \end{rerule*}
  \end{mdframed}
  \vspace{-1em}
  \caption{Local/Global memory}
  \label{fig:low:mem}
\end{subfigure}

\vspace{\ruleSpace}
\begin{subfigure}[b]{1\linewidth}
  \begin{mdframed}
    \vspace{-\bigskipamount}
    \begin{rerule*}{lcl}
      \map\ f
        & \rightarrow &
          \asScalar
            \circ \map\ (\vect\ n\ f)
            \circ \asVector\ n
    \end{rerule*}
  \end{mdframed}
  \vspace{-1em}
  \caption{Vectorization}
  \label{fig:low:vect}
\end{subfigure}

\caption{Overview of the \OpenCL-specific rewrite rules.}
\label{fig:lowRules}
\end{figure}

\FloatBarrier



\subsection{Proving the Correctness of the Rewrite Rules}
\label{section:rules:proofs}



\begin{proof}[Proof of Rule \ref{fig:algo:identity}]
  We show that $f \circ \map$
\end{proof}

%\subsection{Automatic Rewriting Strategy}
%\label{sec:search}
%
%The rules presented in this section define a search space of possible implementations.
%In order to find the best possible low-level expression for a given target device, we have developed a simple automatic search strategy based loosely on Bandit-based optimization~\cite{demesmay09bandit}.
%Note that our current search strategy is just designed to prove that it is possible to find good implementations.
%We envision replacing this exploration strategy in the future by using machine-learning techniques to avoid having to search the space.
%However, this is orthogonal to the work presented in this paper.
%
%Our search strategy starts with the high-level expression and determines all the valid rules that can be applied at this stage.
%We use a Monte-Carlo method for evaluating the potential impact of each rule by randomly walking down the search tree.
%The rule that will lead to the best performance following the Monte-Carlo descent is chosen and applied to the expression.
%This process is repeated until we reach a terminal expression.
%Note that in addition to selecting the rules, we also search at the same time for the parameters controlling our patterns such as the vector size for the $\vect\ n$ pattern.
%Using this simple strategy, we found that less than a thousand expressions were evaluated to reach a solution in most cases.

\subsection{Applying the Rewrite Rules}
\label{sec:example}
In this section, we will discuss some examples to show how the rewrite rules can be used to systematically rewrite applications expressed with the patterns introduced in \autoref{section:patterns}.
We will start by looking back at the introductory example from \autoref{section:code-generation:overview}.
Then we will look at the parallel reduction example and show that we can systematically derive optimized implementations equivalent to the implementations discussed in \autoref{sec:reduce:case-study}.

\subsubsection{A First Example: Scaling a Vector}
\autoref{eq:vectorScal:impl} shows the implementation of the vector scaling example we used in \autoref{section:code-generation:overview}.
This directly corresponds to \autoref{fig:codeex:map}.
\begin{align}
  mul3\ x &= x \times 3\nonumber\\
  vectorScal &= \map\ mul3
  \label{eq:vectorScal:impl}
\end{align}
The application developer uses the algorithmic pattern \map together with the customizing function $mul3$ which multiplies every element with the number $3$.
\autoref{eq:vectorScal:rules} shows how this implementation can be systematically rewritten using the rewrite rules introduced in this section.
The numbers above the equal signs refer to \autoref{fig:algoRules} and \autoref{fig:lowRules} indicating which rule was used in the step.
\begin{align}
  &vectorScal = \map\ mul3\nonumber\\
  &\qquad\begin{aligned}
    &\overset{\ref{fig:algo:splitjoin}}{=\hspace{.2em}}
      \join \circ \map\ (\map\ mul3) \circ \splitN\ n_1\\
    &\overset{\ref{fig:low:vect}}{=\hspace{.2em}}
      \join \circ \map\ \big(\\
      &\qquad\quad \asScalar \circ \map\ (\vect\ n_2\ mul3) \circ \asVector\ n_2\\
      &\qquad\big) \circ \splitN\ n_1\\
    &\overset{\ref{fig:low:map}}{=\hspace{.2em}}
      \join \circ \mapWorkgroup\ \big(\\
      &\qquad\quad \asScalar \circ \mapLocal\ (\\
      &\qquad\qquad\vect\ n_2\ mul3\\
      &\qquad\quad) \circ \asVector\ n_2\\
      &\qquad\big) \circ \splitN\ n_1
  \end{aligned}
  \label{eq:vectorScal:rules}
\end{align}
\todoU{When selecting $n_1=1024$ and $n_2=4$}{sg: wie waehlen wir diese Zahlen?} we obtain the expression shown in \autoref{fig:codeex:impl} from which \OpenCL code can be generated.
We will discuss the process of \OpenCL code generation in the next section.
But first we will discuss possible derivations for the parallel reduction.












\subsubsection{Systematic Deriving Implementations of Parallel Reduction}

In \autoref{section:reduce:case-study} we looked at implementations of the parallel reduction manually optimized for an Nvidia \GPU.
In this subsection we want to resemble these implementations with corresponding expressions comprised of the patterns presented in \autoref{section:patterns}.
The implementations presented in \autoref{lst:reduce0}--\autoref{lst:reduce6} are manual implementations where optimizations have been applied ad-hoc.
The key difference to the implementations presented in this section is, that these are systematically derived from a single high-level expression using the rewrite rules introduced in this section.
Therefore, these implementations can be generated systematically by an optimizing compiler.
The rules guarantee that all derived expressions are semantically equivalent.

Each \OpenCL low-level expression presented in this subsection is derived from the high-level expression \autoref{eq:reduceSum} expressing parallel summation:
\begin{align}
  vecSum = \reduce\ (+)\ 0
  \label{eq:reduceSum}
\end{align}
%
The formal derivations for all expressions are shown in \autoref{chapter:AppendixA} in \autoref{section:derivations}.

\paragraph{First Pattern-Based Expression}
\autoref{eq:reduce11} shows our first expression implementing parallel reduction.
\todo{erst Bild, wo die Formal mit einer OpenCL Platform zusammen gezeigt wird? (?)}
\begin{figure}
  \begin{align*}
    \lineNum{1}&
      \quad vecSum = \reduce \circ \join \circ \mapWorkgroup\ \big(\\
    \lineNum{2}&
      \qquad\quad \join \circ \toGlobal\ (\mapLocal\ (\mapSeq\ \id)) \circ \splitN\ 1\ \circ\\
    \lineNum{3}&
      \qquad\quad\iterateN\ 7\ (\join \circ \mapLocal\ (\reduceSeq\ (+)\ 0) \circ \splitN\ 2)\ \circ\\
    \lineNum{4}&
      \qquad\quad \join \circ \toLocal\ (\mapLocal\ (\mapSeq\ \id)) \circ \splitN\ 1\\
    \lineNum{5}&
      \qquad\big) \circ \splitN\ 128
  \end{align*}
  \caption{Expression resembling the implementation of parallel reduction presented in \autoref{lst:reduce0} and \autoref{lst:reduce1}.}
  \label{eq:reduce11}
\end{figure}
%
This expression closely resembles the structure of the first two implementations presented in \autoref{lst:reduce0} and \autoref{lst:reduce1}.
First the input array is split into chunks of size 128 (line~\lineNum{5}) and each work-group processes such a chunk of data.
128 corresponds to the work-group size we assumed for our implementations in \autoref{section:reduce:case-study}.
Inside of a work-group in line~\lineNum{4} each work-item first copies a single data item (indicated by $\splitN\ 1$) into the local memory using the \textit{id} function to perform a copy nested inside the $\toLocal$ pattern.
Afterwards, in line~\lineNum{3} the entire work-group performs an iterative reduction where in \todoU{7 steps (this equals $log_2(128)$)}{sg: welche Regel berechnet das?} the data is further divided into \todoU{pairs of two}{sg: warum genau so?} elements (using $\splitN\ 2$) which are reduced sequentially by the work-items.
Finally, in line~\lineNum{2} the computed result is copied back to the global memory.

The first two implementations discussed in \autoref{section:reduce:case-study} are very similar and the only difference is which work-item remains active in the parallel reduction tree.
Currently, we do not model this subtle difference in our patterns, therefore, we cannot create an expression which distinguishes between these two implementations.
This is not a major drawback, because none of the three architectures favoured the first over the second implementation, as we saw in \autoref{section:reduce:case-study}.
Therefore, on these three architectures it is always beneficial to choose the second over the first implementation, which is exactly what our \todoU{\OpenCL code generator does as we will see}{sg: aus welchem Grund macht er so? Was macht er fuer andere Beispiele?} in the next section.


\paragraph{Avoiding Interleaved Addressing}
\begin{figure}
  \begin{align*}
    &\hspace{-1.5em}vecSum\\
    &\hspace{-2em}\quad\begin{aligned}
      &=\hspace{.2em}
        \reduce \circ \join \circ \mapWorkgroup\ \big(\\
        &\qquad\quad \join \circ \toGlobal\ (\mapLocal\ (\mapSeq\ \id)) \circ \splitN\ 1\ \circ\\
        &\qquad\quad \iterateN\ 7\ \big(\ \lambda\ xs\ .\\
        &\qquad\qquad \join \circ \mapLocal\ (\reduceSeq\ (+)\ 0) \circ \splitN\ 2\ \circ\\
        &\qquad\qquad\quad \reorderStride\ ((size\ xs)/2)\ \$\ xs\ \big)\ \circ\\
        &\qquad\quad \join \circ \toLocal\ (\mapLocal\ (\mapSeq\ \id)) \circ \splitN\ 1\\
        &\qquad\big) \circ \splitN\ 128
    \end{aligned}
  \end{align*}
  \caption{Expression resembling the implementation of parallel reduction presented in \autoref{lst:reduce2}.}
  \label{eq:reduce12}
\end{figure}
%
\todo{also line number?}
\autoref{eq:reduce12} shows our second expression implementing parallel reduction, which closely \todoU{resembles}{sg: wirklich 1:1 oder doch etwas anders?} the third implementation of parallel reduction shown in \autoref{lst:reduce2}.
The $\reorderStride$ pattern is used which makes local memory bank conflicts highly unlikely.
Please note that the pattern is used inside the $\iterateN$ pattern.
Therefore, the stride changes in every iteration, which is expressed by referring to the size of the array in the current iteration.
We use a lambda expression to name the input array ($xs$), use a $size$ function to access its size, and use the $\$$ operator, known from Haskell, to denote function application, \ie, $f\ \$\ x = (f\ x)$.


\paragraph{Increase Computational Intensity per Work-item\hspace{3em}\strut}
\autoref{eq:reduce13} shows our third expression implementing parallel reduction.
\begin{figure}
  \begin{align*}
    &\hspace{-1.5em}vecSum\\
    &\hspace{-2em}\quad\begin{aligned}
      &=\hspace{.2em}
        \reduce \circ \join \circ \mapWorkgroup\ \big(\\
      &\qquad\quad\join \circ \toGlobal\ (\mapLocal\ (\mapSeq\ \id)) \circ \splitN\ 1\ \circ\\
      &\qquad\quad\iterateN\ 7\ \big(\ \lambda\ xs\ .\\
      &\qquad\qquad \join \circ \mapLocal\ (\reduceSeq\ (+)\ 0) \circ \splitN\ 2\ \circ\\
      &\qquad\qquad\quad \reorderStride\ ((size\ xs)/2)\ \$\ xs\ \big)\ \circ\\
      &\qquad\quad\join \circ \toLocal\ (\mapLocal\ (\reduceSeq\ (+)\ 0)) \circ \splitN\ 2\ \circ\\
      &\qquad\qquad\reorderStride\ 128\\
      &\qquad\big) \circ \splitN\ (2\times 128)
    \end{aligned}
  \end{align*}
  \caption{Expression resembling the implementation of parallel reduction presented in \autoref{lst:reduce3}.}
  \label{eq:reduce13}
\end{figure}
%
\todo{also line number?}
This expression closely resembles the fourth implementation shown in \autoref{lst:reduce3}.
By replacing the copy operation into the local memory with a reduction of two elements we increase the computational intensity per work-item.
The first $\reorderStride$ pattern is used to ensure the coalesced memory access when accessing the global memory.
As now each work-group processes twice the amount of elements, we split the input data in twice the size of a work-group: $2\times 128$.
\todo{sg: ist diese Idee in einer der Rules ausgedrueckt? in welcher?}


\paragraph{Avoid Synchronization Inside a Warp}
\autoref{eq:reduce14} shows our fourth expression implementing parallel reduction.
\begin{figure}
  \begin{align*}
    &\hspace{-1.5em}vecSum\\
    &\hspace{-2em}\quad\begin{aligned}
      &=\hspace{.2em}
        \reduce \circ \join \circ \mapWorkgroup\ \big(\\
      &\qquad\quad \join \circ \toGlobal\ (\mapLocal\ (\mapSeq\ \id)) \circ \splitN\ 1\ \circ\\
      &\qquad\quad \join \circ \mapWarp\ \big(\\
      &\qquad\qquad \join \circ \mapLane\ (\reduceSeq\ (+)\ 0) \circ \splitN\ 2\ \circ\\
      &\qquad\qquad\quad \reorderStride\ 1\ \circ\\
      &\qquad\qquad \join \circ \mapLane\ (\reduceSeq\ (+)\ 0) \circ \splitN\ 2\ \circ\\
      &\qquad\qquad\quad \reorderStride\ 2\ \circ\\
      &\qquad\qquad \join \circ \mapLane\ (\reduceSeq\ (+)\ 0) \circ \splitN\ 2\ \circ\\
      &\qquad\qquad\quad \reorderStride\ 4\ \circ\\
      &\qquad\qquad \join \circ \mapLane\ (\reduceSeq\ (+)\ 0) \circ \splitN\ 2\ \circ\\
      &\qquad\qquad\quad \reorderStride\ 8\ \circ\\
      &\qquad\qquad \join \circ \mapLane\ (\reduceSeq\ (+)\ 0) \circ \splitN\ 2\ \circ\\
      &\qquad\qquad\quad \reorderStride\ 16\ \circ\\
      &\qquad\qquad \join \circ \mapLane\ (\reduceSeq\ (+)\ 0) \circ \splitN\ 2\ \circ\\
      &\qquad\qquad\quad \reorderStride\ 32\\
      &\qquad\quad\big) \circ \splitN\ 64\ \circ\\
      &\qquad\quad \iterateN\ 1\ \big(\ \lambda\ xs\ .\\
      &\qquad\qquad \join \circ \mapLocal\ (\reduceSeq\ (+)\ 0) \circ \splitN\ 2\ \circ\\
      &\qquad\qquad\quad \reorderStride\ ((size\ xs)/2)\ \$\ xs\ \big)\ \circ\\
      &\qquad\quad \join \circ \toLocal\ (\mapLocal\ (\reduceSeq\ (+)\ 0)) \circ \splitN\ 2\ \circ\\
      &\qquad\qquad \reorderStride\ 128\\
      &\qquad\big) \circ \splitN\ 256
    \end{aligned}
  \end{align*}
  \caption{Expression resembling the implementation of parallel reduction presented in \autoref{lst:reduce4}.}
  \label{eq:reduce14}
\end{figure}
%
This expression closely resembles the fifth implementation of the parallel reduction shown in \autoref{lst:reduce4}.
The \iterateN pattern has been changed from performing seven iterations down to a single one.
This reflects the \OpenCL implementation, where the processing of the last 64 elements is performed by a single warp.
We express this using the \mapWarp pattern, where inside the \mapLane pattern is used together with the \splitN and \join patterns to express that each work-item inside the warp performs a reduction of two elements at a time.
Instead of using the \iterateN pattern, the single iteration steps has been unrolled, as it was the case in \autoref{lst:reduce4}.
The strides of the \reorderStride pattern are computed based on the size of the array in each iteration step.











\paragraph{Complete Loop Unrolling}
\autoref{eq:reduce15} shows our fifth expression implementing parallel reduction.
\begin{figure}
  \begin{align*}
    &\hspace{-1.5em}vecSum\\
    &\hspace{-2em}\quad\begin{aligned}
      &=\hspace{.2em}
        \reduce \circ \join \circ \mapWorkgroup\ \big(\\
      &\qquad\quad \join \circ \toGlobal\ (\mapLocal\ (\mapSeq\ \id)) \circ \splitN\ 1\ \circ\\
      &\qquad\quad \join \circ \mapWarp\ \big(\\
      &\qquad\qquad \join \circ \mapLane\ (\reduceSeq\ (+)\ 0) \circ \splitN\ 2\ \circ\\
      &\qquad\qquad\quad \reorderStride\ 1\ \circ\\
      &\qquad\qquad \join \circ \mapLane\ (\reduceSeq\ (+)\ 0) \circ \splitN\ 2\ \circ\\
      &\qquad\qquad\quad \reorderStride\ 2\ \circ\\
      &\qquad\qquad \join \circ \mapLane\ (\reduceSeq\ (+)\ 0) \circ \splitN\ 2\ \circ\\
      &\qquad\qquad\quad \reorderStride\ 4\ \circ\\
      &\qquad\qquad \join \circ \mapLane\ (\reduceSeq\ (+)\ 0) \circ \splitN\ 2\ \circ\\
      &\qquad\qquad\quad \reorderStride\ 8\ \circ\\
      &\qquad\qquad \join \circ \mapLane\ (\reduceSeq\ (+)\ 0) \circ \splitN\ 2\ \circ\\
      &\qquad\qquad\quad \reorderStride\ 16\ \circ\\
      &\qquad\qquad \join \circ \mapLane\ (\reduceSeq\ (+)\ 0) \circ \splitN\ 2\ \circ\\
      &\qquad\qquad\quad \reorderStride\ 32\\
      &\qquad\quad\big) \circ \splitN\ 64\ \circ\\
      &\qquad\quad \join \circ \mapLocal\ (\reduceSeq\ (+)\ 0) \circ \splitN\ 2\ \circ\\
      &\qquad\qquad \reorderStride\ 64\ \circ\\
      &\qquad\quad \join \circ \toLocal\ (\mapLocal\ (\reduceSeq\ (+)\ 0)) \circ \splitN\ 2\ \circ\\
      &\qquad\qquad \reorderStride\ 128\\
      &\qquad\big) \circ \splitN\ 256
    \end{aligned}
  \end{align*}
  \caption{Expression resembling the implementation of parallel reduction presented in \autoref{lst:reduce5}.}
  \label{eq:reduce15}
\end{figure}
%
This expression closely resembles the sixth implementation of the parallel reduction shown in \autoref{lst:reduce5}.
The change to the previous expression in \autoref{eq:reduce14} is small:
we replace the \iterateN pattern with the individual iteration steps.
As we assume a fixed work-group size of 128 work-items, \todoU{we statically know for}{sg: komisch ausgedrueckt} \autoref{eq:reduce14} that only a single iteration step is required.
If a larger work-group size would be chosen, then the expression shown in \autoref{eq:reduce15} would reflect this by including additional iteration steps.




\paragraph{Fully Optimized Implementation}
\autoref{eq:reduce15} shows our sixth and last expression implementing parallel reduction.
\begin{figure}
  \begin{align*}
    &\hspace{-1.5em}vecSum\\
    &\hspace{-2em}\quad\begin{aligned}
      &=\hspace{.2em}
        \reduce \circ \join \circ \mapWorkgroup\ \big(\\
      &\qquad\quad \join \circ \toGlobal\ (\mapLocal\ (\mapSeq\ \id)) \circ \splitN\ 1\ \circ\\
      &\qquad\quad \join \circ \mapWarp\ \big(\\
      &\qquad\qquad \join \circ \mapLane\ (\reduceSeq\ (+)\ 0) \circ \splitN\ 2\ \circ\\
      &\qquad\qquad\quad \reorderStride\ 1\ \circ\\
      &\qquad\qquad \join \circ \mapLane\ (\reduceSeq\ (+)\ 0) \circ \splitN\ 2\ \circ\\
      &\qquad\qquad\quad \reorderStride\ 2\ \circ\\
      &\qquad\qquad \join \circ \mapLane\ (\reduceSeq\ (+)\ 0) \circ \splitN\ 2\ \circ\\
      &\qquad\qquad\quad \reorderStride\ 4\ \circ\\
      &\qquad\qquad \join \circ \mapLane\ (\reduceSeq\ (+)\ 0) \circ \splitN\ 2\ \circ\\
      &\qquad\qquad\quad \reorderStride\ 8\ \circ\\
      &\qquad\qquad \join \circ \mapLane\ (\reduceSeq\ (+)\ 0) \circ \splitN\ 2\ \circ\\
      &\qquad\qquad\quad \reorderStride\ 16\ \circ\\
      &\qquad\qquad \join \circ \mapLane\ (\reduceSeq\ (+)\ 0) \circ \splitN\ 2\ \circ\\
      &\qquad\qquad\quad \reorderStride\ 32\\
      &\qquad\quad\big) \circ \splitN\ 64\ \circ\\
      &\qquad\quad \join \circ \mapLocal\ (\reduceSeq\ (+)\ 0) \circ \splitN\ 2\ \circ\\
      &\qquad\qquad \reorderStride\ 64\ \circ\\
      &\qquad\quad \join \circ \toLocal\ (\mapLocal\ (\reduceSeq\ (+)\ 0))\ \circ\\
      &\qquad\qquad\splitN\ (blockSize/128)\ \circ \reorderStride\ 128\\
      &\qquad\big) \circ \splitN\ blockSize
    \end{aligned}
  \end{align*}
  \caption{Expression resembling the implementation of parallel reduction presented in \autoref{lst:reduce6}.}
  \label{eq:reduce16}
\end{figure}
%
This expression closely resembles the seventh and finally optimized implementation of the parallel reduction shown in \autoref{lst:reduce6}.
As in the original \OpenCL implementation, we increase the computational intensity by increasing the number of elements processed by a single work-group.
We express this by choosing a larger $blockSize$ when splitting the input array the first time.
The first \reorderStride expression ensures that memory accesses to the global memory are coalesced.


\paragraph{Conclusions}
In this subsection, \todoU{we resembled the \OpenCL implementations}{sg: gutes Englisch?} presented in \autoref{section:reduce:case-study} by expressing them as compositions of our patterns.
Furthermore, the presented expressions are all derivable from a simple high-level expression describing the parallel summation.
The derivations are a bit lengthy and thus not shown here, but instead in \autoref{chapter:AppendixA} in \autoref{section:derivation}.

By expressing highly specialized and optimized implementations we show how flexible and versatile our patterns and rules on them are.
We will see in the next section how these expressions can be turned into \OpenCL code.
In \autoref{chapter:codeGeneration-evaluation}, we will come back to these expressions and evaluate the performance achieved by the \OpenCL code generated from them.



















\subsubsection{Systematic Fusion of Patterns}
Before we look at how \OpenCL code is generated, we discuss one additional optimization: fusion of patterns.
Back in \autoref{chapter:skelcl-evaluation} in \autoref{section:skelcl:evaluation:linearAlgebra} we discussed how the sum of absolute values (\emph{asum}) can be implemented in \SkelCL.
Two algorithmic skeletons, \reduce and \map, where composed to express this application as shown in \autoref{eq:asum:patterns}.
\begin{align}
  asum\ \vec{x} &= \reduce\ (+)\ 0\ \big(\ \map\ (|\, .\, |)\ \vec{x}\ \big)\label{eq:asum:patterns}\\
  \text{where:} \qquad | a | &=
    \left\{
      \begin{array}{r l}
      a & \text{if } a \geq 0\\
      -a & \text{if } a < 0
      \end{array}
    \right.\nonumber
\end{align}
%
When evaluating the performance of the \SkelCL implementation, we identified a problem:
\SkelCL treats each algorithmic skeleton separately, thus, forcing the \map skeleton to write a temporary array result back to global memory and then read it again for the next computation, which greatly limits performance.
The temporary array could be avoided, but in the library approach followed by \SkelCL it is extremely difficult to implement a generic mechanism for fusing algorithmic skeletons.


By using our pattern-based approach presented in this chapter together with the rewrite rules, we are now able to address this issue.
\autoref{fig:algo:fusion} shows our current fusion rule, which allows us to fuse two patterns into one, thus, avoiding intermediate results.
\autoref{fig:derivation} shows how we can derive a fused version for calculating \emph{asum} from the high-level expression written by the programmer.

\begin{figure*}[t]
\begin{align*}
  &\text{\textit{asum}} = \reduce\ (+)\ 0\ \circ\ \map\ (|\, .\, |)\\[.5em]
  %
  &\begin{aligned}
  & \overset{\ref{fig:algo:red}}{\hspace{.2em}=\hspace{.2em}}
      \begin{aligned}
        & \reduce\ (+)\ 0\ \circ\\
        &\quad \join\ \circ\ \map\ (\partRed\ (+)\ 0)\ \circ\ \splitN\ n\ \circ \map\ (|\, .\, |)
      \end{aligned}\\[.5em]
  %
  & \overset{\ref{fig:algo:splitjoin}}{=\hspace{.2em}}
      \begin{aligned}
        & \reduce\ (+)\ 0\ \circ\\
        &\quad \join\ \circ\ \map\ (\partRed\ (+)\ 0)\ \circ\ \splitN\ n\ \circ\\
        &\quad \join\ \circ\ \map\ (\map\ (|\, .\, |))\ \circ\ \splitN\ n
      \end{aligned}\\[.5em]
  %
  & \overset{\ref{fig:algo:simpl}}{=\hspace{.2em}}
      \begin{aligned}
        & \reduce\ (+)\ 0\ \circ\\
        &\quad \join\ \circ\ \map\ (\partRed\ (+)\ 0)\ \circ \map\ (\map\ (|\, .\, |))\ \circ\ \splitN\ n
      \end{aligned}\\[.5em]
  %
  & \overset{\ref{fig:algo:fusion}}{=\hspace{.2em}}
      \begin{aligned}
        & \reduce\ (+)\ 0\ \circ\\
        &\quad \join\ \circ\ \map\ (\partRed\ (+)\ 0\ \circ\ \map\ (|\, .\, |))\ \circ \splitN\ n
      \end{aligned}\\[.5em]
  %
  & \overset{\ref{fig:low:map}}{=\hspace{.2em}}
      \begin{aligned}
        & \reduce\ (+)\ 0\ \circ\\
        &\quad \join\ \circ\ \map\ (\partRed\ (+)\ 0\ \circ\ \mapSeq\ (|\, .\, |))\ \circ \splitN\ n
      \end{aligned}\\[-1em]
  %
  & \overset{\hspace{-1.3em}\ref{fig:algo:red}\& \ref{fig:low:red}\hspace{-1.1em}}{=\hspace{.2em}}
      \begin{aligned}
        &\\[.5em]
        & \reduce\ (+)\ 0\ \circ\\
        &\quad \join\ \circ\ \map\ (\reduceSeq\ (+)\ 0\ \circ\ \mapSeq\ (|\, .\, |))\ \circ \splitN\ n
      \end{aligned}\\[.5em]
  %
  & \overset{\ref{fig:algo:fusion}}{=\hspace{.2em}}
      \begin{aligned}
        & \reduce\ (+)\ 0\ \circ\\
        &\quad  \join\ \circ \map\ \big(\reduceSeq\ (\lambda\ a, b\ .\ a+|\, b\, |)\ 0\big)\ \circ \splitN\ n
      \end{aligned}
  \end{aligned}
\end{align*}
\caption{Derivation for \emph{asum} to a fused parallel version.
  The numbers above the equality sign refer to the rules from \autoref{fig:algoRules}.
}
\label{fig:derivation}
\end{figure*}


\todo{sg: Wird der Compiler das genauso machen koennen, d.h., die richtigen Regeln in der richtigen Reihenfolgen anwenden?}
We start by applying the reduction rule~\ref{fig:algo:red} twice:
first to replace \reduce with $\reduce\ \circ\ \partRed$ and then a second time to expand \partRed.
We expand \map, which can be simplified by removing the two corresponding \join and \splitN patterns.
Then two \map patterns are fused and in the next step the nested \map is lowered into the \mapSeq pattern.
We then first transform \partRed back into \reduce (using rule~\ref{fig:algo:red}) and then apply the \OpenCL rule~\ref{fig:low:red}.
Finally, we apply rule~\ref{fig:algo:fusion} to fuse the \mapSeq and \reduceSeq into a single \reduceSeq.
This sequence of transformations results in an expression which allows for better \OpenCL implementation since no temporary storage is required for the intermediate result.

One interesting observation is that in the final expression the two customizing functions $+$ and $|\, .\,|$ are merged and a lambda expression has been created which uses these two functions: $\lambda\ a, b\ .\ a+|\, b\, |$.
The generated lambda expression is not associative, as $a+|\, b\, | \neq b+|\, a\, |$.
Therefore, this lambda is used in a sequential implementation of reduction and can not be used as the customizing function for the entire parallel reduction.
Nevertheless, the final expression implements a parallel reduction, as the \map pattern is used with the \splitN and \join patterns to split the array in a divide-and-conquer style and the \map pattern is followed by a \reduce pattern customized with addition which can be implemented in parallel, as we know.


\FloatBarrier



\subsection{Summary}
In this section, we have introduced a set of rewrite rules which allow to \todoU{safely}{sg: ??} and systematically rewrite expressions written using the patterns introduced earlier in \autoref{section:patterns}.
The power of our approach lies in the composition of the rules that produce complex low-level expressions from simple high-level expressions.

We have seen how the rules can be used to transform simple expressions written by an application developer into highly specialized and optimized low-level \OpenCL expressions.
These low-level expressions match hardware-specific concepts of \OpenCL, such as mapping computation and data to the thread and memory hierarchy, exploiting memory coalescing, and vectorization.
Each single rule encodes a simple, easy to understand, provable fact.
By composition of the rules we systematically derive low-level expressions which are semantically equivalent to the high-level expressions by construction.
\todoU{This results in a powerful mechanism to safely explore the space of possible implementations.}{sg: ??warum? es wurde doch nix von ``explore'' vorher gesagt?}

In the next section we will investigate how \OpenCL code is generated from the low-level expressions.

