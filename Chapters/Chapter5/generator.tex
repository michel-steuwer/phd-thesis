\section{Code Generator \& Implementation Details}
In this section, we discuss how a low-level expression comprising patterns from \autoref{section:patterns} and possibly derived using the rewrite rules from \autoref{section:rules} is turned into \OpenCL code.
We will see that this process is surprisingly simple and straightforward.
This is due to the fact, that all complex decisions regarding optimizations are made at an earlier stage: when applying the rewrite rules.
This design was chosen deliberately:
it follows the principle of separation of concerns and keeps the implementation of the code generator simple.
The expression used as input for the code generator explicitly specifies every important detail of the \OpenCL implementation to be generated, such that for every expression there is a one-to-one mapping to \OpenCL code.

We will start our discussion by looking at the parallel reduction example and studying how \OpenCL code is generated for the expressions discussed in the previous section.
We will then show how \OpenCL code is generated for each of the patterns defined in \autoref{section:patterns}.
We will see that there are patterns for which it is not possible to generate \OpenCL code.
These expressions do not specify the \OpenCL implementation detailed enough.
We can use the rewrite rules presented in \autoref{section:rules} to transform the expression until, finally, the expression is precise enough for the code generator.

We will then shift our focus to the implementation of the type system and how this helps to implement a static memory allocator.

Finally, we will provide some details about the software infrastructure we used in our implementation.

\subsection{Generating \OpenCL Code for Parallel Reduction}
In the previous section, we discussed how multiple low-level expressions can be derived from the simple high-level pattern $\reduce\ (+)\ 0$.
These derived expressions resembled the \OpenCL implementations we discussed in \autoref{section:reduce:case-study} at the beginning of this chapter.

\autoref{eq:reduce11:again} shows the first expression we derived.
\begin{align}
  \begin{aligned}
    &\join \circ \mapWorkgroup\ \big(\\
    &\quad \join \circ \toGlobal\ (\mapLocal\ (\mapSeq\ \id)) \circ \splitN\ 1\ \circ\\
    &\quad \iterateN\ 7\ (\join \circ \mapLocal\ (\reduceSeq\ (+)\ 0) \circ \splitN\ 2)\ \circ\\
    &\quad \join \circ \toLocal\ (\mapLocal\ (\mapSeq\ \id)) \circ \splitN\ 1\\
    &\big) \circ \splitN\ 128
  \end{aligned}
  \label{eq:reduce11:again}
\end{align}
\autoref{lst:reduce11} shows the \OpenCL code \todoU{generated}{sg: von wem?} for this expression.
%
\begin{lstlisting}[%
  caption={\OpenCL code generated for the expression in \autoref{eq:reduce11:again} implementing parallel reduction.},%
  numbers=left,%
  float=p,
  %framexleftmargin=1em,
  escapechar=|,
  label={lst:reduce11}]
float id(float x) { return x; }$\label{lst:reduce11:id}$
float sumUp(float x, float y) { return x+y; }$\label{lst:reduce11:sumUp}$
kernel
  void vecSum(global float* g_idata, global float* g_odata,
              unsigned int N, local float* sdata) {
     local float* sdata1 = sdata;
     local float* sdata2 = &sdata1[128];
     local float* sdata3 = &sdata2[64];
   |\tikzmark{wg_start}|  for (int wg_id = get_group_id(0); wg_id < (N / (128));$\label{lst:reduce11:for_wg}\label{lst:reduce11:wg_start}$
          wg_id += get_num_groups(0)) {
       |\tikzmark{lc1_start}|  {$\label{lst:reduce11:lc1_start}$
           int l_id = get_local_id(0);$\label{lst:reduce11:lc1}$
           sdata1[l_id] = id(g_idata[(wg_id * 128) + l_id]);$\label{lst:reduce11:lc1:copy}$
         }
       |\tikzmark{lc1_end}|  barrier(CLK_LOCAL_MEM_FENCE);$\label{lst:reduce11:lc1:barrier}\label{lst:reduce11:lc1_end}$
       |\tikzmark{iter_start}|  {$\label{lst:reduce11:iter_start}$
           int size = 128;$\label{lst:reduce11:iter:size}$
           local float* sin = sdata1;$\label{lst:reduce11:iter:inCreate}$
           local float* sout =$\label{lst:reduce11:iter:outCreate}$
                    ((7 & 1) != 0) ? sdata2 : sdata3;
           #pragma unroll 1
           for (int j = 0; j < 7; j += 1) {$\label{lst:reduce11:iter:for}$
           |\tikzmark{lc2_start}|  int l_id = get_local_id(0);$\label{lst:reduce11:lc2_start}$
             if (l_id < size / 2) {$\label{lst:reduce11:lc2:if}$
               float acc = 0.0f;$\label{lst:reduce11:lc2:acc}$
               for(int i = 0; i < 2; ++i) {$\label{lst:reduce11:lc2:reduce}$
                 acc = sumUp(acc, sin[(l_id * 2) + i]); }
               sout[l_id] = acc;$\label{lst:reduce11:lc2:reduceStore}$
             }
           |\tikzmark{lc2_end}|  barrier(CLK_LOCAL_MEM_FENCE);$\label{lst:reduce11:lc2_end}$
             size = (size / (2));$\label{lst:reduce11:iter:size:update}$
             sin = ( sout==sdata3 ) ? sdata3:sdata2;$\label{lst:reduce11:iter:inSwap}$
             sout = ( sout==sdata3 ) ? sdata2:sdata3;$\label{lst:reduce11:iter:outSwap}$
           }
       |\tikzmark{iter_end}|  }$\label{lst:reduce11:iter_end}$
       |\tikzmark{lc3_start}|  {$\label{lst:reduce11:lc3_start}$
           int l_id = get_local_id(0);
           if (l_id < 1) {
             g_odata[wg_id + l_id] = id(sdata2[l_id]);$\label{lst:reduce11:lc3:copy}$
           }
       |\tikzmark{lc3_end}|  }$\label{lst:reduce11:lc3_end}$
   |\tikzmark{wg_end}|  }$\label{lst:reduce11:wg_end}$
}|\begin{tikzpicture}[remember picture,overlay]
  \begin{scope}[line width=2pt, color=RoyalBlue]
    \draw (wg_start.north) -- +(.25,0);
    \draw (wg_start.north) -- %
      node[rotate=90, above, color=black] {\mapWorkgroup} (wg_end.center);
    \draw (wg_end.center)  -- +(.25,0);

    \draw (lc1_start.north) -- +(.25,0);
    \draw (lc1_start.north) -- %
      node[rotate=90, above, color=black] {\mapLocal} (lc1_end.center);
    \draw (lc1_end.center)  -- +(.25,0);

    \draw (iter_start.north) -- +(.25,0);
    \draw (iter_start.north) -- %
      node[rotate=90, above, color=black] {\iterateN} (iter_end.center);
    \draw (iter_end.center)  -- +(.25,0);

    \draw (lc2_start.north) -- +(.25,0);
    \draw (lc2_start.north) -- %
      node[rotate=90, above, color=black] {\mapLocal} (lc2_end.center);
    \draw (lc2_end.center)  -- +(.25,0);

    \draw (lc3_start.north) -- +(.25,0);
    \draw (lc3_start.north) -- %
      node[rotate=90, above, color=black] {\mapLocal} (lc3_end.center);
    \draw (lc3_end.center)  -- +(.25,0);
  \end{scope}
\end{tikzpicture}|
\end{lstlisting}
%
The overall structure of the expression can also be found in the \OpenCL code, as highlighted on the left-handed side.
\OpenCL code is generated by traversing the expression and following the data flow.
For each pattern visited, \OpenCL code is generated.
For some patterns, no code is generated, instead they change the array type and, therefore, have an effect on how the code for following patterns is generated.

The outermost for-loop (line~\ref{lst:reduce11:for_wg} in \autoref{lst:reduce11}) is generated for the \mapWorkgroup pattern.
The loop variable \code{wg\_id} is based on the identifier of the work-group.
No code is emitted for the $\splitN\ 128$ pattern, but rather \todoU{the type}{sg: von wem?} is changed which influences the boundaries of the loop generated for the following \mapWorkgroup pattern. 
In each iteration of the loop, a work-group processes a chunk of 128 elements of the input array.

The first block nested inside the for-loop (line~\ref{lst:reduce11:lc1_start}--\ref{lst:reduce11:lc1_end}) corresponds to the subexpression:
$\join \circ \toLocal\ (\mapLocal\ (\mapSeq\ \id)) \circ \splitN\ 1$.
As previously, we assume a work-group size of 128 work-items, therefore, we know that after splitting the 128 elements processed by the work-group with $\splitN\ 1$ each work-item processes \emph{exactly one} element.
Based on this knowledge, we do not emit a for-loop for the \mapLocal pattern, as we did for the \mapWorkgroup pattern, instead we emit the single line~\ref{lst:reduce11:lc1}, where the local identifier of the work-item is obtained.
In the following line~\ref{lst:reduce11:lc1:copy}, the \code{id} function is invoked (defined in line~\ref{lst:reduce11:id}) to copy data from the input array to a local array \code{sdata1}.
This implementation follows the subexpression, where the \toLocal pattern dictates that we write to an array in the local memory and the $\mapSeq\ \id$ pattern performs the copy operation.
Again there is no for-loop generated for the $\mapSeq$ pattern.
This time, the for-loop would only iterate over a single element, therefore, the loop can be avoided.
The index used to read from the global memory is derived from \code{wg\_id} and \code{l\_id}, which ensures that each work-item in each work-group reads a separate element.
In contrast, the index for writing only uses \code{l\_id}, because the result is stored in the local memory and, therefore, each work-group operates on a separate copy of \code{sdata1}.
This section of the code is finished by the \code{barrier} in line~\ref{lst:reduce11:lc1:barrier}, which is emitted for the \mapLocal pattern to ensure proper synchronization between the work-items in the work-group.
There is no synchronization statement necessary for the \mapSeq pattern, as this pattern is only used sequentially in the context of a single work-item.

The second block (lines~\ref{lst:reduce11:iter_start}--\ref{lst:reduce11:iter_end}) corresponds to the subexpression performing the iteration:
$\iterateN\ 7\ (\ldots)$.
For the \iterateN pattern, a for-loop is emitted (line~\ref{lst:reduce11:iter:for}) performing the actual iteration.
The loop is annotated with a \code{\#pragma unroll 1} statement, which prevents the \OpenCL compiler from unrolling the loop.
The reason for this implementation is, that we want to control the unrolling of this loop explicitly with the iteration rewrite rule introduced in \autoref{section:rules}.
Additional code is generated before and after the for-loop.
Before, a variable (\code{size}) capturing the current size of the input array is created (line~\ref{lst:reduce11:iter:size}).
The size of this variable is updated after the loop, based on the effect of the nested pattern on the size of the array.
In this case, a partial reduction is performed reducing the array by half.
Therefore, the \code{size} variable is halved in line~\ref{lst:reduce11:iter:size:update} after every iteration.
A pair of pointers is created (lines~\ref{lst:reduce11:iter:inCreate} and~\ref{lst:reduce11:iter:outCreate}) and swapped after each iteration (lines~\ref{lst:reduce11:iter:inSwap} and~\ref{lst:reduce11:iter:outSwap}), so that the nested pattern has fixed input and output pointers to operate on.

The nested block (lines~\ref{lst:reduce11:lc2_start}--\ref{lst:reduce11:lc2_end}) corresponds to the pattern composition nested inside the \iterateN pattern:
$\join \circ \mapLocal\ (\reduceSeq\ (+)\ 0) \circ \splitN\ 2$.
For the \mapLocal pattern, the local work-group identifier is obtained and an if statement is emitted (line~\ref{lst:reduce11:lc2:if}).
No for-loop is emitted, as it is clear that the size of the array is maximal 128 (from the definition of \code{size} in line~\ref{lst:reduce11:iter:size}), as the variable is halved after each iteration (line~\ref{lst:reduce11:iter:size:update}).
The size information of the array is available in the array's type and, therefore, available when code is generated for the \mapLocal pattern.
Inside the if statement the code for the \reduceSeq pattern is emitted.
A for-loop is generated (line~\ref{lst:reduce11:lc2:reduce}) which iterates over the input chunk, which is in this case of size 2, due, to the $\splitN\ 2$ pattern.
An accumulation variable (\code{acc}, line~\ref{lst:reduce11:lc2:acc}) is initialized with the neutral value of the reduction and then used to store the temporary reduction results.
In line~\ref{lst:reduce11:lc2:reduceStore}, the result is stored to the memory using the \code{out} pointer prepared by the \iterateN pattern.
Finally, for the \mapLocal pattern a \code{barrier} for synchronization is emitted.

The final block (lines~\ref{lst:reduce11:lc3_start}--\ref{lst:reduce11:lc3_end}) corresponds to the last subexpression:
$\join \circ \toGlobal\ (\mapLocal\ (\mapSeq\ \id)) \circ \splitN\ 1$.
Similar to before, an if-statement is emitted for the \mapLocal pattern and the copy from the local to the global memory is performed in line~\ref{lst:reduce11:lc3:copy} by the code emitted for the $\mapSeq\ \id$ pattern.
No synchronization statement has to be emitted for the \mapLocal pattern, as there is no following pattern to synchronize with.

\bigskip

The \OpenCL codes for the other parallel reduction implementations are similar and can be found in \autoref{chapter:AppendixA}.
The code generation process remains the same:
the expression is traversed following the data flow.
For each visited pattern, \OpenCL code is emitted, or they influence the code generation by changing the type and, thus, encode information for the following patterns.

In the following, section we study the \OpenCL code generated for each pattern individually.





\subsection{Generating \OpenCL Code for Patterns}
\begin{table}[t]
\centering
\begin{tabular}{llll}
\toprule
    \multicolumn{2}{c}{\tabhead{Algorithmic Patterns}}
  & \multicolumn{2}{c}{\tabhead{\OpenCL Patterns}}\\
\midrule
 \map &
  \textbf{\zip} &
    \textbf{\mapWorkgroup} &
      \textbf{\reduceSeq}\\
 \reduce&
  \textbf{\splitN}&
    \textbf{\mapLocal}&
      \textbf{\reorderStride}\\
 \reorder&
  \textbf{\join} &
    \textbf{\mapGlobal}&
      \textbf{\toLocal}\\
 &
  \textbf{\iterateN} &
    \textbf{\mapWarp}&
      \textbf{\toGlobal}\\
 & &
    \textbf{\mapLane} &
      \textbf{\asVector}\\
 & & \textbf{\mapSeq} &
        \textbf{\asScalar}\\
 & & & \textbf{\vect}\\
\bottomrule
\end{tabular}
\caption{Overview of all algorithmic and \OpenCL patterns.}
\label{fig:patterns:generation}
\end{table}

\autoref{fig:patterns:generation} shows all patterns introduced in \autoref{section:patterns}.
The code generator does not know how to generate \OpenCL code for all patterns, for example there are are many different options to implement the \reduce pattern in \OpenCL, as we discussed in \autoref{section:reduce:case-study}.
Therefore, the code generator would have to make a choice which of the possible implementations to pick.
We want to avoid such situations, as this complicates the implementation of the code generator and limits both its flexibility and the performance of the generated code.
In our approach this decision about the implementation of \reduce has to be made before the code generator is invoked by applying the rewrite rules presented in \autoref{section:rules} which allow to \todoU{safely}{sg: ?? correctly?} derive specialized low-level implementations from high-level expressions.

The code generator generates code only for the patterns highlighted in bold in \autoref{fig:patterns:generation} (all patterns in the last three columns).
The three patterns in the first column: \map, \reduce, and \reorder, have to be eliminated from an expression before the code generation process can begin.

We will now discuss in more detail the code generation process for all highlighted patterns from \autoref{fig:patterns:generation}.

\paragraph{Zip}
The code generator emits no \OpenCL code for the \zip pattern.
Instead \zip's type has an effect on how code for following patterns is generated.
Let us look at the following example, where the \zip and \mapGlobal patterns are used together:
\begin{align}
  \mapGlobal\ (+)\ (\zip\ \vec{x}\ \vec{y})
\end{align}
When processing this expression, the \zip pattern is visited first.
The type of \zip makes it explicit to the code generator that, when emitting code for the following \mapGlobal, two elements -- one element from each array -- have to be read together.
In the implementation, the code generator will investigate the type of the input array before emitting code for the \mapGlobal.

\paragraph{Split and Join}
Similar to the \zip pattern, no \OpenCL code is emitted for neither \splitN nor \join.
By encoding the size of arrays in the type system, the information on how the data was shaped by the \splitN and \join patterns is passed to the following patterns.
This information is used later when generating the \OpenCL code for performing \OpenCL memory accesses. 
We will discuss the type system implementation in more detail in \autoref{section:typeSystem}.

\paragraph{Iterate}
%\autoref{lst:iterate:impl} shows the structure of the \OpenCL code generated for the \iterateN pattern.
%%
%\begin{lstlisting}[%                                                             
%caption={Structure of the \OpenCL code emitted for the \iterateN pattern.},%
%numbers=left,%
%float=tb,
%label={lst:iterate:impl}]
%int size = 128;
%local float* in = array0;
%local float* out = ((7 & 1) != 0) ? array1 : array2;
%for (int i = 0; i < 7; i +=1) {$\label{lst:iterate:impl:for}$
%  ... // code emitted for nested pattern$\label{lst:iterate:nested}$
%  in  = (out == array2) ? array2 : array1;$\label{lst:iterate:impl:in}$
%  out = (out == array2) ? array2 : array1;$label{lst:iterate:impl:out}$
%  size = size / 2;$\label{lst:iterate:impl:size}$
%}
%\end{lstlisting}
%%
%This code segment is actually taken from the parallel reduction example discussed in \autoref{eq:reduce11}.
%A for loop is generated (line~\ref{lst:iterate:impl:for}) for performing the actual iterations.
%Two pointers (\code{in} and \code{out}) are swapped after each iteration (lines~\ref{lst:iterate:impl:in} and~\ref{lst:iterate:impl:out}) and used in the code generated for the nested pattern which is emitted inside the body of the for loop (line~\ref{lst:iterate:nested}).
%Because this code segment is taken from the parallel reduction example, the \code{size} variable, which represents the size of the current input array, is reduced by half after each iteration step (line~\ref{lst:iterate:impl:size}).
A for-loop is emitted by the code generator for the \iterateN pattern.
As seen in the previous section, two pointers are generated and swapped after each iteration to provide input and output pointers to the nested pattern.
A variable storing the size of the input array is generated and updated after each iteration, based on the effect the \todoU{nested pattern}{sg: ?} has on the array size when processing the array.


\paragraph{Parallel OpenCL Maps}
The generation of \OpenCL code for each of the \map patterns: \mapWorkgroup, \mapLocal, \mapGlobal, \mapWarp, and \mapLane is rather straightforward.

We saw examples of the generated \OpenCL code in the previous section.
In general, a for-loop is emitted, where the loop variable refers to the corresponding identifier, \ie, the work-group id, local work-item id, global work-item id, and so on.
After the loop, an appropriate synchronization mechanism is emitted.
As there is no synchronization between work-items of different work-groups, the \mapWorkgroup and \mapGlobal patterns emit no synchronization statement directly, but after these patterns the \OpenCL kernel is terminated and a new \OpenCL kernel is created to continue the computation.
For the \mapLocal and \mapWarp patterns, a \code{barrier} is emitted to synchronize the work-items organized inside a work-group.
For the work-items organized inside a warp no synchronization is required, therefore, the \mapLane emits no synchronization statement.

When the code generator knows statically that a for-loop will be iterated at most once, an if statement is emitted instead.
If it is even clear that the loop will be iterated exactly once, the loop can be avoided completely.
We saw both of these cases in \autoref{lst:reduce11} in the previous section.
The code generator uses the array size information from the type system to make these decisions.

%\autoref{lst:mapWG:impl} shows the structure of the \OpenCL code generated for the \mapWorkgroup pattern.
%%
%\begin{lstlisting}[%                                                             
%caption={Structure of the \OpenCL code emitted for the \mapWorkgroup pattern.},%
%numbers=left,%
%float=tb,
%label={lst:mapWG:impl}]
%for (int wg_id = get_group_id(0); wg_id < size;
%     wg_id += get_num_groups(0)) {
%  ... // code emitted for nested pattern
%}
%return;
%\end{lstlisting}
%%
%A for loop is emitted where the loop variable (\code{wg\_id}) represents the work-group id in \OpenCL.
%The loop variable is used by the nested pattern inside the loop to ensure that the correct index is accessed in memory.
%After the for loop an synchronization mechanism is emitted, in this case \code{return}.
%As global synchronization between work-items is not possible in a single kernel in \OpenCL, the only way for global synchronization is to terminate the \OpenCL kernel and continue the processing in a new one. 
%
%\autoref{lst:mapLocal:impl} shows the \OpenCL code generated for the \mapLocal pattern.
%%
%\begin{lstlisting}[%                                                             
%caption={Structure of the \OpenCL code emitted for the \mapLocal pattern.},%
%numbers=left,%
%float=tb,
%label={lst:mapLocal:impl}]
%for (int l_id_id = get_local_id(0); l_id < size;
%     l_id += get_local_size(0)) {
%  ... // code emitted for nested pattern
%}
%barrier(CLK_LOCAL_MEM_FENCE);
%\end{lstlisting}
%%
%Here the \OpenCL \code{get\_local\_id(0)} function is used to obtain the id of the work-item inside the work-group.
%The \code{barrier} function is used for synchronization.
%
%The \OpenCL code emitted for \mapGlobal, \mapWarp, and \mapLane is similar reflecting the specific way of obtaining the loop variable and synchronization mechanism in \OpenCL.
%As there is no synchronization required between work-item of a single warp, the \mapLane pattern emits no synchronization statement.

\paragraph{Sequential Map and Reduction}
The \OpenCL implementations of the sequential \mapSeq and \reduceSeq patterns are shown in \autoref{lst:mapSeq:impl} and \autoref{lst:redSeq:impl}.
%
\begin{lstlisting}[%                                                             
caption={Structure of the \OpenCL code emitted for the \mapSeq pattern.},%
numbers=left,%
float=tb,
label={lst:mapSeq:impl}]
for (int i = 0; i < size; ++i) {
  output[out_index] = f(input[in_index]);
}
\end{lstlisting}
%
%
\begin{lstlisting}[%                                                             
caption={Structure of the \OpenCL code emitted for the \reduceSeq pattern.},%
numbers=left,%
float=tb,
label={lst:redSeq:impl}]
float acc = 0.0f;
for (int i = 0; i < size; ++i) {
  acc = f(acc, input[in_index]);
}
output[out_index] = acc;
\end{lstlisting}
%
Both implementations are straightforward.
The \mapSeq implementation applies its customizing function to each element of the input array and stores the outputs in an output array.
The \reduceSeq implementation uses an accumulation variable to accumulate the result of the reduction while it iterates through the input array.
After the for-loop, the result is stored in the output array.

The input and output indices: \code{in\_index} and \code{out\_index} are generated based on the pattern in which the \mapSeq or \reduceSeq is nested in.
We saw in \autoref{lst:reduce11} that access to the global memory is based on the work-group and work-item identifier, while the local memory access is only based on the work-item identifier, because each work-item has its exclusive copy of local memory to operate on.
The \reorderStride pattern influences the generation of the indices too, as we describe below.

%The most complicated bit is the generation of the input and output indices: \code{in\_index} and \code{out\_index}.
%To understand how this works let us investigate an example which combines the previous code segments.
%\autoref{lst:mapTogether:impl} shows the code generated for the expression:
%\begin{align*}
%  &\join \circ \mapWorkgroup\ \big(\\
%  &\qquad\join \circ \mapLocal\ (\mapSeq\ id) \circ \splitN\ 1\big) \circ \splitN\ 128
%\end{align*}
%%
%\begin{lstlisting}[%                                                             
%caption={Structure of the \OpenCL code emitted for the \mapLocal pattern.},%
%numbers=left,%
%float=tb,
%label={lst:mapTogether:impl}]
%for (int wg_id = get_group_id(0); wg_id < size / 128;
%     wg_id += get_num_groups(0)) {
%  for (int l_id_id = get_local_id(0); l_id < 128;
%       l_id += get_local_size(0)) {
%    for (int i = 0; i < 1; ++i) {
%      output[wg_id * 128 + l_id + i]$\label{lst:mapTogether:impl:output}$
%        = id(input[wg_id * 128 + l_id + i]);$\label{lst:mapTogether:impl:input}$
%    }
%  }
%  barrier(CLK_LOCAL_MEM_FENCE);
%}
%return;
%\end{lstlisting}
%%
%The input and output indices (line~\ref{lst:mapTogether:impl:output} and~\ref{lst:mapTogether:impl:input})
%
%\begin{lstlisting}[%                                                             
%caption={Structure of the \OpenCL code emitted for the \mapLocal pattern.},%
%numbers=left,%
%float=tb,
%label={lst:mapTogether:impl2}]
%for (int wg_id = get_group_id(0); wg_id < size / 128;
%     wg_id += get_num_groups(0)) {
%  int l_id = get_local_id(0);
%  output[wg_id * 128 + l_id + 0]
%    = id(input[wg_id * 128 + l_id + 0]);
%  barrier(CLK_LOCAL_MEM_FENCE);
%}
%return;
%\end{lstlisting}

\paragraph{Reorder-Stride}
No code is emitted for the $\reorderStride$ pattern, rather it influences the generation of the next input index which is used to read from memory.
This implementation follows the observation that there is no difference between reordering an array and then reading from it in the normal oder, and reading directly from the array in an \todoU{reordered order}{sg: engl?}.

For the implementation, when visiting the \reorderStride pattern an information about the stride used ($s$) is stored on a stack which is consumed the next time an input index is generated.
The index generation will then emit an index which ultimately can be used \todoU{to change which element}{sg: engl.} is read by which thread.

The second expression implementing parallel reduction shown in \autoref{eq:reduce12} added a $\reorderStride\ size/2$ pattern, where $size$ is denoting the size of the input array.
In \autoref{definition:pattern:reorderStride} the formula for $\reorderStride\ s$ was defines as: $j / n + s \times (j \bmod{n})$, where $j$ is the index to be changed and $n = size / s$.

When we look back at \OpenCL code generated for the first expression in \autoref{lst:reduce11}, we can see that the index generated for the for-loop in line~\ref{lst:reduce11:lc2:reduce} was:
\lstinline!l_id * 2 + i!.
Which gives us:
\begin{lstlisting}[numbers=none, frame=none]
    j = l_id * 2 + i
    s = size/2
    n = 2
\end{lstlisting}
Applied to the $\reorderStride$ formula, we get:
\begin{lstlisting}[numbers=none, frame=none]
    (l_id * 2 + i) / 2 + (size/2) * ((l_id * 2 + i) % 2)
\end{lstlisting}
Which can be simplified by applying integer division and modulo rules. Finally, giving us:
\begin{lstlisting}[numbers=none, frame=none]
    l_id + (size/2) * i
\end{lstlisting}
Which is the index used after performing the reordering.
Now we can also see why this reordering is called $\reorderStride$, as $i$ is the innermost loop index starting at $0$ and incrementing it will multiply the \emph{stride} which is added as an offset to \code{l\_id}.
This index will ensure that each work-item reads a different element from the input array than before.
In this case, the reordering is applied to avoid local memory bank conflicts.

\paragraph{{\footnotesize to}Local and {\footnotesize to}Global}
For both patterns, no \OpenCL code is emitted.
The patterns change the memory location of the nested pattern, \ie, where the nested pattern should write to.
When traversing an expression, these patterns will be always visited first before the nested pattern.
The information on where to write is passed along when visiting the nested pattern.

\paragraph{{\footnotesize as}Vector, {\footnotesize as}Scalar, and Vectorize}
The \asVector, and \asScalar patterns influence the type system and no \OpenCL code is emitted when visiting them.
Type casts are emitted by the following patterns to reflect the change of data type in \OpenCL.

The \vect pattern is used to vectorize functions.
Our current strategy is quite simplistic.
When emitting the \OpenCL code for these functions, the data types of the input and output arguments are changed appropriately and the body of the function is checked if it contains operations which are supported on vector data types in \OpenCL, \eg, the usual arithmetic operators.
If the body contains other operations, our process fails and no \OpenCL code can be generated.
In the future, we plan to incorporate a more advanced approach for vectorization of functions, \eg, like~\cite{KarrenbergHa2011}.


\subsection{The Type System and Static Memory Allocation}
\label{section:typeSystem}
Data types are usually primarily used to prevent errors when composing functions.
While we use data types for the same purpose in our system, we also use them for an additional purpose: to store information about the size of arrays processed by our expressions.
This information is used by the code generator to perform static memory allocation, \ie, to reserve the correct amount of memory required by a pattern.

Our $\splitN\ n$ pattern splits an array in chunks of a fixed size $n$.
As this size is stored in the type system, it is naturally available when processing the next patterns, by investigating their input type.
Therefore, there is no need to pass this information explicitly inside the code generator, it is rather implicitly available through the types involved.

When computing the amount of memory required to store the result of a pattern, we can look at the result type of the pattern and easily infer the amount of memory from it.
Using this approach, no dynamic memory allocation is required so far.

We currently do not perform any analysis to reuse memory objects, \ie, each pattern is assigned a newly allocated output array, even though arrays could be reused.
In \autoref{lst:reduce11} three local memory arrays (\code{sdata1}, \code{sdata2}, and \code{sdata3}) are used, where only a single array was used in the original \OpenCL implementation.
In the future, we will develop strategies to reuse memory and, thus, reduce the overall memory footprint.
We intend to adapt well-understood algorithms which are used for register allocation in compilers, like graph coloring~\cite{CooperTo2004}.

\subsection{Implementation Details}
Our system is implemented in \Cpp, using the template system and support for lambda functions. 
When generating code for a given low-level expression, two basic steps are performed.
First, we use the Clang/LLVM compiler infrastructure to parse the expression and produce an abstract syntax tree for it.
Second, we traverse the tree and emit code for every function call representing one of our low-level hardware patterns.
We have implemented the type system using template meta-programming techniques.

To simplify our implementation we leverage the \SkelCL library infrastructure for eventually executing the generated \OpenCL expression and to transfer data to and from the \GPU.

%As part of the first step, we have developed a type system which plays a dual role.
%Firstly, it prevents the user, or a rewrite rule, to produce an expression that is not correct.
%Secondly, the type system encodes informations that are necessary for code generation, such as memory address space and array size information, which are used to allocate memory.

%The design of our code generator is straightforward since no optimization decisions are made at this stage.
%We avoid performing complex analysis of the code which makes our design very different compared to traditional optimizing compilers.

